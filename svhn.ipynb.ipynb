{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport requests\nimport os\nplt.style.use([\"dark_background\"])","metadata":{"id":"8TUEBvGTupie","execution":{"iopub.status.busy":"2024-03-19T11:43:45.537894Z","iopub.execute_input":"2024-03-19T11:43:45.538199Z","iopub.status.idle":"2024-03-19T11:44:04.746405Z","shell.execute_reply.started":"2024-03-19T11:43:45.538172Z","shell.execute_reply":"2024-03-19T11:44:04.745649Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-19 11:43:49.146094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-19 11:43:49.146194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-19 11:43:49.425053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Workflow\n---\n\n### 1. Understanding the SVHN Dataset\n- **Objective**: Familiarize yourself with the SVHN dataset, its structure, and the type of data it contains.\n- **Actions**: Study the dataset's documentation and explore a few samples to understand the format and challenges (e.g., image size, color, diversity in house numbers).\n\n### 2. Setting Up Your Environment\n- **Objective**: Prepare your software and hardware environment for the project.\n- **Actions**:\n    - Ensure you have a suitable programming environment (like Python with TensorFlow or PyTorch).\n    - Set up necessary libraries and dependencies (e.g., NumPy, Matplotlib for visualization).\n    - If using a GPU, ensure drivers and necessary libraries (like CUDA) are installed.\n\n### 3. Data Preprocessing\n- **Objective**: Prepare the SVHN dataset for training and testing.\n- **Actions**:\n    - Download and load the SVHN dataset.\n    - Normalize the images and labels (e.g., scale pixel values, one-hot encode labels).\n    - Split the dataset into training, validation, and testing sets.\n    - Implement data augmentation techniques if necessary.\n\n### 4. Implementing the New Activation Function\n- **Objective**: Code your new activation function.\n- **Actions**:\n    - Define the mathematical function in your chosen programming language.\n    - Ensure the function is differentiable as it will be used in backpropagation.\n    - Test the function independently to verify its correctness.\n\n### 5. Model Architecture\n- **Objective**: Design a neural network model suitable for the SVHN dataset.\n- **Actions**:\n    - Choose an appropriate model architecture (e.g., a Convolutional Neural Network).\n    - Integrate your new activation function into the model.\n    - Define the optimizer, loss function, and metrics for model evaluation.\n\n### 6. Training the Model\n- **Objective**: Train the model on the SVHN dataset.\n- **Actions**:\n    - Train the model using the training set, and validate it using the validation set.\n    - Monitor performance metrics like loss and accuracy.\n    - Experiment with different hyperparameters (e.g., learning rate, batch size) for optimal performance.\n\n### 7. Evaluation and Testing\n- **Objective**: Evaluate the performance of your model.\n- **Actions**:\n    - Test the model on the test set.\n    - Analyze the results using various metrics (e.g., accuracy, precision, recall).\n    - Compare the performance with baseline models or other activation functions.\n\n### 8. Documentation and Analysis\n- **Objective**: Document your findings and analyze the performance of the new activation function.\n- **Actions**:\n    - Document the experiment setup, model architecture, and training process.\n    - Analyze how the new activation function affected the modelâ€™s performance.\n    - Compare with existing activation functions and draw conclusions.\n\n### 9. Sharing Results\n- **Objective**: Share your findings with the community or stakeholders.\n- **Actions**:\n    - Prepare a report or presentation summarizing your methodology, results, and conclusions.\n    - Consider publishing your code and results in a repository or submitting them to relevant journals or conferences.\n\n### 10. Iterative Improvement\n- **Objective**: Refine your approach based on feedback and results.\n- **Actions**:\n    - Gather feedback from peers or the community.\n    - Make iterative improvements to the activation function or model based on this feedback.\n    - Continuously evaluate the performance after each iteration.","metadata":{"id":"e9G1pyuktWNr"}},{"cell_type":"markdown","source":"# The SVHN Dataset\n---\n\nThe Street View House Numbers (SVHN) dataset is a widely-used benchmark in the field of machine learning, particularly for tasks involving image recognition and digit classification. Here's a detailed description of the dataset:\n\n### Overview\n- **Purpose**: The SVHN dataset is primarily used for developing and evaluating machine learning models, especially in the context of computer vision and object recognition tasks.\n- **Content**: It contains images of house numbers collected from Google Street View images. These are real-world, natural images of house numbers, as opposed to hand-written or artificially generated digits.\n\n### Characteristics of the Images\n- **Format**: The images are in color and are in the JPEG format.\n- **Resolution**: They vary in size but often need resizing for consistent input to models.\n- **Composition**: Each image typically includes a number (which can be a single digit or a sequence of digits) and some background visual context, like parts of the house or street.\n- **Variability**: The dataset includes a wide variety of fonts, colors, styles, and occlusions, making it more complex and realistic compared to simpler digit datasets like MNIST.\n\n### Dataset Structure\n- **Two Formats**:\n  - **Original Images**: Contains the entire picture of a house with a number on it.\n  - **Cropped Digits**: Contains cropped images centered around each digit. These are more directly comparable to other digit datasets.\n- **Labels**: Each image is labeled with the number it represents, which can range from 0 to 9.\n\n### Data Split\n- **Training Set**: Contains a larger portion of the images, used for training machine learning models.\n- **Test Set**: A separate set of images used to evaluate the performance of trained models.\n- **Extra Set**: Some versions of the dataset include an 'extra' set of images, which are somewhat less difficult than those in the standard training set and can be used for additional training.\n\n### Use in Research and Development\n- **Challenges**: The SVHN dataset is challenging due to factors like varying lighting conditions, low resolution, occlusions, and diverse backgrounds.\n- **Applications**: It's widely used in tasks like digit recognition, object detection, and more complex challenges such as sequence recognition or contextual understanding.\n\n### Availability\n- The dataset is publicly available and can be downloaded from its official website. It is often provided in a format that is ready to be used with common machine learning frameworks.\n\nThe SVHN dataset is considered more challenging than simpler digit datasets like MNIST due to the complexity and variability of the images. It provides a more realistic benchmark for real-world applications of image recognition technologies.","metadata":{"id":"DMDchQXuuDZQ"}},{"cell_type":"markdown","source":"# Acquiring the data","metadata":{"id":"x90bfFq2wMLA"}},{"cell_type":"code","source":"def download_svhn_data(url, path):\n    response = requests.get(url, stream=True)\n    with open(path, 'wb') as file:\n        for chunk in response.iter_content(chunk_size=128):\n            file.write(chunk)\n\ndef load_svhn_dataset(url, path):\n    if not os.path.exists(path):\n        print(f\"Downloading {url}...\")\n        download_svhn_data(url, path)\n    return scipy.io.loadmat(path)\n\n# URLs for the SVHN dataset (train and test)\ntrain_url = \"http://ufldl.stanford.edu/housenumbers/train_32x32.mat\"\ntest_url = \"http://ufldl.stanford.edu/housenumbers/test_32x32.mat\"\n\n# Paths where the datasets will be saved\ntrain_path = \"train_32x32.mat\"\ntest_path = \"test_32x32.mat\"\n\n# Load the datasets\ntrain_data = load_svhn_dataset(train_url, train_path)\ntest_data = load_svhn_dataset(test_url, test_path)\n\n# Extract images and labels\nx_train = np.array(train_data['X']).transpose((3, 0, 1, 2))\ny_train = train_data['y'].flatten()\nx_test = np.array(test_data['X']).transpose((3, 0, 1, 2))\ny_test = test_data['y'].flatten()\n\n# Print shapes of the loaded dataset as a basic check\nprint(\"Training data shape:\", x_train.shape)\nprint(\"Training labels shape:\", y_train.shape)\nprint(\"Test data shape:\", x_test.shape)\nprint(\"Test labels shape:\", y_test.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nB2pGPjluctl","outputId":"9f66d526-869e-4c2a-e895-cae2248ae2b3","execution":{"iopub.status.busy":"2024-03-19T11:44:04.747859Z","iopub.execute_input":"2024-03-19T11:44:04.748348Z","iopub.status.idle":"2024-03-19T11:44:39.428550Z","shell.execute_reply.started":"2024-03-19T11:44:04.748324Z","shell.execute_reply":"2024-03-19T11:44:39.427671Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat...\nDownloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat...\nTraining data shape: (73257, 32, 32, 3)\nTraining labels shape: (73257,)\nTest data shape: (26032, 32, 32, 3)\nTest labels shape: (26032,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{"id":"xQsKfyuNvIQG"}},{"cell_type":"code","source":"def load_svhn_data(file_path):\n    data = scipy.io.loadmat(file_path)\n    images = np.array(data['X']).transpose((3, 0, 1, 2))\n    labels = data['y'].flatten()\n    labels[labels == 10] = 0  # Replace label 10 with 0\n    return images, labels\n\n# Load dataset (assuming you have already downloaded it as shown in previous steps)\nx_train, y_train = load_svhn_data('train_32x32.mat')\nx_test, y_test = load_svhn_data('test_32x32.mat')\n\n# Function to visualize images from the dataset\ndef visualize_data(images, labels, num_samples=10):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i, ax in enumerate(axes):\n        ax.imshow(images[i])\n        ax.set_title(f'Label: {labels[i]}')\n        ax.axis('off')\n    plt.show()\n\n# Visualize some training images\nvisualize_data(x_train, y_train)\n\n# Explore label distribution\ndef plot_label_distribution(labels, title):\n    unique, counts = np.unique(labels, return_counts=True)\n    plt.figure(figsize=(8, 4))\n    plt.bar(unique, counts)\n    plt.title(title)\n    plt.xlabel('Labels')\n    plt.ylabel('Frequency')\n    plt.xticks(unique)\n    plt.show()\n\n# Plotting label distribution for training and test sets\nplot_label_distribution(y_train, 'Training Set Label Distribution')\nplot_label_distribution(y_test, 'Test Set Label Distribution')\n\n# Explore pixel value distribution\ndef plot_pixel_distribution(images, title):\n    plt.figure(figsize=(8, 4))\n    plt.hist(images.ravel(), bins=255, range=[0,255])\n    plt.title(title)\n    plt.xlabel('Pixel intensity')\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Plotting pixel value distribution for training set\nplot_pixel_distribution(x_train, 'Training Set Pixel Value Distribution')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"NPD8NenFwLd1","outputId":"e5c7f083-cdde-402e-bc5c-d018332162c9","execution":{"iopub.status.busy":"2024-03-19T11:44:39.429774Z","iopub.execute_input":"2024-03-19T11:44:39.430055Z","iopub.status.idle":"2024-03-19T11:44:47.905040Z","shell.execute_reply.started":"2024-03-19T11:44:39.430030Z","shell.execute_reply":"2024-03-19T11:44:47.904173Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x300 with 10 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### Explanation\n1. **Load SVHN Data**: The `load_svhn_data` function loads the data from the `.mat` files.\n2. **Visualize Images**: The `visualize_data` function plots a subset of images from the dataset along with their labels.\n3. **Label Distribution**: The `plot_label_distribution` function displays the frequency of each label in the dataset, which is useful for understanding class balance.\n4. **Pixel Value Distribution**: The `plot_pixel_distribution` function shows the distribution of pixel intensity values across all images, providing insight into the overall contrast and brightness characteristics of the dataset.","metadata":{"id":"uRkBBshExhnb"}},{"cell_type":"markdown","source":"# Preprocessing the dataset","metadata":{"id":"1V_TgOCMxq5h"}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndef preprocess_svhn_data(images, labels):\n    # Normalize pixel values\n    normalized_images = images.astype('float32') / 255.0\n\n    # One-hot encode labels\n    one_hot_labels = to_categorical(labels)\n\n    return normalized_images, one_hot_labels\n\n# Preprocess the data\nx_train_normalized, y_train_one_hot = preprocess_svhn_data(x_train, y_train)\nx_test_normalized, y_test_one_hot = preprocess_svhn_data(x_test, y_test)\n\n\n# Splitting the training data into a smaller training set and a validation set\n# Here, I'm using 80% of the data for training and 20% for validation\nx_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n    x_train_normalized,\n    y_train_one_hot,\n    test_size=0.20,  # 20% for validation\n    random_state=42  # for reproducibility\n)\n\n# Print the shapes of the splits to verify\nprint(\"Shape of new training images:\", x_train_split.shape)\nprint(\"Shape of new training labels:\", y_train_split.shape)\nprint(\"Shape of validation images:\", x_val_split.shape)\nprint(\"Shape of validation labels:\", y_val_split.shape)\n\n\n# Check the shapes of testing data after preprocessing\nprint(\"Shape of test images:\", x_test_normalized.shape)\nprint(\"Shape of test labels:\", y_test_one_hot.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFWD2DRGyCSs","outputId":"5a6536b3-5a93-4c3f-c661-64cdf8bbe260","execution":{"iopub.status.busy":"2024-03-19T11:44:47.907254Z","iopub.execute_input":"2024-03-19T11:44:47.907552Z","iopub.status.idle":"2024-03-19T11:44:49.147747Z","shell.execute_reply.started":"2024-03-19T11:44:47.907527Z","shell.execute_reply":"2024-03-19T11:44:49.146762Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Shape of new training images: (58605, 32, 32, 3)\nShape of new training labels: (58605, 10)\nShape of validation images: (14652, 32, 32, 3)\nShape of validation labels: (14652, 10)\nShape of test images: (26032, 32, 32, 3)\nShape of test labels: (26032, 10)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Defining the model","metadata":{"id":"mhfSPJz7yM3Q"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \\\nActivation, Add, AveragePooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\ndef resnet_block(input_data, filters, conv_size, activation_func):\n    x = Conv2D(filters, conv_size, padding='same')(input_data)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    x = Conv2D(filters, conv_size, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    # Adding the input data to the output of the block (Skip Connection)\n    x = Add()([x, input_data])\n\n    x = Activation(activation_func)(x)\n    return x\n\ndef build_resnet20(input_shape, num_classes, activation_func):\n    inputs = Input(shape=input_shape)\n\n    # Initial Conv Layer\n    x = Conv2D(16, (3, 3), padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    # ResNet Blocks\n    for _ in range(3):\n        x = resnet_block(x, 16, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 32, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 64, (3, 3), activation_func)\n\n    # Average Pooling\n    x = AveragePooling2D(pool_size=(8, 8))(x)\n    x = Flatten()(x)\n\n    # Output Layer\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Custom activation function\ndef custom_activation(x):\n    # Define your custom activation logic here\n    return tf.nn.relu(x)  # Example: using ReLU as a placeholder\n\n# Building the model with the custom activation function\ninput_shape = (32, 32, 3)  # Change based on your dataset\nnum_classes = 10  # Change based on your dataset\n\nmodel = build_resnet20(input_shape, num_classes, custom_activation)\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFAi61t_z2ZE","outputId":"f7215752-ba51-46f4-b31f-1c9da0e76787","execution":{"iopub.status.busy":"2024-03-19T11:44:49.149130Z","iopub.execute_input":"2024-03-19T11:44:49.149763Z","iopub.status.idle":"2024-03-19T11:44:50.700248Z","shell.execute_reply.started":"2024-03-19T11:44:49.149735Z","shell.execute_reply":"2024-03-19T11:44:50.699406Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚        \u001b[38;5;34m448\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_1        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_2        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_3        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_4        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_5        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚      \u001b[38;5;34m2,320\u001b[0m â”‚ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_2 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_6        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m4,640\u001b[0m â”‚ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_7        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_8        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_3 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_9        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_10       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_10[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_4 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_11       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_11[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_12       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚      \u001b[38;5;34m9,248\u001b[0m â”‚ activation_12[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚        \u001b[38;5;34m128\u001b[0m â”‚ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_5 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚ activation_11[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_13       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚ \u001b[38;5;34m32\u001b[0m)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m18,496\u001b[0m â”‚ activation_13[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_14       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_14[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_15       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_15[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_6 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_14[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_16       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_16[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_17       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_17[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_7 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_16[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_18       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_18[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_19       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,928\u001b[0m â”‚ activation_19[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_8 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_18[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_20       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mActivation\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ average_pooling2d   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ activation_20[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚ (\u001b[38;5;33mAveragePooling2D\u001b[0m)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ average_pooling2â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        â”‚        \u001b[38;5;34m650\u001b[0m â”‚ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_1        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_2        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_3        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_4        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_5        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> â”‚ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_6        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> â”‚ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_7        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_8        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_9        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_10       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_11       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_12       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> â”‚ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_13       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_14       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_15       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_16       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_17       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_18       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_19       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> â”‚ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ activation_20       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ average_pooling2d   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)  â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ average_pooling2â€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m318,346\u001b[0m (1.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">318,346</span> (1.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m316,778\u001b[0m (1.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,778</span> (1.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,568\u001b[0m (6.12 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> (6.12 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train the Model with Different Activation Functions\n\nIterate over the chosen activation functions, training a new model with each and observing the results.","metadata":{"id":"5x33BeUZ0z69"}},{"cell_type":"code","source":"def train_model(activation_func, x_train, y_train, x_val, y_val, batch_size, learning_rate, name):\n    # Build the model\n    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1], activation_func=activation_func)\n\n    # Compile the model with specified learning rate\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                  metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n    \n        # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        f'{name}.keras', # Path where to save the model\n        save_best_only=True, # Only save a model if `val_loss` has improved\n        monitor='val_loss', # Monitor the validation loss\n        mode='min', # The lower the validation loss, the better the model\n        verbose=1 # Log a message when a better model is found\n    )\n\n    # Train the model with specified batch size\n    history = model.fit(x_train, y_train, epochs=60, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1, callbacks = [checkpoint_cb])\n    return history, model\n\n\n\n# Parameters\nbatch_size = 32\nlearning_rate = 0.005\n\n# Activation functions to try\nactivation_functions = [tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\nnames = [\"relu\", \"sigmoid\", \"tanh\"]\nhistories = {}\n\n# Train and evaluate the model with each activation function\nprint(f\"Training with Relu activation function\")\nhistory_relu, model_relu = train_model(tf.nn.relu, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"relu\")\nhistories[\"relu\"] = history_relu\n\nprint(f\"\\n\\n Training with Sigmoid activation function\")\nhistory_sigmoid, model_sigmoid = train_model( tf.nn.sigmoid, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"sigmoid\")\nhistories[\"sigmoid\"] = history_sigmoid\n\n\nprint(f\"\\n\\n Training with Tanh activation function\")\nhistory_tanh, model_tanh = train_model(tf.nn.tanh, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"tanh\")\nhistories[\"tanh\"] = history_tanh\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"id":"w0Onwc7Bz42u","outputId":"6d511399-45f0-40db-e41f-f67607b4ca07","execution":{"iopub.status.busy":"2024-03-19T11:44:50.701361Z","iopub.execute_input":"2024-03-19T11:44:50.701651Z","iopub.status.idle":"2024-03-19T12:44:11.160028Z","shell.execute_reply.started":"2024-03-19T11:44:50.701627Z","shell.execute_reply":"2024-03-19T12:44:11.159180Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Training with Relu activation function\nEpoch 1/60\n\u001b[1m   7/1832\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - accuracy: 0.1719 - auc: 0.5625 - loss: 3.0494 - precision: 0.0961 - recall: 0.0173           ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710848717.650238     103 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4880 - auc: 0.8309 - loss: 1.4803 - precision: 0.7331 - recall: 0.3557\nEpoch 1: val_loss improved from inf to 0.44140, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 18ms/step - accuracy: 0.4882 - auc: 0.8310 - loss: 1.4800 - precision: 0.7332 - recall: 0.3559 - val_accuracy: 0.8652 - val_auc: 0.9866 - val_loss: 0.4414 - val_precision: 0.9267 - val_recall: 0.8252\nEpoch 2/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9039 - auc: 0.9915 - loss: 0.3241 - precision: 0.9375 - recall: 0.8774\nEpoch 2: val_loss improved from 0.44140 to 0.30721, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9039 - auc: 0.9915 - loss: 0.3241 - precision: 0.9375 - recall: 0.8774 - val_accuracy: 0.9094 - val_auc: 0.9928 - val_loss: 0.3072 - val_precision: 0.9427 - val_recall: 0.8830\nEpoch 3/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9232 - auc: 0.9939 - loss: 0.2625 - precision: 0.9478 - recall: 0.9028\nEpoch 3: val_loss improved from 0.30721 to 0.26410, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9233 - auc: 0.9939 - loss: 0.2625 - precision: 0.9478 - recall: 0.9028 - val_accuracy: 0.9239 - val_auc: 0.9930 - val_loss: 0.2641 - val_precision: 0.9450 - val_recall: 0.9102\nEpoch 4/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9339 - auc: 0.9947 - loss: 0.2287 - precision: 0.9563 - recall: 0.9169\nEpoch 4: val_loss did not improve from 0.26410\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9339 - auc: 0.9947 - loss: 0.2287 - precision: 0.9563 - recall: 0.9169 - val_accuracy: 0.9021 - val_auc: 0.9903 - val_loss: 0.3311 - val_precision: 0.9286 - val_recall: 0.8856\nEpoch 5/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9424 - auc: 0.9954 - loss: 0.2044 - precision: 0.9604 - recall: 0.9280\nEpoch 5: val_loss did not improve from 0.26410\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9424 - auc: 0.9954 - loss: 0.2044 - precision: 0.9604 - recall: 0.9280 - val_accuracy: 0.9253 - val_auc: 0.9926 - val_loss: 0.2660 - val_precision: 0.9439 - val_recall: 0.9124\nEpoch 6/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9517 - auc: 0.9965 - loss: 0.1697 - precision: 0.9678 - recall: 0.9400\nEpoch 6: val_loss improved from 0.26410 to 0.23896, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9517 - auc: 0.9965 - loss: 0.1698 - precision: 0.9678 - recall: 0.9400 - val_accuracy: 0.9335 - val_auc: 0.9936 - val_loss: 0.2390 - val_precision: 0.9510 - val_recall: 0.9212\nEpoch 7/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9574 - auc: 0.9967 - loss: 0.1517 - precision: 0.9704 - recall: 0.9475\nEpoch 7: val_loss improved from 0.23896 to 0.23876, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9574 - auc: 0.9967 - loss: 0.1517 - precision: 0.9704 - recall: 0.9475 - val_accuracy: 0.9338 - val_auc: 0.9941 - val_loss: 0.2388 - val_precision: 0.9523 - val_recall: 0.9201\nEpoch 8/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9619 - auc: 0.9972 - loss: 0.1354 - precision: 0.9726 - recall: 0.9530\nEpoch 8: val_loss improved from 0.23876 to 0.22092, saving model to relu.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9619 - auc: 0.9972 - loss: 0.1355 - precision: 0.9726 - recall: 0.9530 - val_accuracy: 0.9389 - val_auc: 0.9943 - val_loss: 0.2209 - val_precision: 0.9571 - val_recall: 0.9265\nEpoch 9/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9657 - auc: 0.9977 - loss: 0.1177 - precision: 0.9741 - recall: 0.9580\nEpoch 9: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9657 - auc: 0.9977 - loss: 0.1178 - precision: 0.9741 - recall: 0.9580 - val_accuracy: 0.9360 - val_auc: 0.9930 - val_loss: 0.2369 - val_precision: 0.9497 - val_recall: 0.9273\nEpoch 10/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9704 - auc: 0.9979 - loss: 0.1060 - precision: 0.9773 - recall: 0.9638\nEpoch 10: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9704 - auc: 0.9979 - loss: 0.1060 - precision: 0.9772 - recall: 0.9638 - val_accuracy: 0.9238 - val_auc: 0.9905 - val_loss: 0.2836 - val_precision: 0.9384 - val_recall: 0.9148\nEpoch 11/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9719 - auc: 0.9985 - loss: 0.0951 - precision: 0.9781 - recall: 0.9668\nEpoch 11: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9719 - auc: 0.9985 - loss: 0.0951 - precision: 0.9781 - recall: 0.9668 - val_accuracy: 0.9291 - val_auc: 0.9922 - val_loss: 0.2610 - val_precision: 0.9449 - val_recall: 0.9219\nEpoch 12/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9763 - auc: 0.9987 - loss: 0.0794 - precision: 0.9815 - recall: 0.9724\nEpoch 12: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9763 - auc: 0.9987 - loss: 0.0794 - precision: 0.9815 - recall: 0.9724 - val_accuracy: 0.9393 - val_auc: 0.9918 - val_loss: 0.2441 - val_precision: 0.9497 - val_recall: 0.9339\nEpoch 13/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9787 - auc: 0.9989 - loss: 0.0695 - precision: 0.9833 - recall: 0.9753\nEpoch 13: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9787 - auc: 0.9989 - loss: 0.0695 - precision: 0.9833 - recall: 0.9753 - val_accuracy: 0.9325 - val_auc: 0.9913 - val_loss: 0.2712 - val_precision: 0.9434 - val_recall: 0.9280\nEpoch 14/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0593 - precision: 0.9852 - recall: 0.9777\nEpoch 14: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0593 - precision: 0.9852 - recall: 0.9777 - val_accuracy: 0.9341 - val_auc: 0.9893 - val_loss: 0.2900 - val_precision: 0.9419 - val_recall: 0.9306\nEpoch 15/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9822 - auc: 0.9993 - loss: 0.0560 - precision: 0.9859 - recall: 0.9793\nEpoch 15: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9822 - auc: 0.9993 - loss: 0.0560 - precision: 0.9859 - recall: 0.9793 - val_accuracy: 0.9339 - val_auc: 0.9910 - val_loss: 0.2760 - val_precision: 0.9460 - val_recall: 0.9268\nEpoch 16/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9844 - auc: 0.9995 - loss: 0.0493 - precision: 0.9868 - recall: 0.9818\nEpoch 16: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9844 - auc: 0.9995 - loss: 0.0493 - precision: 0.9868 - recall: 0.9818 - val_accuracy: 0.9424 - val_auc: 0.9902 - val_loss: 0.2771 - val_precision: 0.9491 - val_recall: 0.9388\nEpoch 17/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9852 - auc: 0.9994 - loss: 0.0465 - precision: 0.9874 - recall: 0.9833\nEpoch 17: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9851 - auc: 0.9994 - loss: 0.0465 - precision: 0.9874 - recall: 0.9833 - val_accuracy: 0.9367 - val_auc: 0.9907 - val_loss: 0.2791 - val_precision: 0.9457 - val_recall: 0.9322\nEpoch 18/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9882 - auc: 0.9997 - loss: 0.0365 - precision: 0.9899 - recall: 0.9864\nEpoch 18: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9882 - auc: 0.9997 - loss: 0.0365 - precision: 0.9899 - recall: 0.9864 - val_accuracy: 0.9344 - val_auc: 0.9874 - val_loss: 0.3298 - val_precision: 0.9408 - val_recall: 0.9308\nEpoch 19/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9874 - auc: 0.9997 - loss: 0.0378 - precision: 0.9894 - recall: 0.9862\nEpoch 19: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9873 - auc: 0.9997 - loss: 0.0378 - precision: 0.9894 - recall: 0.9862 - val_accuracy: 0.9337 - val_auc: 0.9874 - val_loss: 0.3325 - val_precision: 0.9393 - val_recall: 0.9311\nEpoch 20/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9888 - auc: 0.9997 - loss: 0.0352 - precision: 0.9901 - recall: 0.9872\nEpoch 20: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9888 - auc: 0.9997 - loss: 0.0352 - precision: 0.9901 - recall: 0.9872 - val_accuracy: 0.9303 - val_auc: 0.9863 - val_loss: 0.3680 - val_precision: 0.9359 - val_recall: 0.9282\nEpoch 21/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9884 - auc: 0.9997 - loss: 0.0352 - precision: 0.9900 - recall: 0.9871\nEpoch 21: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9884 - auc: 0.9997 - loss: 0.0352 - precision: 0.9900 - recall: 0.9871 - val_accuracy: 0.9347 - val_auc: 0.9883 - val_loss: 0.3206 - val_precision: 0.9408 - val_recall: 0.9314\nEpoch 22/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9896 - auc: 0.9997 - loss: 0.0318 - precision: 0.9910 - recall: 0.9884\nEpoch 22: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9896 - auc: 0.9997 - loss: 0.0318 - precision: 0.9910 - recall: 0.9884 - val_accuracy: 0.9345 - val_auc: 0.9869 - val_loss: 0.3457 - val_precision: 0.9417 - val_recall: 0.9309\nEpoch 23/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9899 - auc: 0.9998 - loss: 0.0303 - precision: 0.9911 - recall: 0.9888\nEpoch 23: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9998 - loss: 0.0303 - precision: 0.9911 - recall: 0.9888 - val_accuracy: 0.9378 - val_auc: 0.9891 - val_loss: 0.3005 - val_precision: 0.9452 - val_recall: 0.9352\nEpoch 24/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9919 - auc: 0.9998 - loss: 0.0244 - precision: 0.9928 - recall: 0.9911\nEpoch 24: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9919 - auc: 0.9998 - loss: 0.0244 - precision: 0.9928 - recall: 0.9911 - val_accuracy: 0.9351 - val_auc: 0.9874 - val_loss: 0.3478 - val_precision: 0.9412 - val_recall: 0.9312\nEpoch 25/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9921 - auc: 0.9998 - loss: 0.0243 - precision: 0.9929 - recall: 0.9908\nEpoch 25: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9921 - auc: 0.9998 - loss: 0.0243 - precision: 0.9929 - recall: 0.9908 - val_accuracy: 0.9354 - val_auc: 0.9870 - val_loss: 0.3436 - val_precision: 0.9407 - val_recall: 0.9333\nEpoch 26/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9998 - loss: 0.0219 - precision: 0.9934 - recall: 0.9916\nEpoch 26: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9998 - loss: 0.0219 - precision: 0.9934 - recall: 0.9916 - val_accuracy: 0.9374 - val_auc: 0.9870 - val_loss: 0.3475 - val_precision: 0.9424 - val_recall: 0.9356\nEpoch 27/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9920 - auc: 0.9998 - loss: 0.0235 - precision: 0.9928 - recall: 0.9913\nEpoch 27: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9920 - auc: 0.9998 - loss: 0.0236 - precision: 0.9928 - recall: 0.9913 - val_accuracy: 0.9367 - val_auc: 0.9862 - val_loss: 0.3731 - val_precision: 0.9409 - val_recall: 0.9350\nEpoch 28/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9919 - auc: 0.9998 - loss: 0.0237 - precision: 0.9928 - recall: 0.9911\nEpoch 28: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9919 - auc: 0.9998 - loss: 0.0237 - precision: 0.9928 - recall: 0.9911 - val_accuracy: 0.9347 - val_auc: 0.9845 - val_loss: 0.4001 - val_precision: 0.9381 - val_recall: 0.9334\nEpoch 29/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - auc: 0.9998 - loss: 0.0204 - precision: 0.9939 - recall: 0.9926\nEpoch 29: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9932 - auc: 0.9998 - loss: 0.0204 - precision: 0.9939 - recall: 0.9926 - val_accuracy: 0.9356 - val_auc: 0.9858 - val_loss: 0.3890 - val_precision: 0.9400 - val_recall: 0.9341\nEpoch 30/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0239 - precision: 0.9933 - recall: 0.9919\nEpoch 30: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0239 - precision: 0.9933 - recall: 0.9919 - val_accuracy: 0.9369 - val_auc: 0.9853 - val_loss: 0.3807 - val_precision: 0.9405 - val_recall: 0.9355\nEpoch 31/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9998 - loss: 0.0185 - precision: 0.9944 - recall: 0.9935\nEpoch 31: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9998 - loss: 0.0185 - precision: 0.9944 - recall: 0.9935 - val_accuracy: 0.9360 - val_auc: 0.9840 - val_loss: 0.4037 - val_precision: 0.9402 - val_recall: 0.9348\nEpoch 32/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0231 - precision: 0.9934 - recall: 0.9923\nEpoch 32: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9928 - auc: 0.9997 - loss: 0.0231 - precision: 0.9934 - recall: 0.9923 - val_accuracy: 0.9381 - val_auc: 0.9851 - val_loss: 0.3999 - val_precision: 0.9416 - val_recall: 0.9366\nEpoch 33/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9945 - auc: 0.9998 - loss: 0.0170 - precision: 0.9950 - recall: 0.9940\nEpoch 33: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9945 - auc: 0.9998 - loss: 0.0170 - precision: 0.9950 - recall: 0.9940 - val_accuracy: 0.9369 - val_auc: 0.9872 - val_loss: 0.3608 - val_precision: 0.9410 - val_recall: 0.9345\nEpoch 34/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9999 - loss: 0.0206 - precision: 0.9934 - recall: 0.9922\nEpoch 34: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9999 - loss: 0.0206 - precision: 0.9934 - recall: 0.9922 - val_accuracy: 0.9353 - val_auc: 0.9846 - val_loss: 0.4084 - val_precision: 0.9390 - val_recall: 0.9337\nEpoch 35/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0179 - precision: 0.9943 - recall: 0.9936\nEpoch 35: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0179 - precision: 0.9943 - recall: 0.9936 - val_accuracy: 0.9300 - val_auc: 0.9839 - val_loss: 0.4205 - val_precision: 0.9348 - val_recall: 0.9281\nEpoch 36/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9938 - auc: 0.9998 - loss: 0.0192 - precision: 0.9943 - recall: 0.9933\nEpoch 36: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9938 - auc: 0.9998 - loss: 0.0192 - precision: 0.9943 - recall: 0.9933 - val_accuracy: 0.9411 - val_auc: 0.9864 - val_loss: 0.3866 - val_precision: 0.9436 - val_recall: 0.9403\nEpoch 37/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9941 - auc: 0.9998 - loss: 0.0172 - precision: 0.9947 - recall: 0.9935\nEpoch 37: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9941 - auc: 0.9998 - loss: 0.0172 - precision: 0.9947 - recall: 0.9935 - val_accuracy: 0.9344 - val_auc: 0.9855 - val_loss: 0.3944 - val_precision: 0.9386 - val_recall: 0.9324\nEpoch 38/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0134 - precision: 0.9959 - recall: 0.9952\nEpoch 38: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0134 - precision: 0.9959 - recall: 0.9952 - val_accuracy: 0.9361 - val_auc: 0.9837 - val_loss: 0.4402 - val_precision: 0.9387 - val_recall: 0.9350\nEpoch 39/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9941 - auc: 0.9998 - loss: 0.0183 - precision: 0.9947 - recall: 0.9936\nEpoch 39: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9941 - auc: 0.9998 - loss: 0.0183 - precision: 0.9947 - recall: 0.9936 - val_accuracy: 0.9386 - val_auc: 0.9845 - val_loss: 0.4239 - val_precision: 0.9422 - val_recall: 0.9380\nEpoch 40/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9947 - auc: 0.9998 - loss: 0.0165 - precision: 0.9952 - recall: 0.9943\nEpoch 40: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9947 - auc: 0.9998 - loss: 0.0165 - precision: 0.9952 - recall: 0.9943 - val_accuracy: 0.9391 - val_auc: 0.9841 - val_loss: 0.4322 - val_precision: 0.9423 - val_recall: 0.9380\nEpoch 41/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0137 - precision: 0.9959 - recall: 0.9951\nEpoch 41: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0137 - precision: 0.9959 - recall: 0.9951 - val_accuracy: 0.9386 - val_auc: 0.9851 - val_loss: 0.4171 - val_precision: 0.9423 - val_recall: 0.9370\nEpoch 42/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0147 - precision: 0.9955 - recall: 0.9945\nEpoch 42: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0147 - precision: 0.9955 - recall: 0.9945 - val_accuracy: 0.9382 - val_auc: 0.9854 - val_loss: 0.3920 - val_precision: 0.9421 - val_recall: 0.9376\nEpoch 43/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9998 - loss: 0.0119 - precision: 0.9965 - recall: 0.9960\nEpoch 43: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9998 - loss: 0.0119 - precision: 0.9965 - recall: 0.9960 - val_accuracy: 0.9390 - val_auc: 0.9849 - val_loss: 0.4124 - val_precision: 0.9420 - val_recall: 0.9378\nEpoch 44/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9952 - auc: 0.9998 - loss: 0.0141 - precision: 0.9956 - recall: 0.9950\nEpoch 44: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9998 - loss: 0.0141 - precision: 0.9956 - recall: 0.9950 - val_accuracy: 0.9395 - val_auc: 0.9841 - val_loss: 0.4308 - val_precision: 0.9426 - val_recall: 0.9388\nEpoch 45/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0142 - precision: 0.9959 - recall: 0.9952\nEpoch 45: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0142 - precision: 0.9959 - recall: 0.9952 - val_accuracy: 0.9394 - val_auc: 0.9852 - val_loss: 0.3882 - val_precision: 0.9437 - val_recall: 0.9377\nEpoch 46/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0105 - precision: 0.9969 - recall: 0.9964\nEpoch 46: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0106 - precision: 0.9969 - recall: 0.9964 - val_accuracy: 0.9247 - val_auc: 0.9812 - val_loss: 0.4842 - val_precision: 0.9289 - val_recall: 0.9216\nEpoch 47/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - auc: 0.9999 - loss: 0.0135 - precision: 0.9960 - recall: 0.9954\nEpoch 47: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9958 - auc: 0.9999 - loss: 0.0135 - precision: 0.9960 - recall: 0.9954 - val_accuracy: 0.9384 - val_auc: 0.9851 - val_loss: 0.4225 - val_precision: 0.9407 - val_recall: 0.9373\nEpoch 48/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9998 - loss: 0.0147 - precision: 0.9957 - recall: 0.9953\nEpoch 48: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9998 - loss: 0.0147 - precision: 0.9957 - recall: 0.9953 - val_accuracy: 0.9348 - val_auc: 0.9821 - val_loss: 0.4730 - val_precision: 0.9384 - val_recall: 0.9335\nEpoch 49/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0126 - precision: 0.9958 - recall: 0.9953\nEpoch 49: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0126 - precision: 0.9958 - recall: 0.9953 - val_accuracy: 0.9339 - val_auc: 0.9823 - val_loss: 0.4832 - val_precision: 0.9360 - val_recall: 0.9325\nEpoch 50/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9998 - loss: 0.0139 - precision: 0.9959 - recall: 0.9952\nEpoch 50: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9998 - loss: 0.0139 - precision: 0.9959 - recall: 0.9952 - val_accuracy: 0.9393 - val_auc: 0.9861 - val_loss: 0.3913 - val_precision: 0.9432 - val_recall: 0.9378\nEpoch 51/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0091 - precision: 0.9970 - recall: 0.9968\nEpoch 51: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0091 - precision: 0.9970 - recall: 0.9968 - val_accuracy: 0.9404 - val_auc: 0.9847 - val_loss: 0.4280 - val_precision: 0.9428 - val_recall: 0.9392\nEpoch 52/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9998 - loss: 0.0121 - precision: 0.9964 - recall: 0.9962\nEpoch 52: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9998 - loss: 0.0121 - precision: 0.9964 - recall: 0.9962 - val_accuracy: 0.9402 - val_auc: 0.9849 - val_loss: 0.4214 - val_precision: 0.9431 - val_recall: 0.9386\nEpoch 53/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0114 - precision: 0.9967 - recall: 0.9963\nEpoch 53: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0114 - precision: 0.9967 - recall: 0.9963 - val_accuracy: 0.9386 - val_auc: 0.9847 - val_loss: 0.4340 - val_precision: 0.9419 - val_recall: 0.9374\nEpoch 54/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0103 - precision: 0.9966 - recall: 0.9963\nEpoch 54: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0103 - precision: 0.9966 - recall: 0.9963 - val_accuracy: 0.9412 - val_auc: 0.9854 - val_loss: 0.4111 - val_precision: 0.9445 - val_recall: 0.9396\nEpoch 55/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0110 - precision: 0.9971 - recall: 0.9966\nEpoch 55: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0110 - precision: 0.9971 - recall: 0.9966 - val_accuracy: 0.9397 - val_auc: 0.9844 - val_loss: 0.4348 - val_precision: 0.9426 - val_recall: 0.9378\nEpoch 56/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0104 - precision: 0.9969 - recall: 0.9965\nEpoch 56: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0104 - precision: 0.9969 - recall: 0.9965 - val_accuracy: 0.9423 - val_auc: 0.9839 - val_loss: 0.4502 - val_precision: 0.9445 - val_recall: 0.9414\nEpoch 57/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0081 - precision: 0.9971 - recall: 0.9969\nEpoch 57: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 1.0000 - loss: 0.0081 - precision: 0.9971 - recall: 0.9969 - val_accuracy: 0.9406 - val_auc: 0.9841 - val_loss: 0.4364 - val_precision: 0.9430 - val_recall: 0.9401\nEpoch 58/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - auc: 0.9998 - loss: 0.0126 - precision: 0.9960 - recall: 0.9958\nEpoch 58: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9958 - auc: 0.9998 - loss: 0.0126 - precision: 0.9960 - recall: 0.9958 - val_accuracy: 0.9324 - val_auc: 0.9828 - val_loss: 0.4825 - val_precision: 0.9352 - val_recall: 0.9309\nEpoch 59/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0080 - precision: 0.9974 - recall: 0.9972\nEpoch 59: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0080 - precision: 0.9974 - recall: 0.9972 - val_accuracy: 0.9423 - val_auc: 0.9831 - val_loss: 0.4879 - val_precision: 0.9443 - val_recall: 0.9411\nEpoch 60/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9999 - loss: 0.0118 - precision: 0.9964 - recall: 0.9961\nEpoch 60: val_loss did not improve from 0.22092\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9999 - loss: 0.0118 - precision: 0.9964 - recall: 0.9961 - val_accuracy: 0.9386 - val_auc: 0.9826 - val_loss: 0.4965 - val_precision: 0.9405 - val_recall: 0.9374\n\n\n Training with Sigmoid activation function\nEpoch 1/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1760 - auc: 0.5944 - loss: 2.2650 - precision: 0.0000e+00 - recall: 0.0000e+00\nEpoch 1: val_loss improved from inf to 2.24763, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 17ms/step - accuracy: 0.1760 - auc: 0.5944 - loss: 2.2650 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.1942 - val_auc: 0.5975 - val_loss: 2.2476 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\nEpoch 2/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1984 - auc: 0.6201 - loss: 2.2119 - precision: 0.3634 - recall: 0.0169\nEpoch 2: val_loss did not improve from 2.24763\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.1986 - auc: 0.6204 - loss: 2.2114 - precision: 0.3644 - recall: 0.0171 - val_accuracy: 0.1942 - val_auc: 0.5620 - val_loss: 4.9647 - val_precision: 0.1942 - val_recall: 0.1942\nEpoch 3/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6585 - auc: 0.9360 - loss: 1.0386 - precision: 0.8118 - recall: 0.5046\nEpoch 3: val_loss improved from 2.24763 to 2.10244, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.6586 - auc: 0.9360 - loss: 1.0383 - precision: 0.8118 - recall: 0.5048 - val_accuracy: 0.4395 - val_auc: 0.8229 - val_loss: 2.1024 - val_precision: 0.5007 - val_recall: 0.4051\nEpoch 4/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8390 - auc: 0.9808 - loss: 0.5277 - precision: 0.8928 - recall: 0.7988\nEpoch 4: val_loss improved from 2.10244 to 1.80406, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.8390 - auc: 0.9808 - loss: 0.5276 - precision: 0.8929 - recall: 0.7989 - val_accuracy: 0.5344 - val_auc: 0.8622 - val_loss: 1.8041 - val_precision: 0.5870 - val_recall: 0.4882\nEpoch 5/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8762 - auc: 0.9874 - loss: 0.4116 - precision: 0.9157 - recall: 0.8484\nEpoch 5: val_loss improved from 1.80406 to 0.69704, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8762 - auc: 0.9874 - loss: 0.4115 - precision: 0.9158 - recall: 0.8484 - val_accuracy: 0.7764 - val_auc: 0.9702 - val_loss: 0.6970 - val_precision: 0.8148 - val_recall: 0.7499\nEpoch 6/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8905 - auc: 0.9896 - loss: 0.3636 - precision: 0.9262 - recall: 0.8652\nEpoch 6: val_loss did not improve from 0.69704\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.8905 - auc: 0.9896 - loss: 0.3635 - precision: 0.9262 - recall: 0.8652 - val_accuracy: 0.7412 - val_auc: 0.9451 - val_loss: 0.9677 - val_precision: 0.7972 - val_recall: 0.7090\nEpoch 7/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9045 - auc: 0.9914 - loss: 0.3186 - precision: 0.9330 - recall: 0.8828\nEpoch 7: val_loss improved from 0.69704 to 0.54264, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9045 - auc: 0.9914 - loss: 0.3186 - precision: 0.9330 - recall: 0.8828 - val_accuracy: 0.8516 - val_auc: 0.9771 - val_loss: 0.5426 - val_precision: 0.8882 - val_recall: 0.8318\nEpoch 8/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9126 - auc: 0.9921 - loss: 0.2972 - precision: 0.9386 - recall: 0.8929\nEpoch 8: val_loss did not improve from 0.54264\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9126 - auc: 0.9921 - loss: 0.2972 - precision: 0.9386 - recall: 0.8929 - val_accuracy: 0.7262 - val_auc: 0.9590 - val_loss: 0.8498 - val_precision: 0.7613 - val_recall: 0.6990\nEpoch 9/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9175 - auc: 0.9930 - loss: 0.2758 - precision: 0.9449 - recall: 0.8995\nEpoch 9: val_loss did not improve from 0.54264\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9175 - auc: 0.9930 - loss: 0.2758 - precision: 0.9449 - recall: 0.8995 - val_accuracy: 0.8264 - val_auc: 0.9717 - val_loss: 0.6348 - val_precision: 0.8546 - val_recall: 0.8091\nEpoch 10/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9263 - auc: 0.9938 - loss: 0.2507 - precision: 0.9483 - recall: 0.9099\nEpoch 10: val_loss did not improve from 0.54264\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9263 - auc: 0.9938 - loss: 0.2508 - precision: 0.9483 - recall: 0.9099 - val_accuracy: 0.7679 - val_auc: 0.9599 - val_loss: 0.7996 - val_precision: 0.8122 - val_recall: 0.7389\nEpoch 11/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9276 - auc: 0.9937 - loss: 0.2505 - precision: 0.9489 - recall: 0.9102\nEpoch 11: val_loss did not improve from 0.54264\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9276 - auc: 0.9937 - loss: 0.2505 - precision: 0.9489 - recall: 0.9102 - val_accuracy: 0.7815 - val_auc: 0.9652 - val_loss: 0.7543 - val_precision: 0.8162 - val_recall: 0.7585\nEpoch 12/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9303 - auc: 0.9949 - loss: 0.2293 - precision: 0.9519 - recall: 0.9170\nEpoch 12: val_loss did not improve from 0.54264\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9303 - auc: 0.9949 - loss: 0.2293 - precision: 0.9519 - recall: 0.9170 - val_accuracy: 0.6733 - val_auc: 0.9231 - val_loss: 1.2571 - val_precision: 0.7082 - val_recall: 0.6486\nEpoch 13/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9346 - auc: 0.9949 - loss: 0.2193 - precision: 0.9544 - recall: 0.9207\nEpoch 13: val_loss improved from 0.54264 to 0.53644, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9346 - auc: 0.9949 - loss: 0.2193 - precision: 0.9544 - recall: 0.9207 - val_accuracy: 0.8465 - val_auc: 0.9777 - val_loss: 0.5364 - val_precision: 0.8810 - val_recall: 0.8255\nEpoch 14/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9378 - auc: 0.9952 - loss: 0.2136 - precision: 0.9571 - recall: 0.9242\nEpoch 14: val_loss improved from 0.53644 to 0.34728, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9378 - auc: 0.9952 - loss: 0.2136 - precision: 0.9571 - recall: 0.9242 - val_accuracy: 0.8985 - val_auc: 0.9894 - val_loss: 0.3473 - val_precision: 0.9230 - val_recall: 0.8846\nEpoch 15/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9424 - auc: 0.9958 - loss: 0.1938 - precision: 0.9596 - recall: 0.9310\nEpoch 15: val_loss did not improve from 0.34728\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9424 - auc: 0.9958 - loss: 0.1938 - precision: 0.9596 - recall: 0.9310 - val_accuracy: 0.8706 - val_auc: 0.9797 - val_loss: 0.4925 - val_precision: 0.8884 - val_recall: 0.8608\nEpoch 16/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9445 - auc: 0.9962 - loss: 0.1829 - precision: 0.9614 - recall: 0.9328\nEpoch 16: val_loss did not improve from 0.34728\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9445 - auc: 0.9962 - loss: 0.1829 - precision: 0.9614 - recall: 0.9328 - val_accuracy: 0.8838 - val_auc: 0.9878 - val_loss: 0.3894 - val_precision: 0.9224 - val_recall: 0.8591\nEpoch 17/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9469 - auc: 0.9962 - loss: 0.1813 - precision: 0.9631 - recall: 0.9364\nEpoch 17: val_loss improved from 0.34728 to 0.34392, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9469 - auc: 0.9962 - loss: 0.1813 - precision: 0.9631 - recall: 0.9364 - val_accuracy: 0.8963 - val_auc: 0.9906 - val_loss: 0.3439 - val_precision: 0.9260 - val_recall: 0.8769\nEpoch 18/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9521 - auc: 0.9962 - loss: 0.1684 - precision: 0.9656 - recall: 0.9414\nEpoch 18: val_loss did not improve from 0.34392\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9521 - auc: 0.9962 - loss: 0.1684 - precision: 0.9656 - recall: 0.9414 - val_accuracy: 0.8965 - val_auc: 0.9844 - val_loss: 0.4064 - val_precision: 0.9152 - val_recall: 0.8860\nEpoch 19/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9537 - auc: 0.9965 - loss: 0.1605 - precision: 0.9663 - recall: 0.9431\nEpoch 19: val_loss did not improve from 0.34392\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9537 - auc: 0.9965 - loss: 0.1605 - precision: 0.9663 - recall: 0.9431 - val_accuracy: 0.7914 - val_auc: 0.9608 - val_loss: 0.7998 - val_precision: 0.8106 - val_recall: 0.7798\nEpoch 20/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9576 - auc: 0.9971 - loss: 0.1445 - precision: 0.9697 - recall: 0.9485\nEpoch 20: val_loss did not improve from 0.34392\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9576 - auc: 0.9971 - loss: 0.1446 - precision: 0.9697 - recall: 0.9485 - val_accuracy: 0.8633 - val_auc: 0.9786 - val_loss: 0.5161 - val_precision: 0.8899 - val_recall: 0.8513\nEpoch 21/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9576 - auc: 0.9973 - loss: 0.1443 - precision: 0.9683 - recall: 0.9480\nEpoch 21: val_loss did not improve from 0.34392\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9576 - auc: 0.9973 - loss: 0.1443 - precision: 0.9683 - recall: 0.9480 - val_accuracy: 0.8991 - val_auc: 0.9893 - val_loss: 0.3451 - val_precision: 0.9217 - val_recall: 0.8873\nEpoch 22/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9614 - auc: 0.9975 - loss: 0.1320 - precision: 0.9718 - recall: 0.9536\nEpoch 22: val_loss improved from 0.34392 to 0.33470, saving model to sigmoid.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9614 - auc: 0.9975 - loss: 0.1321 - precision: 0.9718 - recall: 0.9536 - val_accuracy: 0.9113 - val_auc: 0.9884 - val_loss: 0.3347 - val_precision: 0.9277 - val_recall: 0.9023\nEpoch 23/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9635 - auc: 0.9977 - loss: 0.1235 - precision: 0.9727 - recall: 0.9549\nEpoch 23: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9635 - auc: 0.9977 - loss: 0.1235 - precision: 0.9727 - recall: 0.9549 - val_accuracy: 0.9075 - val_auc: 0.9869 - val_loss: 0.3605 - val_precision: 0.9222 - val_recall: 0.9002\nEpoch 24/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9647 - auc: 0.9979 - loss: 0.1166 - precision: 0.9731 - recall: 0.9583\nEpoch 24: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9647 - auc: 0.9979 - loss: 0.1166 - precision: 0.9731 - recall: 0.9583 - val_accuracy: 0.8967 - val_auc: 0.9849 - val_loss: 0.3946 - val_precision: 0.9129 - val_recall: 0.8881\nEpoch 25/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9979 - loss: 0.1121 - precision: 0.9746 - recall: 0.9608\nEpoch 25: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9979 - loss: 0.1121 - precision: 0.9746 - recall: 0.9608 - val_accuracy: 0.8948 - val_auc: 0.9845 - val_loss: 0.4084 - val_precision: 0.9115 - val_recall: 0.8862\nEpoch 26/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9982 - loss: 0.1078 - precision: 0.9752 - recall: 0.9609\nEpoch 26: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9982 - loss: 0.1078 - precision: 0.9752 - recall: 0.9609 - val_accuracy: 0.7757 - val_auc: 0.9537 - val_loss: 0.8808 - val_precision: 0.8286 - val_recall: 0.7551\nEpoch 27/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9711 - auc: 0.9983 - loss: 0.0963 - precision: 0.9775 - recall: 0.9653\nEpoch 27: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9711 - auc: 0.9983 - loss: 0.0963 - precision: 0.9775 - recall: 0.9653 - val_accuracy: 0.8952 - val_auc: 0.9848 - val_loss: 0.4087 - val_precision: 0.9120 - val_recall: 0.8860\nEpoch 28/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9734 - auc: 0.9987 - loss: 0.0877 - precision: 0.9788 - recall: 0.9689\nEpoch 28: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9734 - auc: 0.9987 - loss: 0.0877 - precision: 0.9788 - recall: 0.9689 - val_accuracy: 0.9010 - val_auc: 0.9858 - val_loss: 0.3843 - val_precision: 0.9177 - val_recall: 0.8924\nEpoch 29/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9747 - auc: 0.9986 - loss: 0.0847 - precision: 0.9803 - recall: 0.9700\nEpoch 29: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9747 - auc: 0.9986 - loss: 0.0847 - precision: 0.9803 - recall: 0.9700 - val_accuracy: 0.8758 - val_auc: 0.9793 - val_loss: 0.4935 - val_precision: 0.8920 - val_recall: 0.8684\nEpoch 30/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9769 - auc: 0.9988 - loss: 0.0762 - precision: 0.9813 - recall: 0.9735\nEpoch 30: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9769 - auc: 0.9988 - loss: 0.0762 - precision: 0.9813 - recall: 0.9735 - val_accuracy: 0.8950 - val_auc: 0.9839 - val_loss: 0.4151 - val_precision: 0.9140 - val_recall: 0.8863\nEpoch 31/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9779 - auc: 0.9990 - loss: 0.0717 - precision: 0.9824 - recall: 0.9749\nEpoch 31: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9779 - auc: 0.9990 - loss: 0.0717 - precision: 0.9824 - recall: 0.9749 - val_accuracy: 0.8909 - val_auc: 0.9789 - val_loss: 0.4896 - val_precision: 0.9039 - val_recall: 0.8829\nEpoch 32/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9786 - auc: 0.9992 - loss: 0.0702 - precision: 0.9829 - recall: 0.9755\nEpoch 32: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9786 - auc: 0.9992 - loss: 0.0702 - precision: 0.9829 - recall: 0.9755 - val_accuracy: 0.9011 - val_auc: 0.9828 - val_loss: 0.4269 - val_precision: 0.9111 - val_recall: 0.8965\nEpoch 33/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9797 - auc: 0.9992 - loss: 0.0643 - precision: 0.9835 - recall: 0.9771\nEpoch 33: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9797 - auc: 0.9992 - loss: 0.0644 - precision: 0.9835 - recall: 0.9771 - val_accuracy: 0.9049 - val_auc: 0.9824 - val_loss: 0.4311 - val_precision: 0.9150 - val_recall: 0.9006\nEpoch 34/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9820 - auc: 0.9993 - loss: 0.0573 - precision: 0.9852 - recall: 0.9796\nEpoch 34: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9820 - auc: 0.9993 - loss: 0.0573 - precision: 0.9852 - recall: 0.9796 - val_accuracy: 0.8738 - val_auc: 0.9733 - val_loss: 0.5944 - val_precision: 0.8873 - val_recall: 0.8675\nEpoch 35/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0598 - precision: 0.9842 - recall: 0.9788\nEpoch 35: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0598 - precision: 0.9842 - recall: 0.9788 - val_accuracy: 0.9009 - val_auc: 0.9799 - val_loss: 0.4889 - val_precision: 0.9110 - val_recall: 0.8965\nEpoch 36/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9823 - auc: 0.9994 - loss: 0.0541 - precision: 0.9852 - recall: 0.9801\nEpoch 36: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9823 - auc: 0.9994 - loss: 0.0541 - precision: 0.9851 - recall: 0.9801 - val_accuracy: 0.9162 - val_auc: 0.9834 - val_loss: 0.3880 - val_precision: 0.9249 - val_recall: 0.9127\nEpoch 37/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - auc: 0.9994 - loss: 0.0480 - precision: 0.9876 - recall: 0.9831\nEpoch 37: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9849 - auc: 0.9994 - loss: 0.0480 - precision: 0.9876 - recall: 0.9831 - val_accuracy: 0.8648 - val_auc: 0.9671 - val_loss: 0.7172 - val_precision: 0.8748 - val_recall: 0.8605\nEpoch 38/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9844 - auc: 0.9996 - loss: 0.0482 - precision: 0.9867 - recall: 0.9821\nEpoch 38: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9844 - auc: 0.9996 - loss: 0.0482 - precision: 0.9867 - recall: 0.9821 - val_accuracy: 0.9096 - val_auc: 0.9816 - val_loss: 0.4366 - val_precision: 0.9169 - val_recall: 0.9068\nEpoch 39/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - auc: 0.9996 - loss: 0.0454 - precision: 0.9874 - recall: 0.9834\nEpoch 39: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9849 - auc: 0.9996 - loss: 0.0455 - precision: 0.9874 - recall: 0.9833 - val_accuracy: 0.8774 - val_auc: 0.9724 - val_loss: 0.6119 - val_precision: 0.8867 - val_recall: 0.8730\nEpoch 40/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9850 - auc: 0.9996 - loss: 0.0468 - precision: 0.9873 - recall: 0.9831\nEpoch 40: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9850 - auc: 0.9996 - loss: 0.0468 - precision: 0.9873 - recall: 0.9831 - val_accuracy: 0.9096 - val_auc: 0.9824 - val_loss: 0.4262 - val_precision: 0.9190 - val_recall: 0.9051\nEpoch 41/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - auc: 0.9997 - loss: 0.0436 - precision: 0.9875 - recall: 0.9842\nEpoch 41: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9856 - auc: 0.9997 - loss: 0.0436 - precision: 0.9875 - recall: 0.9842 - val_accuracy: 0.9048 - val_auc: 0.9803 - val_loss: 0.4675 - val_precision: 0.9126 - val_recall: 0.9021\nEpoch 42/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9881 - auc: 0.9997 - loss: 0.0372 - precision: 0.9896 - recall: 0.9866\nEpoch 42: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9881 - auc: 0.9997 - loss: 0.0373 - precision: 0.9896 - recall: 0.9866 - val_accuracy: 0.9128 - val_auc: 0.9818 - val_loss: 0.4261 - val_precision: 0.9224 - val_recall: 0.9082\nEpoch 43/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9863 - auc: 0.9997 - loss: 0.0412 - precision: 0.9877 - recall: 0.9849\nEpoch 43: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9863 - auc: 0.9997 - loss: 0.0412 - precision: 0.9877 - recall: 0.9849 - val_accuracy: 0.9032 - val_auc: 0.9793 - val_loss: 0.4817 - val_precision: 0.9122 - val_recall: 0.8998\nEpoch 44/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9868 - auc: 0.9997 - loss: 0.0384 - precision: 0.9886 - recall: 0.9854\nEpoch 44: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9868 - auc: 0.9997 - loss: 0.0384 - precision: 0.9886 - recall: 0.9854 - val_accuracy: 0.8929 - val_auc: 0.9758 - val_loss: 0.5593 - val_precision: 0.9006 - val_recall: 0.8888\nEpoch 45/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9876 - auc: 0.9998 - loss: 0.0364 - precision: 0.9890 - recall: 0.9866\nEpoch 45: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9876 - auc: 0.9998 - loss: 0.0364 - precision: 0.9890 - recall: 0.9866 - val_accuracy: 0.8885 - val_auc: 0.9749 - val_loss: 0.5791 - val_precision: 0.8979 - val_recall: 0.8832\nEpoch 46/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9852 - auc: 0.9996 - loss: 0.0420 - precision: 0.9866 - recall: 0.9840\nEpoch 46: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9852 - auc: 0.9996 - loss: 0.0420 - precision: 0.9866 - recall: 0.9840 - val_accuracy: 0.8989 - val_auc: 0.9772 - val_loss: 0.5279 - val_precision: 0.9073 - val_recall: 0.8954\nEpoch 47/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9889 - auc: 0.9997 - loss: 0.0333 - precision: 0.9901 - recall: 0.9879\nEpoch 47: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9889 - auc: 0.9997 - loss: 0.0333 - precision: 0.9901 - recall: 0.9879 - val_accuracy: 0.9166 - val_auc: 0.9815 - val_loss: 0.4410 - val_precision: 0.9226 - val_recall: 0.9148\nEpoch 48/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9908 - auc: 0.9998 - loss: 0.0278 - precision: 0.9919 - recall: 0.9899\nEpoch 48: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9908 - auc: 0.9998 - loss: 0.0278 - precision: 0.9919 - recall: 0.9899 - val_accuracy: 0.8974 - val_auc: 0.9774 - val_loss: 0.5407 - val_precision: 0.9060 - val_recall: 0.8925\nEpoch 49/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9997 - loss: 0.0314 - precision: 0.9910 - recall: 0.9889\nEpoch 49: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9997 - loss: 0.0314 - precision: 0.9910 - recall: 0.9889 - val_accuracy: 0.9128 - val_auc: 0.9798 - val_loss: 0.4784 - val_precision: 0.9179 - val_recall: 0.9105\nEpoch 50/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9887 - auc: 0.9996 - loss: 0.0320 - precision: 0.9900 - recall: 0.9878\nEpoch 50: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9887 - auc: 0.9996 - loss: 0.0320 - precision: 0.9900 - recall: 0.9878 - val_accuracy: 0.8645 - val_auc: 0.9668 - val_loss: 0.7308 - val_precision: 0.8720 - val_recall: 0.8600\nEpoch 51/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9897 - auc: 0.9997 - loss: 0.0306 - precision: 0.9908 - recall: 0.9889\nEpoch 51: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9897 - auc: 0.9997 - loss: 0.0307 - precision: 0.9908 - recall: 0.9889 - val_accuracy: 0.8939 - val_auc: 0.9744 - val_loss: 0.5862 - val_precision: 0.9008 - val_recall: 0.8903\nEpoch 52/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9999 - loss: 0.0237 - precision: 0.9930 - recall: 0.9915\nEpoch 52: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9999 - loss: 0.0238 - precision: 0.9930 - recall: 0.9915 - val_accuracy: 0.8961 - val_auc: 0.9752 - val_loss: 0.5738 - val_precision: 0.9033 - val_recall: 0.8933\nEpoch 53/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9892 - auc: 0.9998 - loss: 0.0294 - precision: 0.9900 - recall: 0.9884\nEpoch 53: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9892 - auc: 0.9998 - loss: 0.0294 - precision: 0.9900 - recall: 0.9884 - val_accuracy: 0.9050 - val_auc: 0.9774 - val_loss: 0.5241 - val_precision: 0.9113 - val_recall: 0.9018\nEpoch 54/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9998 - loss: 0.0285 - precision: 0.9909 - recall: 0.9889\nEpoch 54: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9998 - loss: 0.0285 - precision: 0.9909 - recall: 0.9889 - val_accuracy: 0.9073 - val_auc: 0.9790 - val_loss: 0.4975 - val_precision: 0.9142 - val_recall: 0.9036\nEpoch 55/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0237 - precision: 0.9931 - recall: 0.9914\nEpoch 55: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0237 - precision: 0.9931 - recall: 0.9914 - val_accuracy: 0.9002 - val_auc: 0.9749 - val_loss: 0.5809 - val_precision: 0.9066 - val_recall: 0.8973\nEpoch 56/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9917 - auc: 0.9998 - loss: 0.0255 - precision: 0.9926 - recall: 0.9910\nEpoch 56: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9917 - auc: 0.9998 - loss: 0.0255 - precision: 0.9926 - recall: 0.9910 - val_accuracy: 0.8943 - val_auc: 0.9736 - val_loss: 0.6150 - val_precision: 0.9000 - val_recall: 0.8921\nEpoch 57/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9896 - auc: 0.9997 - loss: 0.0301 - precision: 0.9909 - recall: 0.9888\nEpoch 57: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9896 - auc: 0.9997 - loss: 0.0301 - precision: 0.9909 - recall: 0.9888 - val_accuracy: 0.9118 - val_auc: 0.9780 - val_loss: 0.5179 - val_precision: 0.9166 - val_recall: 0.9092\nEpoch 58/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9909 - auc: 0.9998 - loss: 0.0265 - precision: 0.9917 - recall: 0.9900\nEpoch 58: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9909 - auc: 0.9998 - loss: 0.0265 - precision: 0.9917 - recall: 0.9900 - val_accuracy: 0.9166 - val_auc: 0.9795 - val_loss: 0.4896 - val_precision: 0.9220 - val_recall: 0.9152\nEpoch 59/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0239 - precision: 0.9931 - recall: 0.9918\nEpoch 59: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9922 - auc: 0.9998 - loss: 0.0239 - precision: 0.9931 - recall: 0.9918 - val_accuracy: 0.8890 - val_auc: 0.9740 - val_loss: 0.5981 - val_precision: 0.8976 - val_recall: 0.8846\nEpoch 60/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9921 - auc: 0.9999 - loss: 0.0231 - precision: 0.9925 - recall: 0.9914\nEpoch 60: val_loss did not improve from 0.33470\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9921 - auc: 0.9999 - loss: 0.0231 - precision: 0.9925 - recall: 0.9914 - val_accuracy: 0.9067 - val_auc: 0.9757 - val_loss: 0.5755 - val_precision: 0.9123 - val_recall: 0.9050\n\n\n Training with Tanh activation function\nEpoch 1/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1879 - auc: 0.6039 - loss: 2.2406 - precision: 0.3920 - recall: 0.0108\nEpoch 1: val_loss improved from inf to 1.39132, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 17ms/step - accuracy: 0.1880 - auc: 0.6040 - loss: 2.2405 - precision: 0.3922 - recall: 0.0108 - val_accuracy: 0.5588 - val_auc: 0.8957 - val_loss: 1.3913 - val_precision: 0.6662 - val_recall: 0.4659\nEpoch 2/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7954 - auc: 0.9728 - loss: 0.6506 - precision: 0.8715 - recall: 0.7333\nEpoch 2: val_loss improved from 1.39132 to 1.30070, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.7955 - auc: 0.9728 - loss: 0.6504 - precision: 0.8715 - recall: 0.7334 - val_accuracy: 0.6723 - val_auc: 0.9146 - val_loss: 1.3007 - val_precision: 0.7184 - val_recall: 0.6405\nEpoch 3/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8895 - auc: 0.9894 - loss: 0.3691 - precision: 0.9243 - recall: 0.8644\nEpoch 3: val_loss improved from 1.30070 to 0.57193, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.8895 - auc: 0.9894 - loss: 0.3691 - precision: 0.9243 - recall: 0.8644 - val_accuracy: 0.8304 - val_auc: 0.9769 - val_loss: 0.5719 - val_precision: 0.8711 - val_recall: 0.8069\nEpoch 4/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9101 - auc: 0.9917 - loss: 0.3023 - precision: 0.9369 - recall: 0.8894\nEpoch 4: val_loss improved from 0.57193 to 0.49525, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9101 - auc: 0.9917 - loss: 0.3023 - precision: 0.9369 - recall: 0.8894 - val_accuracy: 0.8582 - val_auc: 0.9810 - val_loss: 0.4953 - val_precision: 0.8950 - val_recall: 0.8350\nEpoch 5/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9239 - auc: 0.9939 - loss: 0.2554 - precision: 0.9473 - recall: 0.9083\nEpoch 5: val_loss improved from 0.49525 to 0.40371, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9239 - auc: 0.9939 - loss: 0.2554 - precision: 0.9473 - recall: 0.9083 - val_accuracy: 0.8844 - val_auc: 0.9865 - val_loss: 0.4037 - val_precision: 0.9208 - val_recall: 0.8590\nEpoch 6/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9320 - auc: 0.9947 - loss: 0.2277 - precision: 0.9527 - recall: 0.9183\nEpoch 6: val_loss did not improve from 0.40371\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9320 - auc: 0.9947 - loss: 0.2277 - precision: 0.9527 - recall: 0.9183 - val_accuracy: 0.8563 - val_auc: 0.9788 - val_loss: 0.5226 - val_precision: 0.8876 - val_recall: 0.8374\nEpoch 7/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9428 - auc: 0.9956 - loss: 0.1993 - precision: 0.9594 - recall: 0.9303\nEpoch 7: val_loss did not improve from 0.40371\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9428 - auc: 0.9956 - loss: 0.1993 - precision: 0.9594 - recall: 0.9303 - val_accuracy: 0.8544 - val_auc: 0.9771 - val_loss: 0.5388 - val_precision: 0.8850 - val_recall: 0.8360\nEpoch 8/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9490 - auc: 0.9963 - loss: 0.1776 - precision: 0.9628 - recall: 0.9376\nEpoch 8: val_loss did not improve from 0.40371\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9490 - auc: 0.9963 - loss: 0.1776 - precision: 0.9628 - recall: 0.9376 - val_accuracy: 0.8694 - val_auc: 0.9798 - val_loss: 0.4896 - val_precision: 0.9022 - val_recall: 0.8526\nEpoch 9/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9545 - auc: 0.9967 - loss: 0.1561 - precision: 0.9660 - recall: 0.9455\nEpoch 9: val_loss did not improve from 0.40371\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9545 - auc: 0.9967 - loss: 0.1561 - precision: 0.9660 - recall: 0.9455 - val_accuracy: 0.8857 - val_auc: 0.9839 - val_loss: 0.4293 - val_precision: 0.9058 - val_recall: 0.8752\nEpoch 10/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9585 - auc: 0.9975 - loss: 0.1379 - precision: 0.9693 - recall: 0.9514\nEpoch 10: val_loss improved from 0.40371 to 0.37373, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9585 - auc: 0.9975 - loss: 0.1379 - precision: 0.9693 - recall: 0.9514 - val_accuracy: 0.8952 - val_auc: 0.9874 - val_loss: 0.3737 - val_precision: 0.9172 - val_recall: 0.8805\nEpoch 11/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9643 - auc: 0.9975 - loss: 0.1239 - precision: 0.9733 - recall: 0.9582\nEpoch 11: val_loss improved from 0.37373 to 0.35138, saving model to tanh.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9643 - auc: 0.9975 - loss: 0.1239 - precision: 0.9733 - recall: 0.9582 - val_accuracy: 0.9081 - val_auc: 0.9875 - val_loss: 0.3514 - val_precision: 0.9254 - val_recall: 0.8997\nEpoch 12/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9676 - auc: 0.9979 - loss: 0.1104 - precision: 0.9745 - recall: 0.9620\nEpoch 12: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9676 - auc: 0.9979 - loss: 0.1105 - precision: 0.9745 - recall: 0.9620 - val_accuracy: 0.8978 - val_auc: 0.9866 - val_loss: 0.3808 - val_precision: 0.9113 - val_recall: 0.8884\nEpoch 13/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9720 - auc: 0.9985 - loss: 0.0944 - precision: 0.9785 - recall: 0.9674\nEpoch 13: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9720 - auc: 0.9985 - loss: 0.0944 - precision: 0.9785 - recall: 0.9673 - val_accuracy: 0.8774 - val_auc: 0.9808 - val_loss: 0.4750 - val_precision: 0.8968 - val_recall: 0.8668\nEpoch 14/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9738 - auc: 0.9985 - loss: 0.0887 - precision: 0.9792 - recall: 0.9699\nEpoch 14: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9738 - auc: 0.9985 - loss: 0.0888 - precision: 0.9792 - recall: 0.9699 - val_accuracy: 0.9029 - val_auc: 0.9836 - val_loss: 0.4107 - val_precision: 0.9163 - val_recall: 0.8979\nEpoch 15/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9767 - auc: 0.9987 - loss: 0.0791 - precision: 0.9811 - recall: 0.9736\nEpoch 15: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9767 - auc: 0.9987 - loss: 0.0791 - precision: 0.9811 - recall: 0.9735 - val_accuracy: 0.8960 - val_auc: 0.9805 - val_loss: 0.4686 - val_precision: 0.9061 - val_recall: 0.8901\nEpoch 16/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9797 - auc: 0.9990 - loss: 0.0675 - precision: 0.9832 - recall: 0.9761\nEpoch 16: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9797 - auc: 0.9990 - loss: 0.0676 - precision: 0.9832 - recall: 0.9761 - val_accuracy: 0.8636 - val_auc: 0.9722 - val_loss: 0.6274 - val_precision: 0.8818 - val_recall: 0.8550\nEpoch 17/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9820 - auc: 0.9992 - loss: 0.0607 - precision: 0.9854 - recall: 0.9793\nEpoch 17: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9820 - auc: 0.9992 - loss: 0.0607 - precision: 0.9854 - recall: 0.9793 - val_accuracy: 0.8544 - val_auc: 0.9706 - val_loss: 0.6618 - val_precision: 0.8735 - val_recall: 0.8470\nEpoch 18/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9824 - auc: 0.9994 - loss: 0.0557 - precision: 0.9851 - recall: 0.9801\nEpoch 18: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9824 - auc: 0.9994 - loss: 0.0557 - precision: 0.9851 - recall: 0.9801 - val_accuracy: 0.7829 - val_auc: 0.9476 - val_loss: 1.0616 - val_precision: 0.7951 - val_recall: 0.7740\nEpoch 19/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9840 - auc: 0.9994 - loss: 0.0523 - precision: 0.9858 - recall: 0.9820\nEpoch 19: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9840 - auc: 0.9994 - loss: 0.0523 - precision: 0.9858 - recall: 0.9819 - val_accuracy: 0.8899 - val_auc: 0.9806 - val_loss: 0.4887 - val_precision: 0.9005 - val_recall: 0.8847\nEpoch 20/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9827 - auc: 0.9996 - loss: 0.0518 - precision: 0.9855 - recall: 0.9806\nEpoch 20: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9827 - auc: 0.9996 - loss: 0.0518 - precision: 0.9855 - recall: 0.9806 - val_accuracy: 0.8670 - val_auc: 0.9715 - val_loss: 0.6486 - val_precision: 0.8720 - val_recall: 0.8635\nEpoch 21/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - auc: 0.9996 - loss: 0.0440 - precision: 0.9876 - recall: 0.9836\nEpoch 21: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9857 - auc: 0.9996 - loss: 0.0440 - precision: 0.9876 - recall: 0.9836 - val_accuracy: 0.8855 - val_auc: 0.9743 - val_loss: 0.5906 - val_precision: 0.8944 - val_recall: 0.8812\nEpoch 22/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9861 - auc: 0.9997 - loss: 0.0403 - precision: 0.9879 - recall: 0.9849\nEpoch 22: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9861 - auc: 0.9997 - loss: 0.0403 - precision: 0.9879 - recall: 0.9849 - val_accuracy: 0.9113 - val_auc: 0.9815 - val_loss: 0.4419 - val_precision: 0.9200 - val_recall: 0.9092\nEpoch 23/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9881 - auc: 0.9997 - loss: 0.0362 - precision: 0.9898 - recall: 0.9866\nEpoch 23: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9881 - auc: 0.9997 - loss: 0.0362 - precision: 0.9898 - recall: 0.9866 - val_accuracy: 0.8991 - val_auc: 0.9769 - val_loss: 0.5410 - val_precision: 0.9062 - val_recall: 0.8946\nEpoch 24/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9883 - auc: 0.9997 - loss: 0.0357 - precision: 0.9895 - recall: 0.9869\nEpoch 24: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9883 - auc: 0.9997 - loss: 0.0357 - precision: 0.9895 - recall: 0.9869 - val_accuracy: 0.8866 - val_auc: 0.9759 - val_loss: 0.5683 - val_precision: 0.8939 - val_recall: 0.8823\nEpoch 25/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9884 - auc: 0.9997 - loss: 0.0346 - precision: 0.9901 - recall: 0.9874\nEpoch 25: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9884 - auc: 0.9997 - loss: 0.0346 - precision: 0.9901 - recall: 0.9874 - val_accuracy: 0.7125 - val_auc: 0.9118 - val_loss: 1.8048 - val_precision: 0.7215 - val_recall: 0.7067\nEpoch 26/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9880 - auc: 0.9997 - loss: 0.0352 - precision: 0.9891 - recall: 0.9871\nEpoch 26: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9880 - auc: 0.9997 - loss: 0.0352 - precision: 0.9891 - recall: 0.9871 - val_accuracy: 0.9073 - val_auc: 0.9796 - val_loss: 0.4847 - val_precision: 0.9147 - val_recall: 0.9049\nEpoch 27/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9901 - auc: 0.9998 - loss: 0.0298 - precision: 0.9913 - recall: 0.9892\nEpoch 27: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9901 - auc: 0.9998 - loss: 0.0299 - precision: 0.9913 - recall: 0.9892 - val_accuracy: 0.8928 - val_auc: 0.9778 - val_loss: 0.5496 - val_precision: 0.8991 - val_recall: 0.8894\nEpoch 28/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9878 - auc: 0.9997 - loss: 0.0359 - precision: 0.9889 - recall: 0.9869\nEpoch 28: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9878 - auc: 0.9997 - loss: 0.0359 - precision: 0.9889 - recall: 0.9869 - val_accuracy: 0.8608 - val_auc: 0.9655 - val_loss: 0.7948 - val_precision: 0.8665 - val_recall: 0.8582\nEpoch 29/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9907 - auc: 0.9998 - loss: 0.0275 - precision: 0.9917 - recall: 0.9899\nEpoch 29: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9907 - auc: 0.9998 - loss: 0.0275 - precision: 0.9916 - recall: 0.9899 - val_accuracy: 0.8928 - val_auc: 0.9738 - val_loss: 0.6104 - val_precision: 0.8988 - val_recall: 0.8889\nEpoch 30/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0256 - precision: 0.9922 - recall: 0.9905\nEpoch 30: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0256 - precision: 0.9922 - recall: 0.9905 - val_accuracy: 0.9145 - val_auc: 0.9804 - val_loss: 0.4856 - val_precision: 0.9188 - val_recall: 0.9109\nEpoch 31/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9903 - auc: 0.9998 - loss: 0.0280 - precision: 0.9912 - recall: 0.9896\nEpoch 31: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9903 - auc: 0.9998 - loss: 0.0280 - precision: 0.9912 - recall: 0.9896 - val_accuracy: 0.9137 - val_auc: 0.9799 - val_loss: 0.4859 - val_precision: 0.9187 - val_recall: 0.9111\nEpoch 32/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9915 - auc: 0.9998 - loss: 0.0255 - precision: 0.9924 - recall: 0.9910\nEpoch 32: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9915 - auc: 0.9998 - loss: 0.0255 - precision: 0.9924 - recall: 0.9910 - val_accuracy: 0.8928 - val_auc: 0.9728 - val_loss: 0.6267 - val_precision: 0.8982 - val_recall: 0.8905\nEpoch 33/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9911 - auc: 0.9999 - loss: 0.0252 - precision: 0.9918 - recall: 0.9904\nEpoch 33: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9911 - auc: 0.9999 - loss: 0.0252 - precision: 0.9918 - recall: 0.9904 - val_accuracy: 0.8956 - val_auc: 0.9777 - val_loss: 0.5345 - val_precision: 0.9042 - val_recall: 0.8893\nEpoch 34/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9931 - auc: 0.9999 - loss: 0.0216 - precision: 0.9939 - recall: 0.9926\nEpoch 34: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9931 - auc: 0.9999 - loss: 0.0216 - precision: 0.9939 - recall: 0.9926 - val_accuracy: 0.9173 - val_auc: 0.9802 - val_loss: 0.4709 - val_precision: 0.9221 - val_recall: 0.9152\nEpoch 35/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0255 - precision: 0.9921 - recall: 0.9907\nEpoch 35: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0255 - precision: 0.9921 - recall: 0.9907 - val_accuracy: 0.9224 - val_auc: 0.9808 - val_loss: 0.4650 - val_precision: 0.9259 - val_recall: 0.9208\nEpoch 36/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9912 - auc: 0.9998 - loss: 0.0247 - precision: 0.9920 - recall: 0.9907\nEpoch 36: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9912 - auc: 0.9998 - loss: 0.0247 - precision: 0.9920 - recall: 0.9907 - val_accuracy: 0.9037 - val_auc: 0.9755 - val_loss: 0.5684 - val_precision: 0.9085 - val_recall: 0.9017\nEpoch 37/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9914 - auc: 0.9999 - loss: 0.0247 - precision: 0.9920 - recall: 0.9907\nEpoch 37: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9914 - auc: 0.9999 - loss: 0.0247 - precision: 0.9920 - recall: 0.9907 - val_accuracy: 0.8658 - val_auc: 0.9677 - val_loss: 0.7498 - val_precision: 0.8719 - val_recall: 0.8624\nEpoch 38/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9916 - auc: 0.9997 - loss: 0.0248 - precision: 0.9921 - recall: 0.9911\nEpoch 38: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9916 - auc: 0.9997 - loss: 0.0248 - precision: 0.9921 - recall: 0.9911 - val_accuracy: 0.8778 - val_auc: 0.9679 - val_loss: 0.7731 - val_precision: 0.8841 - val_recall: 0.8748\nEpoch 39/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9934 - auc: 0.9999 - loss: 0.0199 - precision: 0.9939 - recall: 0.9929\nEpoch 39: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9934 - auc: 0.9999 - loss: 0.0199 - precision: 0.9939 - recall: 0.9929 - val_accuracy: 0.9189 - val_auc: 0.9791 - val_loss: 0.5034 - val_precision: 0.9225 - val_recall: 0.9169\nEpoch 40/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9933 - auc: 0.9998 - loss: 0.0198 - precision: 0.9940 - recall: 0.9930\nEpoch 40: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9933 - auc: 0.9998 - loss: 0.0198 - precision: 0.9940 - recall: 0.9930 - val_accuracy: 0.9176 - val_auc: 0.9803 - val_loss: 0.4791 - val_precision: 0.9222 - val_recall: 0.9152\nEpoch 41/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9927 - auc: 0.9998 - loss: 0.0219 - precision: 0.9932 - recall: 0.9919\nEpoch 41: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9927 - auc: 0.9998 - loss: 0.0219 - precision: 0.9932 - recall: 0.9919 - val_accuracy: 0.9049 - val_auc: 0.9752 - val_loss: 0.6099 - val_precision: 0.9103 - val_recall: 0.9027\nEpoch 42/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9922 - auc: 0.9999 - loss: 0.0223 - precision: 0.9928 - recall: 0.9917\nEpoch 42: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9922 - auc: 0.9999 - loss: 0.0223 - precision: 0.9928 - recall: 0.9917 - val_accuracy: 0.9177 - val_auc: 0.9812 - val_loss: 0.4688 - val_precision: 0.9221 - val_recall: 0.9156\nEpoch 43/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - auc: 0.9999 - loss: 0.0185 - precision: 0.9943 - recall: 0.9934\nEpoch 43: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9939 - auc: 0.9999 - loss: 0.0185 - precision: 0.9943 - recall: 0.9934 - val_accuracy: 0.9039 - val_auc: 0.9729 - val_loss: 0.6434 - val_precision: 0.9079 - val_recall: 0.9023\nEpoch 44/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9937 - auc: 0.9999 - loss: 0.0172 - precision: 0.9942 - recall: 0.9932\nEpoch 44: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9937 - auc: 0.9999 - loss: 0.0172 - precision: 0.9942 - recall: 0.9932 - val_accuracy: 0.9085 - val_auc: 0.9773 - val_loss: 0.5544 - val_precision: 0.9132 - val_recall: 0.9067\nEpoch 45/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9930 - auc: 0.9999 - loss: 0.0193 - precision: 0.9934 - recall: 0.9927\nEpoch 45: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9930 - auc: 0.9999 - loss: 0.0193 - precision: 0.9934 - recall: 0.9927 - val_accuracy: 0.8378 - val_auc: 0.9533 - val_loss: 1.0595 - val_precision: 0.8464 - val_recall: 0.8337\nEpoch 46/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9934 - auc: 0.9999 - loss: 0.0202 - precision: 0.9938 - recall: 0.9928\nEpoch 46: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9934 - auc: 0.9999 - loss: 0.0202 - precision: 0.9938 - recall: 0.9928 - val_accuracy: 0.9007 - val_auc: 0.9736 - val_loss: 0.6447 - val_precision: 0.9065 - val_recall: 0.8982\nEpoch 47/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - auc: 0.9998 - loss: 0.0179 - precision: 0.9942 - recall: 0.9937\nEpoch 47: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9939 - auc: 0.9998 - loss: 0.0179 - precision: 0.9942 - recall: 0.9937 - val_accuracy: 0.9137 - val_auc: 0.9761 - val_loss: 0.5666 - val_precision: 0.9177 - val_recall: 0.9114\nEpoch 48/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0165 - precision: 0.9946 - recall: 0.9937\nEpoch 48: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0165 - precision: 0.9946 - recall: 0.9937 - val_accuracy: 0.9081 - val_auc: 0.9768 - val_loss: 0.5655 - val_precision: 0.9143 - val_recall: 0.9060\nEpoch 49/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9943 - auc: 0.9999 - loss: 0.0162 - precision: 0.9949 - recall: 0.9941\nEpoch 49: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9943 - auc: 0.9999 - loss: 0.0162 - precision: 0.9949 - recall: 0.9941 - val_accuracy: 0.9119 - val_auc: 0.9787 - val_loss: 0.5328 - val_precision: 0.9175 - val_recall: 0.9097\nEpoch 50/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0138 - precision: 0.9960 - recall: 0.9952\nEpoch 50: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0138 - precision: 0.9960 - recall: 0.9952 - val_accuracy: 0.8941 - val_auc: 0.9724 - val_loss: 0.6573 - val_precision: 0.8991 - val_recall: 0.8914\nEpoch 51/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9939 - auc: 0.9999 - loss: 0.0185 - precision: 0.9944 - recall: 0.9933\nEpoch 51: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9939 - auc: 0.9999 - loss: 0.0185 - precision: 0.9944 - recall: 0.9933 - val_accuracy: 0.8968 - val_auc: 0.9719 - val_loss: 0.6809 - val_precision: 0.9011 - val_recall: 0.8946\nEpoch 52/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9998 - loss: 0.0205 - precision: 0.9935 - recall: 0.9924\nEpoch 52: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9998 - loss: 0.0205 - precision: 0.9935 - recall: 0.9924 - val_accuracy: 0.9096 - val_auc: 0.9751 - val_loss: 0.5902 - val_precision: 0.9136 - val_recall: 0.9081\nEpoch 53/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9942 - auc: 0.9999 - loss: 0.0169 - precision: 0.9946 - recall: 0.9939\nEpoch 53: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9942 - auc: 0.9999 - loss: 0.0169 - precision: 0.9946 - recall: 0.9939 - val_accuracy: 0.8863 - val_auc: 0.9706 - val_loss: 0.7003 - val_precision: 0.8922 - val_recall: 0.8842\nEpoch 54/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9999 - loss: 0.0134 - precision: 0.9957 - recall: 0.9950\nEpoch 54: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9999 - loss: 0.0134 - precision: 0.9956 - recall: 0.9950 - val_accuracy: 0.8918 - val_auc: 0.9728 - val_loss: 0.6647 - val_precision: 0.8970 - val_recall: 0.8888\nEpoch 55/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0140 - precision: 0.9953 - recall: 0.9947\nEpoch 55: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0140 - precision: 0.9953 - recall: 0.9947 - val_accuracy: 0.9055 - val_auc: 0.9767 - val_loss: 0.5724 - val_precision: 0.9105 - val_recall: 0.9029\nEpoch 56/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0120 - precision: 0.9962 - recall: 0.9955\nEpoch 56: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0120 - precision: 0.9962 - recall: 0.9955 - val_accuracy: 0.9149 - val_auc: 0.9784 - val_loss: 0.5416 - val_precision: 0.9197 - val_recall: 0.9127\nEpoch 57/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0181 - precision: 0.9944 - recall: 0.9935\nEpoch 57: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0181 - precision: 0.9944 - recall: 0.9935 - val_accuracy: 0.8642 - val_auc: 0.9605 - val_loss: 0.9513 - val_precision: 0.8698 - val_recall: 0.8625\nEpoch 58/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0159 - precision: 0.9952 - recall: 0.9946\nEpoch 58: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0159 - precision: 0.9952 - recall: 0.9946 - val_accuracy: 0.9092 - val_auc: 0.9768 - val_loss: 0.5663 - val_precision: 0.9136 - val_recall: 0.9072\nEpoch 59/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0126 - precision: 0.9957 - recall: 0.9954\nEpoch 59: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0126 - precision: 0.9957 - recall: 0.9954 - val_accuracy: 0.9118 - val_auc: 0.9754 - val_loss: 0.6086 - val_precision: 0.9160 - val_recall: 0.9105\nEpoch 60/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9999 - loss: 0.0145 - precision: 0.9954 - recall: 0.9949\nEpoch 60: val_loss did not improve from 0.35138\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9999 - loss: 0.0145 - precision: 0.9954 - recall: 0.9949 - val_accuracy: 0.8976 - val_auc: 0.9726 - val_loss: 0.6755 - val_precision: 0.9018 - val_recall: 0.8953\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training with custom function","metadata":{"id":"CYj6cfiE1AsE"}},{"cell_type":"markdown","source":"To implement a custom activation function that behaves like ReLU but with a slope that decreases over iterations, you need to define a function that adjusts its behavior based on the current iteration step. In TensorFlow, this can be done by creating a custom layer that dynamically updates its behavior during training.\n\nIn this implementation, `CustomReLU` is a subclass of `Layer` from TensorFlow. It initializes with an `initial_slope` and the total number of `iterations` for the training. During each call (i.e., each forward pass), it adjusts the slope if the `training` flag is `True`.\n\n### Important Notes:\n- **Iteration Calculation**: `total_iterations` is calculated based on the number of training samples, batch size, and epochs. This represents the total number of training steps.\n- **Model Evaluation**: After training, evaluate the model's performance as usual. Keep an eye on how the changing slope affects the learning process.\n- **Tuning**: The initial slope and how quickly it decreases can significantly affect training. You might need to experiment with these values for optimal performance.\n- **Compatibility**: Ensure your version of TensorFlow supports the custom layer implementation. This code is compatible with TensorFlow 2.x.","metadata":{"id":"Idesc3xv8EOw"}},{"cell_type":"code","source":"import tensorflow as tf\n\n\nclass SmoothTransitionReLU(tf.keras.layers.Layer):\n    def __init__(self, initial_slope, final_slope, steepness=10, **kwargs):\n        super(SmoothTransitionReLU, self).__init__(**kwargs)\n        self.initial_slope = initial_slope\n        self.final_slope = final_slope\n        self.steepness = steepness\n        # Internal counter to track the relative progress of training\n        self.progress = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n\n    def call(self, inputs, training=None):\n        if training:\n            # Increment the progress during training (you might need to adjust how this increments based on your training regime)\n            self.progress.assign_add(0.01)  # Increment by a small value on each call\n\n        # Calculate the current slope based on the sigmoid function\n        x = self.progress\n        current_slope = self.initial_slope + (self.final_slope - self.initial_slope) / (1 + tf.exp(-self.steepness * (x - 0.5)))\n\n        # Apply the dynamic slope to the positive part of the inputs\n        positive_part = tf.maximum(0.0, inputs) * current_slope\n        # For negative inputs, just pass them through or adjust as needed\n        negative_part = tf.minimum(0.0, inputs)\n\n        return positive_part + negative_part\n\n    def get_config(self):\n        config = super(SmoothTransitionReLU, self).get_config()\n        config.update({\n            \"initial_slope\": self.initial_slope,\n            \"final_slope\": self.final_slope,\n            \"steepness\": self.steepness\n        })\n        return config\n\n","metadata":{"id":"ruZpD6v48Lni","execution":{"iopub.status.busy":"2024-03-19T12:44:11.161121Z","iopub.execute_input":"2024-03-19T12:44:11.161401Z","iopub.status.idle":"2024-03-19T12:44:11.171422Z","shell.execute_reply.started":"2024-03-19T12:44:11.161377Z","shell.execute_reply":"2024-03-19T12:44:11.170681Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_model_with_custom_activation(x_train, y_train, x_val, y_val, batch_size, learning_rate,\n                                       initial_slope, target_slope, total_epochs):\n    # Initialize the custom activation function with provided parameters\n    custom_activation = SmoothTransitionReLU(initial_slope=initial_slope, final_slope=target_slope)\n\n    # Build the model using the custom activation function\n    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1],\n                        activation_func=custom_activation)\n\n    # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        'dynamic_relu_model.keras',\n        save_best_only=True,\n        monitor='val_loss',\n        mode='min',\n        verbose=1\n    )\n\n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n                  metrics=['accuracy', 'precision', 'recall', 'auc'])\n\n    # Train the model\n    history = model.fit(x_train, y_train, epochs=total_epochs, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1,\n                        callbacks=[checkpoint_cb])\n\n    return history, model\n\n\n# Example parameters\nbatch_size = 32\nlearning_rate = 0.005\ninitial_slope = 1.732\ntarget_slope = 0.557\nrate = 0.01\ntotal_epochs = 60\n\n# Train the model\nhistory_custom, model_custom = train_model_with_custom_activation(\n    x_train_split, y_train_split, x_val_split, y_val_split, batch_size, learning_rate,\n    initial_slope, target_slope, total_epochs\n)\n\nhistories = {}\nhistories[\"custom\"] = history_custom","metadata":{"colab":{"background_save":true},"id":"Yv0YEJuv8STG","outputId":"a5c68753-205d-455c-dbf6-df10b5a2c1dc","execution":{"iopub.status.busy":"2024-03-19T12:44:11.172398Z","iopub.execute_input":"2024-03-19T12:44:11.172706Z","iopub.status.idle":"2024-03-19T13:03:58.442721Z","shell.execute_reply.started":"2024-03-19T12:44:11.172683Z","shell.execute_reply":"2024-03-19T13:03:58.441913Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2622 - auc: 0.6738 - loss: 2.1343 - precision: 0.4711 - recall: 0.0878\nEpoch 1: val_loss improved from inf to 0.71337, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 18ms/step - accuracy: 0.2624 - auc: 0.6739 - loss: 2.1340 - precision: 0.4713 - recall: 0.0879 - val_accuracy: 0.7948 - val_auc: 0.9682 - val_loss: 0.7134 - val_precision: 0.8405 - val_recall: 0.7683\nEpoch 2/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8379 - auc: 0.9814 - loss: 0.5269 - precision: 0.8960 - recall: 0.7917\nEpoch 2: val_loss improved from 0.71337 to 0.53662, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8379 - auc: 0.9814 - loss: 0.5268 - precision: 0.8960 - recall: 0.7918 - val_accuracy: 0.8360 - val_auc: 0.9805 - val_loss: 0.5366 - val_precision: 0.8920 - val_recall: 0.7984\nEpoch 3/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8798 - auc: 0.9881 - loss: 0.4019 - precision: 0.9210 - recall: 0.8484\nEpoch 3: val_loss improved from 0.53662 to 0.45592, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8799 - auc: 0.9881 - loss: 0.4019 - precision: 0.9210 - recall: 0.8484 - val_accuracy: 0.8600 - val_auc: 0.9848 - val_loss: 0.4559 - val_precision: 0.8976 - val_recall: 0.8342\nEpoch 4/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8944 - auc: 0.9909 - loss: 0.3474 - precision: 0.9292 - recall: 0.8685\nEpoch 4: val_loss improved from 0.45592 to 0.36076, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.8944 - auc: 0.9909 - loss: 0.3474 - precision: 0.9292 - recall: 0.8685 - val_accuracy: 0.8922 - val_auc: 0.9903 - val_loss: 0.3608 - val_precision: 0.9307 - val_recall: 0.8678\nEpoch 5/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9023 - auc: 0.9918 - loss: 0.3214 - precision: 0.9360 - recall: 0.8781\nEpoch 5: val_loss improved from 0.36076 to 0.33408, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9023 - auc: 0.9918 - loss: 0.3214 - precision: 0.9360 - recall: 0.8781 - val_accuracy: 0.8980 - val_auc: 0.9914 - val_loss: 0.3341 - val_precision: 0.9311 - val_recall: 0.8733\nEpoch 6/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9085 - auc: 0.9924 - loss: 0.3075 - precision: 0.9378 - recall: 0.8845\nEpoch 6: val_loss improved from 0.33408 to 0.32765, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9085 - auc: 0.9924 - loss: 0.3075 - precision: 0.9378 - recall: 0.8845 - val_accuracy: 0.9021 - val_auc: 0.9916 - val_loss: 0.3276 - val_precision: 0.9347 - val_recall: 0.8794\nEpoch 7/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9156 - auc: 0.9934 - loss: 0.2772 - precision: 0.9432 - recall: 0.8962\nEpoch 7: val_loss improved from 0.32765 to 0.32621, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9156 - auc: 0.9934 - loss: 0.2772 - precision: 0.9432 - recall: 0.8962 - val_accuracy: 0.9048 - val_auc: 0.9913 - val_loss: 0.3262 - val_precision: 0.9324 - val_recall: 0.8864\nEpoch 8/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9200 - auc: 0.9936 - loss: 0.2674 - precision: 0.9463 - recall: 0.9018\nEpoch 8: val_loss improved from 0.32621 to 0.28715, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9200 - auc: 0.9936 - loss: 0.2674 - precision: 0.9463 - recall: 0.9018 - val_accuracy: 0.9135 - val_auc: 0.9926 - val_loss: 0.2872 - val_precision: 0.9373 - val_recall: 0.8995\nEpoch 9/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9256 - auc: 0.9940 - loss: 0.2529 - precision: 0.9505 - recall: 0.9072\nEpoch 9: val_loss did not improve from 0.28715\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9256 - auc: 0.9940 - loss: 0.2529 - precision: 0.9505 - recall: 0.9072 - val_accuracy: 0.8670 - val_auc: 0.9860 - val_loss: 0.4345 - val_precision: 0.8967 - val_recall: 0.8499\nEpoch 10/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9292 - auc: 0.9948 - loss: 0.2396 - precision: 0.9529 - recall: 0.9129\nEpoch 10: val_loss improved from 0.28715 to 0.26377, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9292 - auc: 0.9948 - loss: 0.2396 - precision: 0.9529 - recall: 0.9129 - val_accuracy: 0.9214 - val_auc: 0.9943 - val_loss: 0.2638 - val_precision: 0.9508 - val_recall: 0.8983\nEpoch 11/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9312 - auc: 0.9948 - loss: 0.2336 - precision: 0.9530 - recall: 0.9154\nEpoch 11: val_loss did not improve from 0.26377\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9312 - auc: 0.9948 - loss: 0.2336 - precision: 0.9530 - recall: 0.9154 - val_accuracy: 0.9087 - val_auc: 0.9925 - val_loss: 0.3073 - val_precision: 0.9389 - val_recall: 0.8852\nEpoch 12/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9316 - auc: 0.9950 - loss: 0.2281 - precision: 0.9541 - recall: 0.9153\nEpoch 12: val_loss improved from 0.26377 to 0.26193, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9316 - auc: 0.9950 - loss: 0.2281 - precision: 0.9541 - recall: 0.9153 - val_accuracy: 0.9244 - val_auc: 0.9933 - val_loss: 0.2619 - val_precision: 0.9436 - val_recall: 0.9142\nEpoch 13/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9385 - auc: 0.9956 - loss: 0.2120 - precision: 0.9577 - recall: 0.9233\nEpoch 13: val_loss did not improve from 0.26193\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9385 - auc: 0.9956 - loss: 0.2120 - precision: 0.9577 - recall: 0.9233 - val_accuracy: 0.9207 - val_auc: 0.9932 - val_loss: 0.2671 - val_precision: 0.9444 - val_recall: 0.9079\nEpoch 14/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9394 - auc: 0.9954 - loss: 0.2078 - precision: 0.9581 - recall: 0.9249\nEpoch 14: val_loss did not improve from 0.26193\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9394 - auc: 0.9954 - loss: 0.2078 - precision: 0.9581 - recall: 0.9249 - val_accuracy: 0.9193 - val_auc: 0.9923 - val_loss: 0.2819 - val_precision: 0.9380 - val_recall: 0.9065\nEpoch 15/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9423 - auc: 0.9959 - loss: 0.1953 - precision: 0.9602 - recall: 0.9307\nEpoch 15: val_loss did not improve from 0.26193\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9423 - auc: 0.9959 - loss: 0.1953 - precision: 0.9602 - recall: 0.9307 - val_accuracy: 0.9234 - val_auc: 0.9934 - val_loss: 0.2658 - val_precision: 0.9474 - val_recall: 0.9075\nEpoch 16/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9435 - auc: 0.9961 - loss: 0.1901 - precision: 0.9601 - recall: 0.9311\nEpoch 16: val_loss did not improve from 0.26193\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9435 - auc: 0.9961 - loss: 0.1901 - precision: 0.9601 - recall: 0.9311 - val_accuracy: 0.8909 - val_auc: 0.9898 - val_loss: 0.3598 - val_precision: 0.9222 - val_recall: 0.8698\nEpoch 17/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9456 - auc: 0.9963 - loss: 0.1838 - precision: 0.9625 - recall: 0.9340\nEpoch 17: val_loss improved from 0.26193 to 0.24446, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9456 - auc: 0.9963 - loss: 0.1838 - precision: 0.9625 - recall: 0.9340 - val_accuracy: 0.9287 - val_auc: 0.9940 - val_loss: 0.2445 - val_precision: 0.9487 - val_recall: 0.9156\nEpoch 18/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9486 - auc: 0.9961 - loss: 0.1790 - precision: 0.9647 - recall: 0.9373\nEpoch 18: val_loss did not improve from 0.24446\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9486 - auc: 0.9961 - loss: 0.1790 - precision: 0.9647 - recall: 0.9373 - val_accuracy: 0.9050 - val_auc: 0.9905 - val_loss: 0.3298 - val_precision: 0.9285 - val_recall: 0.8905\nEpoch 19/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9490 - auc: 0.9964 - loss: 0.1734 - precision: 0.9646 - recall: 0.9381\nEpoch 19: val_loss did not improve from 0.24446\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9490 - auc: 0.9964 - loss: 0.1735 - precision: 0.9646 - recall: 0.9381 - val_accuracy: 0.9211 - val_auc: 0.9931 - val_loss: 0.2756 - val_precision: 0.9411 - val_recall: 0.9055\nEpoch 20/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9506 - auc: 0.9964 - loss: 0.1708 - precision: 0.9651 - recall: 0.9392\nEpoch 20: val_loss did not improve from 0.24446\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9506 - auc: 0.9964 - loss: 0.1708 - precision: 0.9651 - recall: 0.9392 - val_accuracy: 0.9242 - val_auc: 0.9929 - val_loss: 0.2702 - val_precision: 0.9441 - val_recall: 0.9107\nEpoch 21/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9518 - auc: 0.9966 - loss: 0.1649 - precision: 0.9645 - recall: 0.9410\nEpoch 21: val_loss did not improve from 0.24446\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9518 - auc: 0.9966 - loss: 0.1649 - precision: 0.9645 - recall: 0.9410 - val_accuracy: 0.9234 - val_auc: 0.9927 - val_loss: 0.2711 - val_precision: 0.9415 - val_recall: 0.9142\nEpoch 22/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9547 - auc: 0.9970 - loss: 0.1546 - precision: 0.9679 - recall: 0.9455\nEpoch 22: val_loss did not improve from 0.24446\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9547 - auc: 0.9970 - loss: 0.1546 - precision: 0.9679 - recall: 0.9455 - val_accuracy: 0.9206 - val_auc: 0.9928 - val_loss: 0.2754 - val_precision: 0.9430 - val_recall: 0.9061\nEpoch 23/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9540 - auc: 0.9972 - loss: 0.1519 - precision: 0.9667 - recall: 0.9445\nEpoch 23: val_loss improved from 0.24446 to 0.23691, saving model to dynamic_relu_model.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9540 - auc: 0.9972 - loss: 0.1519 - precision: 0.9667 - recall: 0.9445 - val_accuracy: 0.9338 - val_auc: 0.9936 - val_loss: 0.2369 - val_precision: 0.9495 - val_recall: 0.9241\nEpoch 24/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9577 - auc: 0.9971 - loss: 0.1453 - precision: 0.9692 - recall: 0.9487\nEpoch 24: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9577 - auc: 0.9971 - loss: 0.1453 - precision: 0.9692 - recall: 0.9487 - val_accuracy: 0.9148 - val_auc: 0.9911 - val_loss: 0.3053 - val_precision: 0.9335 - val_recall: 0.9055\nEpoch 25/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9590 - auc: 0.9974 - loss: 0.1380 - precision: 0.9703 - recall: 0.9502\nEpoch 25: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9590 - auc: 0.9974 - loss: 0.1380 - precision: 0.9703 - recall: 0.9502 - val_accuracy: 0.9162 - val_auc: 0.9913 - val_loss: 0.2983 - val_precision: 0.9344 - val_recall: 0.9065\nEpoch 26/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9609 - auc: 0.9975 - loss: 0.1326 - precision: 0.9711 - recall: 0.9526\nEpoch 26: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9609 - auc: 0.9975 - loss: 0.1326 - precision: 0.9711 - recall: 0.9526 - val_accuracy: 0.9297 - val_auc: 0.9931 - val_loss: 0.2560 - val_precision: 0.9440 - val_recall: 0.9201\nEpoch 27/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9618 - auc: 0.9976 - loss: 0.1279 - precision: 0.9715 - recall: 0.9549\nEpoch 27: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9618 - auc: 0.9976 - loss: 0.1279 - precision: 0.9715 - recall: 0.9549 - val_accuracy: 0.9251 - val_auc: 0.9913 - val_loss: 0.2912 - val_precision: 0.9387 - val_recall: 0.9169\nEpoch 28/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9635 - auc: 0.9979 - loss: 0.1221 - precision: 0.9725 - recall: 0.9562\nEpoch 28: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9635 - auc: 0.9979 - loss: 0.1222 - precision: 0.9725 - recall: 0.9562 - val_accuracy: 0.9234 - val_auc: 0.9916 - val_loss: 0.2838 - val_precision: 0.9396 - val_recall: 0.9145\nEpoch 29/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9642 - auc: 0.9979 - loss: 0.1196 - precision: 0.9728 - recall: 0.9559\nEpoch 29: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9642 - auc: 0.9979 - loss: 0.1196 - precision: 0.9728 - recall: 0.9559 - val_accuracy: 0.9172 - val_auc: 0.9909 - val_loss: 0.3038 - val_precision: 0.9335 - val_recall: 0.9084\nEpoch 30/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9664 - auc: 0.9979 - loss: 0.1144 - precision: 0.9738 - recall: 0.9594\nEpoch 30: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9664 - auc: 0.9979 - loss: 0.1144 - precision: 0.9738 - recall: 0.9594 - val_accuracy: 0.9082 - val_auc: 0.9892 - val_loss: 0.3430 - val_precision: 0.9251 - val_recall: 0.9008\nEpoch 31/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9661 - auc: 0.9981 - loss: 0.1112 - precision: 0.9748 - recall: 0.9598\nEpoch 31: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9661 - auc: 0.9981 - loss: 0.1112 - precision: 0.9748 - recall: 0.9598 - val_accuracy: 0.9251 - val_auc: 0.9912 - val_loss: 0.2897 - val_precision: 0.9393 - val_recall: 0.9171\nEpoch 32/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9980 - loss: 0.1115 - precision: 0.9745 - recall: 0.9605\nEpoch 32: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9668 - auc: 0.9980 - loss: 0.1115 - precision: 0.9745 - recall: 0.9605 - val_accuracy: 0.9139 - val_auc: 0.9863 - val_loss: 0.3633 - val_precision: 0.9264 - val_recall: 0.9077\nEpoch 33/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9696 - auc: 0.9985 - loss: 0.0993 - precision: 0.9768 - recall: 0.9635\nEpoch 33: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9696 - auc: 0.9985 - loss: 0.0993 - precision: 0.9768 - recall: 0.9635 - val_accuracy: 0.9087 - val_auc: 0.9899 - val_loss: 0.3347 - val_precision: 0.9266 - val_recall: 0.8997\nEpoch 34/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9692 - auc: 0.9985 - loss: 0.0986 - precision: 0.9761 - recall: 0.9640\nEpoch 34: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9692 - auc: 0.9985 - loss: 0.0987 - precision: 0.9761 - recall: 0.9640 - val_accuracy: 0.9116 - val_auc: 0.9867 - val_loss: 0.3704 - val_precision: 0.9228 - val_recall: 0.9069\nEpoch 35/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9705 - auc: 0.9986 - loss: 0.0947 - precision: 0.9775 - recall: 0.9656\nEpoch 35: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9705 - auc: 0.9986 - loss: 0.0947 - precision: 0.9775 - recall: 0.9656 - val_accuracy: 0.9221 - val_auc: 0.9900 - val_loss: 0.3148 - val_precision: 0.9356 - val_recall: 0.9157\nEpoch 36/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9715 - auc: 0.9986 - loss: 0.0906 - precision: 0.9773 - recall: 0.9667\nEpoch 36: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9715 - auc: 0.9986 - loss: 0.0907 - precision: 0.9773 - recall: 0.9667 - val_accuracy: 0.9115 - val_auc: 0.9871 - val_loss: 0.3657 - val_precision: 0.9232 - val_recall: 0.9065\nEpoch 37/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9742 - auc: 0.9990 - loss: 0.0812 - precision: 0.9793 - recall: 0.9709\nEpoch 37: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9742 - auc: 0.9990 - loss: 0.0812 - precision: 0.9793 - recall: 0.9709 - val_accuracy: 0.9076 - val_auc: 0.9855 - val_loss: 0.3959 - val_precision: 0.9198 - val_recall: 0.9011\nEpoch 38/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9724 - auc: 0.9989 - loss: 0.0833 - precision: 0.9781 - recall: 0.9682\nEpoch 38: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9724 - auc: 0.9989 - loss: 0.0833 - precision: 0.9781 - recall: 0.9682 - val_accuracy: 0.9216 - val_auc: 0.9876 - val_loss: 0.3579 - val_precision: 0.9290 - val_recall: 0.9178\nEpoch 39/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9751 - auc: 0.9989 - loss: 0.0787 - precision: 0.9793 - recall: 0.9715\nEpoch 39: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9751 - auc: 0.9989 - loss: 0.0788 - precision: 0.9793 - recall: 0.9715 - val_accuracy: 0.9061 - val_auc: 0.9847 - val_loss: 0.4107 - val_precision: 0.9169 - val_recall: 0.8996\nEpoch 40/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9745 - auc: 0.9990 - loss: 0.0763 - precision: 0.9795 - recall: 0.9707\nEpoch 40: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9745 - auc: 0.9990 - loss: 0.0763 - precision: 0.9795 - recall: 0.9707 - val_accuracy: 0.8847 - val_auc: 0.9795 - val_loss: 0.5181 - val_precision: 0.8961 - val_recall: 0.8792\nEpoch 41/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9763 - auc: 0.9992 - loss: 0.0709 - precision: 0.9810 - recall: 0.9730\nEpoch 41: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9762 - auc: 0.9992 - loss: 0.0709 - precision: 0.9810 - recall: 0.9730 - val_accuracy: 0.9173 - val_auc: 0.9862 - val_loss: 0.3850 - val_precision: 0.9270 - val_recall: 0.9120\nEpoch 42/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9768 - auc: 0.9992 - loss: 0.0712 - precision: 0.9808 - recall: 0.9730\nEpoch 42: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.9768 - auc: 0.9992 - loss: 0.0712 - precision: 0.9808 - recall: 0.9730 - val_accuracy: 0.9006 - val_auc: 0.9843 - val_loss: 0.4271 - val_precision: 0.9140 - val_recall: 0.8935\nEpoch 43/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9766 - auc: 0.9992 - loss: 0.0694 - precision: 0.9804 - recall: 0.9735\nEpoch 43: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9766 - auc: 0.9992 - loss: 0.0694 - precision: 0.9804 - recall: 0.9735 - val_accuracy: 0.9079 - val_auc: 0.9840 - val_loss: 0.4234 - val_precision: 0.9177 - val_recall: 0.9026\nEpoch 44/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9773 - auc: 0.9992 - loss: 0.0694 - precision: 0.9808 - recall: 0.9743\nEpoch 44: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9773 - auc: 0.9992 - loss: 0.0694 - precision: 0.9808 - recall: 0.9743 - val_accuracy: 0.9173 - val_auc: 0.9869 - val_loss: 0.3752 - val_precision: 0.9268 - val_recall: 0.9129\nEpoch 45/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9792 - auc: 0.9993 - loss: 0.0631 - precision: 0.9824 - recall: 0.9762\nEpoch 45: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9792 - auc: 0.9993 - loss: 0.0631 - precision: 0.9824 - recall: 0.9762 - val_accuracy: 0.9226 - val_auc: 0.9857 - val_loss: 0.3866 - val_precision: 0.9297 - val_recall: 0.9197\nEpoch 46/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9774 - auc: 0.9992 - loss: 0.0684 - precision: 0.9814 - recall: 0.9752\nEpoch 46: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9774 - auc: 0.9992 - loss: 0.0684 - precision: 0.9814 - recall: 0.9752 - val_accuracy: 0.8986 - val_auc: 0.9830 - val_loss: 0.4472 - val_precision: 0.9089 - val_recall: 0.8923\nEpoch 47/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9805 - auc: 0.9992 - loss: 0.0603 - precision: 0.9832 - recall: 0.9783\nEpoch 47: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9805 - auc: 0.9992 - loss: 0.0604 - precision: 0.9832 - recall: 0.9783 - val_accuracy: 0.8916 - val_auc: 0.9811 - val_loss: 0.4930 - val_precision: 0.9036 - val_recall: 0.8860\nEpoch 48/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9798 - auc: 0.9991 - loss: 0.0619 - precision: 0.9828 - recall: 0.9775\nEpoch 48: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9798 - auc: 0.9991 - loss: 0.0619 - precision: 0.9828 - recall: 0.9775 - val_accuracy: 0.9229 - val_auc: 0.9874 - val_loss: 0.3522 - val_precision: 0.9304 - val_recall: 0.9194\nEpoch 49/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9815 - auc: 0.9994 - loss: 0.0564 - precision: 0.9842 - recall: 0.9791\nEpoch 49: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9815 - auc: 0.9994 - loss: 0.0564 - precision: 0.9842 - recall: 0.9791 - val_accuracy: 0.9264 - val_auc: 0.9883 - val_loss: 0.3304 - val_precision: 0.9340 - val_recall: 0.9225\nEpoch 50/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9816 - auc: 0.9995 - loss: 0.0539 - precision: 0.9842 - recall: 0.9793\nEpoch 50: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9816 - auc: 0.9995 - loss: 0.0539 - precision: 0.9842 - recall: 0.9793 - val_accuracy: 0.9180 - val_auc: 0.9847 - val_loss: 0.4201 - val_precision: 0.9255 - val_recall: 0.9155\nEpoch 51/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9845 - auc: 0.9995 - loss: 0.0490 - precision: 0.9866 - recall: 0.9826\nEpoch 51: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9845 - auc: 0.9995 - loss: 0.0490 - precision: 0.9866 - recall: 0.9826 - val_accuracy: 0.8913 - val_auc: 0.9798 - val_loss: 0.5122 - val_precision: 0.9008 - val_recall: 0.8865\nEpoch 52/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9834 - auc: 0.9995 - loss: 0.0499 - precision: 0.9853 - recall: 0.9814\nEpoch 52: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9834 - auc: 0.9995 - loss: 0.0499 - precision: 0.9853 - recall: 0.9814 - val_accuracy: 0.9201 - val_auc: 0.9843 - val_loss: 0.4160 - val_precision: 0.9270 - val_recall: 0.9174\nEpoch 53/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9825 - auc: 0.9995 - loss: 0.0519 - precision: 0.9846 - recall: 0.9806\nEpoch 53: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9825 - auc: 0.9995 - loss: 0.0519 - precision: 0.9846 - recall: 0.9806 - val_accuracy: 0.9055 - val_auc: 0.9798 - val_loss: 0.5034 - val_precision: 0.9115 - val_recall: 0.9019\nEpoch 54/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - auc: 0.9996 - loss: 0.0457 - precision: 0.9865 - recall: 0.9824\nEpoch 54: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9842 - auc: 0.9996 - loss: 0.0458 - precision: 0.9865 - recall: 0.9824 - val_accuracy: 0.9178 - val_auc: 0.9819 - val_loss: 0.4655 - val_precision: 0.9237 - val_recall: 0.9150\nEpoch 55/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9846 - auc: 0.9994 - loss: 0.0470 - precision: 0.9863 - recall: 0.9828\nEpoch 55: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9845 - auc: 0.9994 - loss: 0.0470 - precision: 0.9863 - recall: 0.9828 - val_accuracy: 0.9141 - val_auc: 0.9817 - val_loss: 0.4651 - val_precision: 0.9206 - val_recall: 0.9109\nEpoch 56/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9840 - auc: 0.9995 - loss: 0.0465 - precision: 0.9862 - recall: 0.9826\nEpoch 56: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9840 - auc: 0.9995 - loss: 0.0465 - precision: 0.9862 - recall: 0.9826 - val_accuracy: 0.9225 - val_auc: 0.9853 - val_loss: 0.3985 - val_precision: 0.9301 - val_recall: 0.9198\nEpoch 57/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - auc: 0.9995 - loss: 0.0462 - precision: 0.9856 - recall: 0.9828\nEpoch 57: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9842 - auc: 0.9995 - loss: 0.0462 - precision: 0.9856 - recall: 0.9828 - val_accuracy: 0.9059 - val_auc: 0.9823 - val_loss: 0.4615 - val_precision: 0.9144 - val_recall: 0.9010\nEpoch 58/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9851 - auc: 0.9996 - loss: 0.0443 - precision: 0.9871 - recall: 0.9833\nEpoch 58: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9851 - auc: 0.9996 - loss: 0.0443 - precision: 0.9871 - recall: 0.9833 - val_accuracy: 0.9197 - val_auc: 0.9825 - val_loss: 0.4592 - val_precision: 0.9245 - val_recall: 0.9173\nEpoch 59/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9855 - auc: 0.9996 - loss: 0.0430 - precision: 0.9879 - recall: 0.9845\nEpoch 59: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9855 - auc: 0.9996 - loss: 0.0430 - precision: 0.9879 - recall: 0.9845 - val_accuracy: 0.9225 - val_auc: 0.9827 - val_loss: 0.4428 - val_precision: 0.9287 - val_recall: 0.9202\nEpoch 60/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9867 - auc: 0.9996 - loss: 0.0403 - precision: 0.9883 - recall: 0.9854\nEpoch 60: val_loss did not improve from 0.23691\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9867 - auc: 0.9996 - loss: 0.0403 - precision: 0.9883 - recall: 0.9854 - val_accuracy: 0.9070 - val_auc: 0.9777 - val_loss: 0.5568 - val_precision: 0.9122 - val_recall: 0.9042\n","output_type":"stream"}]},{"cell_type":"code","source":"def mish(x):\n    return x * tf.math.tanh(tf.math.softplus(x))\n\nhistory_mish, model_mish = train_model(mish, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"mish\")\nhistories[\"mish\"] = history_mish","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:03:58.443826Z","iopub.execute_input":"2024-03-19T13:03:58.444098Z","iopub.status.idle":"2024-03-19T13:24:37.965392Z","shell.execute_reply.started":"2024-03-19T13:03:58.444076Z","shell.execute_reply":"2024-03-19T13:24:37.964370Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.2928 - auc: 0.6968 - loss: 2.0128 - precision: 0.5011 - recall: 0.1219\nEpoch 1: val_loss improved from inf to 0.47587, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 18ms/step - accuracy: 0.2929 - auc: 0.6969 - loss: 2.0125 - precision: 0.5013 - recall: 0.1220 - val_accuracy: 0.8497 - val_auc: 0.9842 - val_loss: 0.4759 - val_precision: 0.8968 - val_recall: 0.8188\nEpoch 2/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8913 - auc: 0.9900 - loss: 0.3600 - precision: 0.9283 - recall: 0.8627\nEpoch 2: val_loss did not improve from 0.47587\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.8913 - auc: 0.9900 - loss: 0.3600 - precision: 0.9283 - recall: 0.8627 - val_accuracy: 0.8108 - val_auc: 0.9766 - val_loss: 0.5981 - val_precision: 0.8749 - val_recall: 0.7688\nEpoch 3/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9208 - auc: 0.9939 - loss: 0.2648 - precision: 0.9482 - recall: 0.9001\nEpoch 3: val_loss improved from 0.47587 to 0.30213, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9208 - auc: 0.9939 - loss: 0.2648 - precision: 0.9482 - recall: 0.9001 - val_accuracy: 0.9090 - val_auc: 0.9919 - val_loss: 0.3021 - val_precision: 0.9362 - val_recall: 0.8930\nEpoch 4/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9333 - auc: 0.9950 - loss: 0.2287 - precision: 0.9559 - recall: 0.9157\nEpoch 4: val_loss improved from 0.30213 to 0.24551, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9333 - auc: 0.9950 - loss: 0.2287 - precision: 0.9559 - recall: 0.9157 - val_accuracy: 0.9309 - val_auc: 0.9933 - val_loss: 0.2455 - val_precision: 0.9501 - val_recall: 0.9183\nEpoch 5/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9415 - auc: 0.9957 - loss: 0.1997 - precision: 0.9592 - recall: 0.9270\nEpoch 5: val_loss improved from 0.24551 to 0.23734, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9415 - auc: 0.9957 - loss: 0.1997 - precision: 0.9592 - recall: 0.9270 - val_accuracy: 0.9321 - val_auc: 0.9941 - val_loss: 0.2373 - val_precision: 0.9526 - val_recall: 0.9171\nEpoch 6/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9513 - auc: 0.9963 - loss: 0.1725 - precision: 0.9661 - recall: 0.9399\nEpoch 6: val_loss did not improve from 0.23734\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9513 - auc: 0.9963 - loss: 0.1725 - precision: 0.9661 - recall: 0.9399 - val_accuracy: 0.9173 - val_auc: 0.9922 - val_loss: 0.2845 - val_precision: 0.9412 - val_recall: 0.9032\nEpoch 7/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9584 - auc: 0.9969 - loss: 0.1475 - precision: 0.9708 - recall: 0.9490\nEpoch 7: val_loss improved from 0.23734 to 0.23416, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9584 - auc: 0.9969 - loss: 0.1475 - precision: 0.9708 - recall: 0.9490 - val_accuracy: 0.9352 - val_auc: 0.9935 - val_loss: 0.2342 - val_precision: 0.9522 - val_recall: 0.9250\nEpoch 8/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9630 - auc: 0.9974 - loss: 0.1279 - precision: 0.9734 - recall: 0.9550\nEpoch 8: val_loss improved from 0.23416 to 0.23229, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9630 - auc: 0.9974 - loss: 0.1279 - precision: 0.9734 - recall: 0.9550 - val_accuracy: 0.9361 - val_auc: 0.9932 - val_loss: 0.2323 - val_precision: 0.9505 - val_recall: 0.9264\nEpoch 9/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9692 - auc: 0.9981 - loss: 0.1044 - precision: 0.9774 - recall: 0.9630\nEpoch 9: val_loss improved from 0.23229 to 0.22874, saving model to mish.keras\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9692 - auc: 0.9981 - loss: 0.1044 - precision: 0.9774 - recall: 0.9630 - val_accuracy: 0.9415 - val_auc: 0.9932 - val_loss: 0.2287 - val_precision: 0.9530 - val_recall: 0.9351\nEpoch 10/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9737 - auc: 0.9986 - loss: 0.0860 - precision: 0.9796 - recall: 0.9691\nEpoch 10: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9737 - auc: 0.9986 - loss: 0.0860 - precision: 0.9796 - recall: 0.9691 - val_accuracy: 0.9344 - val_auc: 0.9913 - val_loss: 0.2597 - val_precision: 0.9457 - val_recall: 0.9292\nEpoch 11/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9790 - auc: 0.9990 - loss: 0.0685 - precision: 0.9834 - recall: 0.9755\nEpoch 11: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9790 - auc: 0.9990 - loss: 0.0685 - precision: 0.9834 - recall: 0.9755 - val_accuracy: 0.9401 - val_auc: 0.9924 - val_loss: 0.2417 - val_precision: 0.9514 - val_recall: 0.9330\nEpoch 12/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9818 - auc: 0.9993 - loss: 0.0585 - precision: 0.9855 - recall: 0.9792\nEpoch 12: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9818 - auc: 0.9993 - loss: 0.0585 - precision: 0.9855 - recall: 0.9792 - val_accuracy: 0.9349 - val_auc: 0.9894 - val_loss: 0.2935 - val_precision: 0.9425 - val_recall: 0.9307\nEpoch 13/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9834 - auc: 0.9995 - loss: 0.0504 - precision: 0.9866 - recall: 0.9813\nEpoch 13: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9834 - auc: 0.9995 - loss: 0.0504 - precision: 0.9866 - recall: 0.9813 - val_accuracy: 0.9311 - val_auc: 0.9881 - val_loss: 0.3162 - val_precision: 0.9397 - val_recall: 0.9264\nEpoch 14/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9861 - auc: 0.9995 - loss: 0.0433 - precision: 0.9882 - recall: 0.9840\nEpoch 14: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9861 - auc: 0.9995 - loss: 0.0433 - precision: 0.9882 - recall: 0.9840 - val_accuracy: 0.9362 - val_auc: 0.9885 - val_loss: 0.3169 - val_precision: 0.9424 - val_recall: 0.9331\nEpoch 15/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9875 - auc: 0.9995 - loss: 0.0413 - precision: 0.9891 - recall: 0.9856\nEpoch 15: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9875 - auc: 0.9995 - loss: 0.0413 - precision: 0.9891 - recall: 0.9856 - val_accuracy: 0.9248 - val_auc: 0.9863 - val_loss: 0.3536 - val_precision: 0.9343 - val_recall: 0.9204\nEpoch 16/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9891 - auc: 0.9998 - loss: 0.0326 - precision: 0.9907 - recall: 0.9878\nEpoch 16: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9891 - auc: 0.9998 - loss: 0.0326 - precision: 0.9906 - recall: 0.9877 - val_accuracy: 0.9322 - val_auc: 0.9859 - val_loss: 0.3510 - val_precision: 0.9379 - val_recall: 0.9292\nEpoch 17/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9900 - auc: 0.9997 - loss: 0.0315 - precision: 0.9911 - recall: 0.9886\nEpoch 17: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9900 - auc: 0.9997 - loss: 0.0315 - precision: 0.9911 - recall: 0.9886 - val_accuracy: 0.9397 - val_auc: 0.9872 - val_loss: 0.3314 - val_precision: 0.9449 - val_recall: 0.9368\nEpoch 18/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9904 - auc: 0.9998 - loss: 0.0283 - precision: 0.9916 - recall: 0.9893\nEpoch 18: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9904 - auc: 0.9998 - loss: 0.0283 - precision: 0.9916 - recall: 0.9893 - val_accuracy: 0.9249 - val_auc: 0.9842 - val_loss: 0.4040 - val_precision: 0.9318 - val_recall: 0.9219\nEpoch 19/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9907 - auc: 0.9997 - loss: 0.0282 - precision: 0.9918 - recall: 0.9895\nEpoch 19: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9907 - auc: 0.9997 - loss: 0.0282 - precision: 0.9918 - recall: 0.9895 - val_accuracy: 0.9331 - val_auc: 0.9853 - val_loss: 0.3650 - val_precision: 0.9384 - val_recall: 0.9305\nEpoch 20/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0257 - precision: 0.9924 - recall: 0.9905\nEpoch 20: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9913 - auc: 0.9998 - loss: 0.0257 - precision: 0.9924 - recall: 0.9905 - val_accuracy: 0.9369 - val_auc: 0.9855 - val_loss: 0.3659 - val_precision: 0.9414 - val_recall: 0.9346\nEpoch 21/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9926 - auc: 0.9998 - loss: 0.0233 - precision: 0.9932 - recall: 0.9918\nEpoch 21: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9925 - auc: 0.9998 - loss: 0.0234 - precision: 0.9932 - recall: 0.9918 - val_accuracy: 0.9282 - val_auc: 0.9852 - val_loss: 0.3900 - val_precision: 0.9327 - val_recall: 0.9262\nEpoch 22/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - auc: 0.9999 - loss: 0.0207 - precision: 0.9939 - recall: 0.9926\nEpoch 22: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9932 - auc: 0.9999 - loss: 0.0207 - precision: 0.9939 - recall: 0.9926 - val_accuracy: 0.9330 - val_auc: 0.9857 - val_loss: 0.3699 - val_precision: 0.9384 - val_recall: 0.9301\nEpoch 23/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9936 - auc: 0.9998 - loss: 0.0195 - precision: 0.9942 - recall: 0.9930\nEpoch 23: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9936 - auc: 0.9998 - loss: 0.0195 - precision: 0.9942 - recall: 0.9930 - val_accuracy: 0.9351 - val_auc: 0.9848 - val_loss: 0.3895 - val_precision: 0.9392 - val_recall: 0.9327\nEpoch 24/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9925 - auc: 0.9999 - loss: 0.0223 - precision: 0.9931 - recall: 0.9920\nEpoch 24: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9925 - auc: 0.9999 - loss: 0.0223 - precision: 0.9931 - recall: 0.9920 - val_accuracy: 0.9361 - val_auc: 0.9859 - val_loss: 0.3807 - val_precision: 0.9411 - val_recall: 0.9339\nEpoch 25/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9998 - loss: 0.0191 - precision: 0.9945 - recall: 0.9935\nEpoch 25: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9940 - auc: 0.9998 - loss: 0.0191 - precision: 0.9945 - recall: 0.9935 - val_accuracy: 0.9211 - val_auc: 0.9804 - val_loss: 0.5038 - val_precision: 0.9257 - val_recall: 0.9189\nEpoch 26/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9998 - loss: 0.0206 - precision: 0.9937 - recall: 0.9923\nEpoch 26: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9929 - auc: 0.9998 - loss: 0.0206 - precision: 0.9937 - recall: 0.9923 - val_accuracy: 0.9267 - val_auc: 0.9834 - val_loss: 0.4261 - val_precision: 0.9316 - val_recall: 0.9240\nEpoch 27/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9997 - loss: 0.0234 - precision: 0.9930 - recall: 0.9919\nEpoch 27: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9923 - auc: 0.9997 - loss: 0.0233 - precision: 0.9930 - recall: 0.9919 - val_accuracy: 0.9264 - val_auc: 0.9817 - val_loss: 0.4681 - val_precision: 0.9304 - val_recall: 0.9249\nEpoch 28/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0162 - precision: 0.9952 - recall: 0.9945\nEpoch 28: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0162 - precision: 0.9952 - recall: 0.9945 - val_accuracy: 0.9350 - val_auc: 0.9864 - val_loss: 0.3734 - val_precision: 0.9396 - val_recall: 0.9329\nEpoch 29/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0167 - precision: 0.9949 - recall: 0.9942\nEpoch 29: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0167 - precision: 0.9949 - recall: 0.9942 - val_accuracy: 0.9342 - val_auc: 0.9842 - val_loss: 0.4207 - val_precision: 0.9376 - val_recall: 0.9326\nEpoch 30/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9999 - loss: 0.0157 - precision: 0.9952 - recall: 0.9945\nEpoch 30: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9949 - auc: 0.9999 - loss: 0.0157 - precision: 0.9952 - recall: 0.9945 - val_accuracy: 0.9357 - val_auc: 0.9849 - val_loss: 0.4019 - val_precision: 0.9395 - val_recall: 0.9334\nEpoch 31/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0158 - precision: 0.9951 - recall: 0.9946\nEpoch 31: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0158 - precision: 0.9951 - recall: 0.9946 - val_accuracy: 0.9391 - val_auc: 0.9843 - val_loss: 0.3981 - val_precision: 0.9426 - val_recall: 0.9379\nEpoch 32/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0159 - precision: 0.9949 - recall: 0.9943\nEpoch 32: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0159 - precision: 0.9949 - recall: 0.9943 - val_accuracy: 0.9376 - val_auc: 0.9834 - val_loss: 0.4213 - val_precision: 0.9406 - val_recall: 0.9360\nEpoch 33/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0100 - precision: 0.9968 - recall: 0.9963\nEpoch 33: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0100 - precision: 0.9968 - recall: 0.9963 - val_accuracy: 0.9392 - val_auc: 0.9858 - val_loss: 0.3850 - val_precision: 0.9432 - val_recall: 0.9373\nEpoch 34/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0154 - precision: 0.9954 - recall: 0.9946\nEpoch 34: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0154 - precision: 0.9954 - recall: 0.9946 - val_accuracy: 0.9377 - val_auc: 0.9835 - val_loss: 0.4339 - val_precision: 0.9412 - val_recall: 0.9359\nEpoch 35/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0109 - precision: 0.9965 - recall: 0.9962\nEpoch 35: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0109 - precision: 0.9965 - recall: 0.9962 - val_accuracy: 0.9350 - val_auc: 0.9833 - val_loss: 0.4196 - val_precision: 0.9391 - val_recall: 0.9338\nEpoch 36/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - auc: 0.9999 - loss: 0.0120 - precision: 0.9964 - recall: 0.9960\nEpoch 36: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9962 - auc: 0.9999 - loss: 0.0120 - precision: 0.9964 - recall: 0.9960 - val_accuracy: 0.9399 - val_auc: 0.9849 - val_loss: 0.3987 - val_precision: 0.9434 - val_recall: 0.9388\nEpoch 37/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9962 - auc: 0.9999 - loss: 0.0119 - precision: 0.9965 - recall: 0.9960\nEpoch 37: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9962 - auc: 0.9999 - loss: 0.0119 - precision: 0.9965 - recall: 0.9960 - val_accuracy: 0.9415 - val_auc: 0.9844 - val_loss: 0.4081 - val_precision: 0.9438 - val_recall: 0.9401\nEpoch 38/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0111 - precision: 0.9967 - recall: 0.9964\nEpoch 38: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0112 - precision: 0.9967 - recall: 0.9964 - val_accuracy: 0.9325 - val_auc: 0.9818 - val_loss: 0.4813 - val_precision: 0.9357 - val_recall: 0.9311\nEpoch 39/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9958 - auc: 0.9999 - loss: 0.0133 - precision: 0.9959 - recall: 0.9954\nEpoch 39: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9958 - auc: 0.9999 - loss: 0.0133 - precision: 0.9959 - recall: 0.9954 - val_accuracy: 0.9307 - val_auc: 0.9827 - val_loss: 0.4389 - val_precision: 0.9344 - val_recall: 0.9294\nEpoch 40/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9950 - auc: 0.9999 - loss: 0.0137 - precision: 0.9952 - recall: 0.9948\nEpoch 40: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9950 - auc: 0.9999 - loss: 0.0137 - precision: 0.9952 - recall: 0.9948 - val_accuracy: 0.9380 - val_auc: 0.9852 - val_loss: 0.4061 - val_precision: 0.9418 - val_recall: 0.9367\nEpoch 41/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0099 - precision: 0.9971 - recall: 0.9967\nEpoch 41: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0099 - precision: 0.9971 - recall: 0.9967 - val_accuracy: 0.9397 - val_auc: 0.9841 - val_loss: 0.4304 - val_precision: 0.9427 - val_recall: 0.9384\nEpoch 42/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9957 - auc: 0.9999 - loss: 0.0121 - precision: 0.9960 - recall: 0.9956\nEpoch 42: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9957 - auc: 0.9999 - loss: 0.0121 - precision: 0.9960 - recall: 0.9956 - val_accuracy: 0.9316 - val_auc: 0.9831 - val_loss: 0.4632 - val_precision: 0.9360 - val_recall: 0.9298\nEpoch 43/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0097 - precision: 0.9970 - recall: 0.9968\nEpoch 43: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0097 - precision: 0.9970 - recall: 0.9968 - val_accuracy: 0.9365 - val_auc: 0.9834 - val_loss: 0.4220 - val_precision: 0.9405 - val_recall: 0.9348\nEpoch 44/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0086 - precision: 0.9974 - recall: 0.9971\nEpoch 44: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0086 - precision: 0.9974 - recall: 0.9971 - val_accuracy: 0.9411 - val_auc: 0.9844 - val_loss: 0.4247 - val_precision: 0.9445 - val_recall: 0.9396\nEpoch 45/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9975 - auc: 1.0000 - loss: 0.0078 - precision: 0.9976 - recall: 0.9974\nEpoch 45: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9975 - auc: 1.0000 - loss: 0.0078 - precision: 0.9976 - recall: 0.9974 - val_accuracy: 0.9358 - val_auc: 0.9831 - val_loss: 0.4490 - val_precision: 0.9393 - val_recall: 0.9354\nEpoch 46/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9963 - auc: 0.9999 - loss: 0.0107 - precision: 0.9964 - recall: 0.9962\nEpoch 46: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9963 - auc: 0.9999 - loss: 0.0107 - precision: 0.9964 - recall: 0.9962 - val_accuracy: 0.9412 - val_auc: 0.9844 - val_loss: 0.4388 - val_precision: 0.9441 - val_recall: 0.9396\nEpoch 47/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0083 - precision: 0.9975 - recall: 0.9973\nEpoch 47: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0083 - precision: 0.9975 - recall: 0.9973 - val_accuracy: 0.9369 - val_auc: 0.9824 - val_loss: 0.5015 - val_precision: 0.9399 - val_recall: 0.9360\nEpoch 48/60\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0102 - precision: 0.9969 - recall: 0.9968\nEpoch 48: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0102 - precision: 0.9969 - recall: 0.9968 - val_accuracy: 0.9367 - val_auc: 0.9832 - val_loss: 0.4355 - val_precision: 0.9409 - val_recall: 0.9354\nEpoch 49/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0082 - precision: 0.9974 - recall: 0.9972\nEpoch 49: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0082 - precision: 0.9974 - recall: 0.9972 - val_accuracy: 0.9380 - val_auc: 0.9837 - val_loss: 0.4379 - val_precision: 0.9407 - val_recall: 0.9364\nEpoch 50/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0076 - precision: 0.9973 - recall: 0.9970\nEpoch 50: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9972 - auc: 1.0000 - loss: 0.0076 - precision: 0.9973 - recall: 0.9970 - val_accuracy: 0.9366 - val_auc: 0.9823 - val_loss: 0.4557 - val_precision: 0.9397 - val_recall: 0.9357\nEpoch 51/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0097 - precision: 0.9971 - recall: 0.9969\nEpoch 51: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0097 - precision: 0.9971 - recall: 0.9969 - val_accuracy: 0.9300 - val_auc: 0.9818 - val_loss: 0.4801 - val_precision: 0.9344 - val_recall: 0.9287\nEpoch 52/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0090 - precision: 0.9974 - recall: 0.9972\nEpoch 52: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0090 - precision: 0.9974 - recall: 0.9972 - val_accuracy: 0.9334 - val_auc: 0.9828 - val_loss: 0.4726 - val_precision: 0.9363 - val_recall: 0.9324\nEpoch 53/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0064 - precision: 0.9980 - recall: 0.9979\nEpoch 53: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0064 - precision: 0.9980 - recall: 0.9979 - val_accuracy: 0.9352 - val_auc: 0.9824 - val_loss: 0.4606 - val_precision: 0.9384 - val_recall: 0.9343\nEpoch 54/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9971 - auc: 0.9999 - loss: 0.0095 - precision: 0.9974 - recall: 0.9970\nEpoch 54: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9971 - auc: 0.9999 - loss: 0.0095 - precision: 0.9974 - recall: 0.9970 - val_accuracy: 0.9342 - val_auc: 0.9832 - val_loss: 0.4722 - val_precision: 0.9380 - val_recall: 0.9332\nEpoch 55/60\n\u001b[1m1830/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 1.0000 - loss: 0.0066 - precision: 0.9979 - recall: 0.9978\nEpoch 55: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9978 - auc: 1.0000 - loss: 0.0066 - precision: 0.9979 - recall: 0.9978 - val_accuracy: 0.9383 - val_auc: 0.9829 - val_loss: 0.4530 - val_precision: 0.9406 - val_recall: 0.9373\nEpoch 56/60\n\u001b[1m1827/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0057 - precision: 0.9979 - recall: 0.9978\nEpoch 56: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0057 - precision: 0.9979 - recall: 0.9978 - val_accuracy: 0.9348 - val_auc: 0.9832 - val_loss: 0.4669 - val_precision: 0.9381 - val_recall: 0.9340\nEpoch 57/60\n\u001b[1m1831/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0076 - precision: 0.9978 - recall: 0.9977\nEpoch 57: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0076 - precision: 0.9978 - recall: 0.9977 - val_accuracy: 0.9391 - val_auc: 0.9831 - val_loss: 0.4711 - val_precision: 0.9417 - val_recall: 0.9383\nEpoch 58/60\n\u001b[1m1829/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0047 - precision: 0.9987 - recall: 0.9985\nEpoch 58: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0047 - precision: 0.9987 - recall: 0.9985 - val_accuracy: 0.9384 - val_auc: 0.9837 - val_loss: 0.4448 - val_precision: 0.9418 - val_recall: 0.9371\nEpoch 59/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 0.9999 - loss: 0.0088 - precision: 0.9973 - recall: 0.9970\nEpoch 59: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9972 - auc: 0.9999 - loss: 0.0088 - precision: 0.9973 - recall: 0.9970 - val_accuracy: 0.9341 - val_auc: 0.9828 - val_loss: 0.4717 - val_precision: 0.9365 - val_recall: 0.9330\nEpoch 60/60\n\u001b[1m1828/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0083 - precision: 0.9977 - recall: 0.9975\nEpoch 60: val_loss did not improve from 0.22874\n\u001b[1m1832/1832\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0083 - precision: 0.9977 - recall: 0.9975 - val_accuracy: 0.9358 - val_auc: 0.9819 - val_loss: 0.4806 - val_precision: 0.9389 - val_recall: 0.9345\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to calculate F1 scores from precision and recall\ndef calculate_f1_scores(precision, recall):\n    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n\ndef print_info(history, model_name):\n    top_3_dict = {}\n    history = history.history\n    \n    print(\"*\" * 50)\n    print(f\"\\n{model_name} Results:\")\n    print(\"*\" * 50)\n    print(\"\\n\")\n\n    # Assuming history['loss'], history['val_loss'], etc., exist\n    training_loss = history['loss']\n    validation_loss = history['val_loss']\n    training_accuracy = history['accuracy']\n    validation_accuracy = history['val_accuracy']\n    training_auc = history['auc']\n    validation_auc = history['val_auc']\n    training_precision = history['precision']\n    validation_precision = history['val_precision']\n    training_recall = history['recall']\n    validation_recall = history['val_recall']\n    \n\n\n    # Calculate F1 scores based on available precision and recall in history\n    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n\n\n    top_3_dict[model_name] = {\n        \"training_loss\": sorted(training_loss)[:3],\n        \"validation_loss\": sorted(validation_loss)[:3],\n        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n\n    }\n\n    # Print Top 3 Lowest Losses\n    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n\n    # Print Top 3 Highest Accuracies\n    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n\n    # Print Top 3 AUCs\n    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n\n    # Print Top 3 F1 Scores\n    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n\n    # Print Top 3 Precision\n    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n\n    # Print Top 3 Recall\n    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:30:19.449032Z","iopub.execute_input":"2024-03-19T13:30:19.449655Z","iopub.status.idle":"2024-03-19T13:30:19.465529Z","shell.execute_reply.started":"2024-03-19T13:30:19.449622Z","shell.execute_reply":"2024-03-19T13:30:19.464693Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print_info(history_relu, \"RELU\")\nprint_info(history_sigmoid, \"SIGMOID\")\nprint_info(history_tanh, \"TANH\")\nprint_info(history_custom, \"DSRELU\")\nprint_info(history_mish, \"MISH\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:31:28.168632Z","iopub.execute_input":"2024-03-19T13:31:28.169495Z","iopub.status.idle":"2024-03-19T13:31:28.177548Z","shell.execute_reply.started":"2024-03-19T13:31:28.169448Z","shell.execute_reply":"2024-03-19T13:31:28.176603Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"**************************************************\n\nRELU Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.009875877760350704, 0.009910495951771736, 0.011064033955335617]\nTop 3 Lowest Validation Losses: [0.22091799974441528, 0.23693464696407318, 0.23875713348388672]\nTop 3 Highest Training Accuracies: [0.9967067837715149, 0.9965019822120667, 0.9964849352836609]\nTop 3 Highest Validation Accuracies: [0.9423969388008118, 0.942328691482544, 0.9422604441642761]\nTop 3 Training AUCs: [0.9999482035636902, 0.9999189972877502, 0.9999024868011475]\nTop 3 Validation AUCs: [0.9942930340766907, 0.9941419363021851, 0.993573784828186]\nTop 3 Training F1 Scores: [0.996731984271196, 0.9965267465160724, 0.9964673284015821]\nTop 3 Validation F1 Scores: [0.9439374265724321, 0.942949898165159, 0.9426764625876604]\nTop 3 Training Precision: [0.9968425631523132, 0.9967734217643738, 0.9966204166412354]\nTop 3 Validation Precision: [0.9570642709732056, 0.9522531628608704, 0.9509617686271667]\nTop 3 Training Recall: [0.9966214299201965, 0.996314287185669, 0.9962801933288574]\nTop 3 Validation Recall: [0.9413731694221497, 0.9411001801490784, 0.9402812123298645]\n**************************************************\n\nSIGMOID Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.026160383597016335, 0.027168136090040207, 0.027773980051279068]\nTop 3 Lowest Validation Losses: [0.33470436930656433, 0.3439169228076935, 0.34505724906921387]\nTop 3 Highest Training Accuracies: [0.9910417199134827, 0.9908710718154907, 0.9908369779586792]\nTop 3 Highest Validation Accuracies: [0.9165984392166138, 0.9165984392166138, 0.9161888957023621]\nTop 3 Training AUCs: [0.999841570854187, 0.9997912645339966, 0.9997791647911072]\nTop 3 Validation AUCs: [0.9906296133995056, 0.9894407987594604, 0.9893085360527039]\nTop 3 Training F1 Scores: [nan, 0.9909831481305998, 0.9909477407242676]\nTop 3 Validation F1 Scores: [nan, 0.9187592239585598, 0.9187114331528717]\nTop 3 Training Precision: [0.9918964505195618, 0.9917961359024048, 0.991675078868866]\nTop 3 Validation Precision: [0.9277192950248718, 0.9259819984436035, 0.9248910546302795]\nTop 3 Training Recall: [0.990171492099762, 0.9901544451713562, 0.99000084400177]\nTop 3 Validation Recall: [0.9151651859283447, 0.9148238897323608, 0.9127081632614136]\n**************************************************\n\nTANH Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.016078343614935875, 0.01659521646797657, 0.016850484535098076]\nTop 3 Lowest Validation Losses: [0.35137826204299927, 0.3737342655658722, 0.38080713152885437]\nTop 3 Highest Training Accuracies: [0.9945908784866333, 0.9944543838500977, 0.9941643476486206]\nTop 3 Highest Validation Accuracies: [0.9223996996879578, 0.9188506603240967, 0.9176903963088989]\nTop 3 Training AUCs: [0.9999104142189026, 0.9999094605445862, 0.9998732805252075]\nTop 3 Validation AUCs: [0.987468421459198, 0.987395703792572, 0.9866131544113159]\nTop 3 Training F1 Scores: [0.9945546643749463, 0.9944952518663667, 0.9941454485377413]\nTop 3 Validation F1 Scores: [0.9233454177565993, 0.9197330056521736, 0.9188726363130403]\nTop 3 Training Precision: [0.9949452877044678, 0.9948263764381409, 0.9945850968360901]\nTop 3 Validation Precision: [0.9259437322616577, 0.9254422783851624, 0.9225434064865112]\nTop 3 Training Recall: [0.9941643476486206, 0.9941643476486206, 0.9938400983810425]\nTop 3 Validation Recall: [0.9207616448402405, 0.9169396758079529, 0.9156429171562195]\n**************************************************\n\nDSRELU Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.04601069539785385, 0.04685546085238457, 0.047607772052288055]\nTop 3 Lowest Validation Losses: [0.23691166937351227, 0.2444552779197693, 0.25597915053367615]\nTop 3 Highest Training Accuracies: [0.9843869805335999, 0.9841992855072021, 0.9840286374092102]\nTop 3 Highest Validation Accuracies: [0.9337974190711975, 0.9297024011611938, 0.9286786913871765]\nTop 3 Training AUCs: [0.9995156526565552, 0.9994999766349792, 0.9994751214981079]\nTop 3 Validation AUCs: [0.9942924976348877, 0.9939771294593811, 0.9936438798904419]\nTop 3 Training F1 Scores: [0.9847155955083091, 0.9845157485504089, 0.9842621449480751]\nTop 3 Validation F1 Scores: [0.93663532979891, 0.9319093106408831, 0.9318236957598421]\nTop 3 Training Precision: [0.9866385459899902, 0.9862096309661865, 0.9860999584197998]\nTop 3 Validation Precision: [0.9508054852485657, 0.9495091438293457, 0.9486598968505859]\nTop 3 Training Recall: [0.9829366207122803, 0.9828001260757446, 0.9823223352432251]\nTop 3 Validation Recall: [0.9241059422492981, 0.9224679470062256, 0.9202156662940979]\n**************************************************\n\nMISH Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.008089900948107243, 0.008747168816626072, 0.009436916559934616]\nTop 3 Lowest Validation Losses: [0.22873800992965698, 0.23229435086250305, 0.2341645210981369]\nTop 3 Highest Training Accuracies: [0.9973551630973816, 0.9973381161689758, 0.9969285726547241]\nTop 3 Highest Validation Accuracies: [0.9415096640586853, 0.9415096640586853, 0.941236674785614]\nTop 3 Training AUCs: [0.9999300241470337, 0.999928891658783, 0.9999211430549622]\nTop 3 Validation AUCs: [0.9940751791000366, 0.9934651255607605, 0.9933214783668518]\nTop 3 Training F1 Scores: [0.9973292824106164, 0.9973035815508895, 0.9969197090956141]\nTop 3 Validation F1 Scores: [0.9439525972545836, 0.9421089039021362, 0.9420418821001433]\nTop 3 Training Precision: [0.9974567890167236, 0.9974399209022522, 0.9970303177833557]\nTop 3 Validation Precision: [0.9529804587364197, 0.9526442885398865, 0.9521567821502686]\nTop 3 Training Recall: [0.997218668460846, 0.9971504211425781, 0.9968091249465942]\nTop 3 Validation Recall: [0.9401447176933289, 0.9395986795425415, 0.9395986795425415]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1976106723.py:3: RuntimeWarning: invalid value encountered in divide\n  return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n","output_type":"stream"}]},{"cell_type":"code","source":"model_mish = histories[\"mish\"][1]\nhistories[\"mish\"] = histories[\"mish\"][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:24:38.783052Z","iopub.status.idle":"2024-03-19T13:24:38.783464Z","shell.execute_reply.started":"2024-03-19T13:24:38.783282Z","shell.execute_reply":"2024-03-19T13:24:38.783303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting training and validation accuracies\ntrain_accuracies_relu = history_custom_relu['accuracy']\nval_accuracies_relu = history_relu['val_accuracy']\n\ntrain_accuracies_sigmoid = history_sigmoid['accuracy']\nval_accuracies_sigmoid = history_sigmoid['val_accuracy']\n\ntrain_accuracies_tanh = history_tanh['accuracy']\nval_accuracies_tanh = history_tanh['val_accuracy']\n\ntrain_accuracies_custom_relu = history_custom_relu['accuracy']\nval_accuracies_custom_relu = history_custom_relu['val_accuracy']\n\n# Display the accuracies for each model\nprint(\"ReLU Model - Training Accuracy:\", train_accuracies_relu[-1], \"Validation Accuracy:\", val_accuracies_relu[-1])\nprint(\"Sigmoid Model - Training Accuracy:\", train_accuracies_sigmoid[-1], \"Validation Accuracy:\", val_accuracies_sigmoid[-1])\nprint(\"Tanh Model - Training Accuracy:\", train_accuracies_tanh[-1], \"Validation Accuracy:\", val_accuracies_tanh[-1])\nprint(\"Custom ReLU Model - Training Accuracy:\", train_accuracies_custom_relu[-1], \"Validation Accuracy:\", val_accuracies_custom_relu[-1])","metadata":{"id":"SJc-0gawD2dp","execution":{"iopub.status.busy":"2024-03-19T13:24:38.784984Z","iopub.status.idle":"2024-03-19T13:24:38.785346Z","shell.execute_reply.started":"2024-03-19T13:24:38.785175Z","shell.execute_reply":"2024-03-19T13:24:38.785190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_Save = {\n    \"relu\" : histories[\"relu\"].history, \n    \"sigmoid\" : histories[\"sigmoid\"].history, \n    \"tanh\" : histories[\"tanh\"].history, \n    \"custom\" : histories[\"custom\"].history, \n    \"mish\" : histories[\"mish\"].history}","metadata":{"id":"lzul6M8EPGCx","execution":{"iopub.status.busy":"2024-03-19T13:24:38.786383Z","iopub.status.idle":"2024-03-19T13:24:38.786716Z","shell.execute_reply.started":"2024-03-19T13:24:38.786555Z","shell.execute_reply":"2024-03-19T13:24:38.786569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dumping the dictionary into a pickle file\nimport pickle \n\nfile_path = \"history_svhn.pkl\"\nwith open(file_path, 'wb') as file:\n    pickle.dump(history_Save, file)\n\nfile_path","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:24:38.788048Z","iopub.status.idle":"2024-03-19T13:24:38.788409Z","shell.execute_reply.started":"2024-03-19T13:24:38.788229Z","shell.execute_reply":"2024-03-19T13:24:38.788243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"histories","metadata":{"execution":{"iopub.status.busy":"2024-03-19T13:24:38.789626Z","iopub.status.idle":"2024-03-19T13:24:38.789962Z","shell.execute_reply.started":"2024-03-19T13:24:38.789794Z","shell.execute_reply":"2024-03-19T13:24:38.789809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}