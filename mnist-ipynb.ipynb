{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow_datasets as tfds\nimport tensorflow as tf\nimport numpy as np\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:34.526729Z","iopub.execute_input":"2024-03-16T10:58:34.527394Z","iopub.status.idle":"2024-03-16T10:58:34.532091Z","shell.execute_reply.started":"2024-03-16T10:58:34.527350Z","shell.execute_reply":"2024-03-16T10:58:34.531101Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\n\n# Load the MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Normalize the images to the range 0-1\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\n# Reshape dataset to have a single channel\nx_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\nx_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n\n# Convert class vectors to binary class matrices (one-hot encoding)\nnum_classes = 10\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)\n\nprint('MNIST dataset loaded and preprocessed.')\nprint('Training set size:', x_train.shape, y_train.shape)\nprint('Test set size:', x_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:35.609790Z","iopub.execute_input":"2024-03-16T10:58:35.610454Z","iopub.status.idle":"2024-03-16T10:58:36.981040Z","shell.execute_reply.started":"2024-03-16T10:58:35.610423Z","shell.execute_reply":"2024-03-16T10:58:36.980118Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nMNIST dataset loaded and preprocessed.\nTraining set size: (60000, 28, 28, 1) (60000, 10)\nTest set size: (10000, 28, 28, 1) (10000, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the original training data to create a new training set and a validation set\nx_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n    x_train, y_train, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:39.300030Z","iopub.execute_input":"2024-03-16T10:58:39.300606Z","iopub.status.idle":"2024-03-16T10:58:39.812770Z","shell.execute_reply.started":"2024-03-16T10:58:39.300573Z","shell.execute_reply":"2024-03-16T10:58:39.811948Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#  Print the shapes of the splits to verify\nprint(\"Shape of new training images:\", x_train_split.shape)\nprint(\"Shape of new training labels:\", y_train_split.shape)\nprint(\"Shape of validation images:\", x_val_split.shape)\nprint(\"Shape of validation labels:\", y_val_split.shape)\n\n\n# Check the shapes of testing data after preprocessing\nprint(\"Shape of test images:\", x_test.shape)\nprint(\"Shape of test labels:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:40.140296Z","iopub.execute_input":"2024-03-16T10:58:40.141467Z","iopub.status.idle":"2024-03-16T10:58:40.147324Z","shell.execute_reply.started":"2024-03-16T10:58:40.141431Z","shell.execute_reply":"2024-03-16T10:58:40.146336Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Shape of new training images: (48000, 28, 28, 1)\nShape of new training labels: (48000, 10)\nShape of validation images: (12000, 28, 28, 1)\nShape of validation labels: (12000, 10)\nShape of test images: (10000, 28, 28, 1)\nShape of test labels: (10000, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \\\nActivation, Add, AveragePooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\ndef resnet_block(input_data, filters, conv_size, activation_func):\n    x = Conv2D(filters, conv_size, padding='same')(input_data)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    x = Conv2D(filters, conv_size, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    # Adding the input data to the output of the block (Skip Connection)\n    x = Add()([x, input_data])\n\n    x = Activation(activation_func)(x)\n    return x\n\ndef build_resnet20(input_shape, num_classes, activation_func):\n    inputs = Input(shape=input_shape)\n\n    # Initial Conv Layer\n    x = Conv2D(16, (3, 3), padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    # ResNet Blocks\n    for _ in range(3):\n        x = resnet_block(x, 16, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 32, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 64, (3, 3), activation_func)\n\n    # Average Pooling\n    x = AveragePooling2D(pool_size=(7, 7))(x)\n    x = Flatten()(x)\n\n    # Output Layer\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Custom activation function\ndef custom_activation(x):\n    # Define your custom activation logic here\n    return tf.nn.relu(x)  # Example: using ReLU as a placeholder\n\n# Building the model with the custom activation function\ninput_shape = (32, 32, 3)  # Change based on your dataset\nnum_classes = 10  # Change based on your dataset\n\nmodel = build_resnet20(input_shape, num_classes, custom_activation)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:44.304130Z","iopub.execute_input":"2024-03-16T10:58:44.304964Z","iopub.status.idle":"2024-03-16T10:58:45.625881Z","shell.execute_reply.started":"2024-03-16T10:58:44.304928Z","shell.execute_reply":"2024-03-16T10:58:45.624907Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │        \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │      \u001b[38;5;34m2,320\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │         \u001b[38;5;34m64\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m16\u001b[0m)               │            │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m4,640\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_8        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_12       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │      \u001b[38;5;34m9,248\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m]… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │ \u001b[38;5;34m32\u001b[0m)               │            │ activation_11[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_13       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m18,496\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_14       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_14[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_15       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │                   │            │ activation_14[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_16       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_16[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_17       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_17[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │                   │            │ activation_16[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_18       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_19       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m36,928\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n│                     │                   │            │ activation_18[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_20       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ average_pooling2d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ average_pooling2… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m650\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_8        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_12       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_13       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_14       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_15       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │                   │            │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_16       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_17       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │                   │            │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_18       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_19       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n│                     │                   │            │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_20       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ average_pooling2d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling2… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m318,346\u001b[0m (1.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">318,346</span> (1.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m316,778\u001b[0m (1.21 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">316,778</span> (1.21 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,568\u001b[0m (6.12 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> (6.12 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"def train_model(activation_func, x_train, y_train, x_val, y_val, batch_size, learning_rate, name):\n    # Build the model\n    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1], activation_func=activation_func)\n\n    # Compile the model with specified learning rate\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                  metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n    \n        # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        f'{name}.keras', # Path where to save the model\n        save_best_only=True, # Only save a model if `val_loss` has improved\n        save_weights_only = False,\n        monitor='val_loss', # Monitor the validation loss\n        mode='min', # The lower the validation loss, the better the model\n        verbose=1 # Log a message when a better model is found\n    )\n\n\n    # Train the model with specified batch size\n    history = model.fit(x_train, y_train, epochs=60, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1, callbacks = [checkpoint_cb])\n\n    return history, model\n\n\n\n# Parameters\nbatch_size = 32\nlearning_rate = 0.005\n\n# Activation functions to try\nactivation_functions = [tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\nnames = [\"relu\", \"sigmoid\", \"tanh\"]\nhistories = {}\n\n# Train and evaluate the model with each activation function\nprint(f\"Training with Relu activation function\")\nhistory_relu, model_relu = train_model(tf.nn.relu, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"relu\")\nhistories[\"relu\"] = history_relu\n\nprint(f\"\\n\\n Training with Sigmoid activation function\")\nhistory_sigmoid, model_sig = train_model( tf.nn.sigmoid, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"sigmoid\")\nhistories[\"sigmoid\"] = history_sigmoid\n\n\nprint(f\"\\n\\n Training with Tanh activation function\")\nhistory_tanh, model_tanh = train_model(tf.nn.tanh, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"tanh\")\nhistories[\"tanh\"] = history_tanh","metadata":{"execution":{"iopub.status.busy":"2024-03-16T11:18:19.248887Z","iopub.execute_input":"2024-03-16T11:18:19.249242Z","iopub.status.idle":"2024-03-16T12:04:43.434639Z","shell.execute_reply.started":"2024-03-16T11:18:19.249213Z","shell.execute_reply":"2024-03-16T12:04:43.433761Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Training with Relu activation function\nEpoch 1/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9043 - auc: 0.9879 - loss: 0.3089 - precision: 0.9421 - recall: 0.8771\nEpoch 1: val_loss improved from inf to 0.17684, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - accuracy: 0.9045 - auc: 0.9879 - loss: 0.3082 - precision: 0.9422 - recall: 0.8774 - val_accuracy: 0.9542 - val_auc: 0.9949 - val_loss: 0.1768 - val_precision: 0.9576 - val_recall: 0.9517\nEpoch 2/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9799 - auc: 0.9988 - loss: 0.0678 - precision: 0.9821 - recall: 0.9776\nEpoch 2: val_loss improved from 0.17684 to 0.07466, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9799 - auc: 0.9988 - loss: 0.0678 - precision: 0.9821 - recall: 0.9776 - val_accuracy: 0.9786 - val_auc: 0.9981 - val_loss: 0.0747 - val_precision: 0.9817 - val_recall: 0.9766\nEpoch 3/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9846 - auc: 0.9992 - loss: 0.0489 - precision: 0.9866 - recall: 0.9834\nEpoch 3: val_loss improved from 0.07466 to 0.06082, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9846 - auc: 0.9992 - loss: 0.0489 - precision: 0.9866 - recall: 0.9834 - val_accuracy: 0.9832 - val_auc: 0.9984 - val_loss: 0.0608 - val_precision: 0.9841 - val_recall: 0.9826\nEpoch 4/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9868 - auc: 0.9994 - loss: 0.0435 - precision: 0.9879 - recall: 0.9858\nEpoch 4: val_loss improved from 0.06082 to 0.03643, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9868 - auc: 0.9994 - loss: 0.0435 - precision: 0.9879 - recall: 0.9858 - val_accuracy: 0.9888 - val_auc: 0.9993 - val_loss: 0.0364 - val_precision: 0.9895 - val_recall: 0.9881\nEpoch 5/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9888 - auc: 0.9995 - loss: 0.0365 - precision: 0.9898 - recall: 0.9882\nEpoch 5: val_loss did not improve from 0.03643\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9888 - auc: 0.9995 - loss: 0.0365 - precision: 0.9898 - recall: 0.9882 - val_accuracy: 0.9847 - val_auc: 0.9992 - val_loss: 0.0495 - val_precision: 0.9860 - val_recall: 0.9835\nEpoch 6/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9911 - auc: 0.9995 - loss: 0.0297 - precision: 0.9921 - recall: 0.9903\nEpoch 6: val_loss did not improve from 0.03643\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9911 - auc: 0.9995 - loss: 0.0297 - precision: 0.9921 - recall: 0.9903 - val_accuracy: 0.9877 - val_auc: 0.9992 - val_loss: 0.0383 - val_precision: 0.9880 - val_recall: 0.9871\nEpoch 7/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9923 - auc: 0.9997 - loss: 0.0246 - precision: 0.9931 - recall: 0.9917\nEpoch 7: val_loss did not improve from 0.03643\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9923 - auc: 0.9997 - loss: 0.0246 - precision: 0.9931 - recall: 0.9917 - val_accuracy: 0.9803 - val_auc: 0.9983 - val_loss: 0.0681 - val_precision: 0.9827 - val_recall: 0.9796\nEpoch 8/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9925 - auc: 0.9997 - loss: 0.0232 - precision: 0.9930 - recall: 0.9921\nEpoch 8: val_loss did not improve from 0.03643\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9925 - auc: 0.9997 - loss: 0.0232 - precision: 0.9930 - recall: 0.9921 - val_accuracy: 0.9889 - val_auc: 0.9992 - val_loss: 0.0377 - val_precision: 0.9892 - val_recall: 0.9884\nEpoch 9/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9946 - auc: 0.9998 - loss: 0.0170 - precision: 0.9950 - recall: 0.9943\nEpoch 9: val_loss improved from 0.03643 to 0.03438, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9946 - auc: 0.9998 - loss: 0.0170 - precision: 0.9950 - recall: 0.9943 - val_accuracy: 0.9898 - val_auc: 0.9992 - val_loss: 0.0344 - val_precision: 0.9909 - val_recall: 0.9893\nEpoch 10/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9948 - auc: 0.9999 - loss: 0.0154 - precision: 0.9952 - recall: 0.9945\nEpoch 10: val_loss did not improve from 0.03438\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9948 - auc: 0.9999 - loss: 0.0154 - precision: 0.9952 - recall: 0.9945 - val_accuracy: 0.9892 - val_auc: 0.9987 - val_loss: 0.0422 - val_precision: 0.9902 - val_recall: 0.9888\nEpoch 11/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9954 - auc: 0.9998 - loss: 0.0150 - precision: 0.9957 - recall: 0.9951\nEpoch 11: val_loss did not improve from 0.03438\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9954 - auc: 0.9998 - loss: 0.0150 - precision: 0.9957 - recall: 0.9951 - val_accuracy: 0.9897 - val_auc: 0.9991 - val_loss: 0.0369 - val_precision: 0.9901 - val_recall: 0.9896\nEpoch 12/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9957 - auc: 0.9998 - loss: 0.0136 - precision: 0.9963 - recall: 0.9956\nEpoch 12: val_loss improved from 0.03438 to 0.02439, saving model to relu.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9957 - auc: 0.9998 - loss: 0.0136 - precision: 0.9963 - recall: 0.9956 - val_accuracy: 0.9925 - val_auc: 0.9994 - val_loss: 0.0244 - val_precision: 0.9931 - val_recall: 0.9923\nEpoch 13/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0102 - precision: 0.9970 - recall: 0.9967\nEpoch 13: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0102 - precision: 0.9970 - recall: 0.9967 - val_accuracy: 0.9931 - val_auc: 0.9993 - val_loss: 0.0252 - val_precision: 0.9936 - val_recall: 0.9930\nEpoch 14/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9972 - auc: 0.9997 - loss: 0.0112 - precision: 0.9972 - recall: 0.9969\nEpoch 14: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 0.9997 - loss: 0.0112 - precision: 0.9972 - recall: 0.9969 - val_accuracy: 0.9909 - val_auc: 0.9987 - val_loss: 0.0370 - val_precision: 0.9912 - val_recall: 0.9907\nEpoch 15/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0110 - precision: 0.9966 - recall: 0.9964\nEpoch 15: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0110 - precision: 0.9966 - recall: 0.9964 - val_accuracy: 0.9921 - val_auc: 0.9991 - val_loss: 0.0320 - val_precision: 0.9924 - val_recall: 0.9916\nEpoch 16/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0099 - precision: 0.9969 - recall: 0.9967\nEpoch 16: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0099 - precision: 0.9969 - recall: 0.9967 - val_accuracy: 0.9929 - val_auc: 0.9995 - val_loss: 0.0267 - val_precision: 0.9937 - val_recall: 0.9925\nEpoch 17/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0075 - precision: 0.9975 - recall: 0.9973\nEpoch 17: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0075 - precision: 0.9975 - recall: 0.9973 - val_accuracy: 0.9932 - val_auc: 0.9993 - val_loss: 0.0263 - val_precision: 0.9933 - val_recall: 0.9930\nEpoch 18/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0067 - precision: 0.9979 - recall: 0.9976\nEpoch 18: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0067 - precision: 0.9979 - recall: 0.9976 - val_accuracy: 0.9912 - val_auc: 0.9989 - val_loss: 0.0369 - val_precision: 0.9914 - val_recall: 0.9912\nEpoch 19/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0070 - precision: 0.9977 - recall: 0.9975\nEpoch 19: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0070 - precision: 0.9977 - recall: 0.9975 - val_accuracy: 0.9935 - val_auc: 0.9991 - val_loss: 0.0282 - val_precision: 0.9937 - val_recall: 0.9933\nEpoch 20/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 0.9999 - loss: 0.0058 - precision: 0.9982 - recall: 0.9982\nEpoch 20: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 0.9999 - loss: 0.0058 - precision: 0.9982 - recall: 0.9982 - val_accuracy: 0.9895 - val_auc: 0.9983 - val_loss: 0.0485 - val_precision: 0.9896 - val_recall: 0.9890\nEpoch 21/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 0.9999 - loss: 0.0062 - precision: 0.9983 - recall: 0.9981\nEpoch 21: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 0.9999 - loss: 0.0062 - precision: 0.9983 - recall: 0.9981 - val_accuracy: 0.9926 - val_auc: 0.9988 - val_loss: 0.0328 - val_precision: 0.9928 - val_recall: 0.9925\nEpoch 22/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0057 - precision: 0.9984 - recall: 0.9982\nEpoch 22: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0057 - precision: 0.9984 - recall: 0.9982 - val_accuracy: 0.9918 - val_auc: 0.9987 - val_loss: 0.0374 - val_precision: 0.9918 - val_recall: 0.9918\nEpoch 23/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0054 - precision: 0.9983 - recall: 0.9982\nEpoch 23: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0054 - precision: 0.9983 - recall: 0.9982 - val_accuracy: 0.9907 - val_auc: 0.9985 - val_loss: 0.0459 - val_precision: 0.9910 - val_recall: 0.9906\nEpoch 24/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0050 - precision: 0.9982 - recall: 0.9982\nEpoch 24: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0050 - precision: 0.9982 - recall: 0.9982 - val_accuracy: 0.9924 - val_auc: 0.9987 - val_loss: 0.0361 - val_precision: 0.9927 - val_recall: 0.9923\nEpoch 25/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0040 - precision: 0.9985 - recall: 0.9985\nEpoch 25: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0040 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9828 - val_auc: 0.9964 - val_loss: 0.0973 - val_precision: 0.9832 - val_recall: 0.9827\nEpoch 26/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0039 - precision: 0.9987 - recall: 0.9985\nEpoch 26: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0039 - precision: 0.9987 - recall: 0.9985 - val_accuracy: 0.9873 - val_auc: 0.9976 - val_loss: 0.0648 - val_precision: 0.9878 - val_recall: 0.9872\nEpoch 27/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0067 - precision: 0.9982 - recall: 0.9981\nEpoch 27: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0067 - precision: 0.9982 - recall: 0.9981 - val_accuracy: 0.9933 - val_auc: 0.9989 - val_loss: 0.0292 - val_precision: 0.9934 - val_recall: 0.9933\nEpoch 28/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991\nEpoch 28: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9944 - val_auc: 0.9990 - val_loss: 0.0273 - val_precision: 0.9946 - val_recall: 0.9942\nEpoch 29/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0048 - precision: 0.9984 - recall: 0.9983\nEpoch 29: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0048 - precision: 0.9984 - recall: 0.9983 - val_accuracy: 0.9919 - val_auc: 0.9988 - val_loss: 0.0375 - val_precision: 0.9922 - val_recall: 0.9918\nEpoch 30/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0052 - precision: 0.9987 - recall: 0.9985\nEpoch 30: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0052 - precision: 0.9987 - recall: 0.9985 - val_accuracy: 0.9918 - val_auc: 0.9986 - val_loss: 0.0394 - val_precision: 0.9922 - val_recall: 0.9917\nEpoch 31/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0029 - precision: 0.9991 - recall: 0.9990\nEpoch 31: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0029 - precision: 0.9991 - recall: 0.9990 - val_accuracy: 0.9909 - val_auc: 0.9984 - val_loss: 0.0419 - val_precision: 0.9911 - val_recall: 0.9906\nEpoch 32/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9992 - auc: 0.9999 - loss: 0.0038 - precision: 0.9992 - recall: 0.9991\nEpoch 32: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 0.9999 - loss: 0.0038 - precision: 0.9992 - recall: 0.9991 - val_accuracy: 0.9919 - val_auc: 0.9984 - val_loss: 0.0414 - val_precision: 0.9920 - val_recall: 0.9918\nEpoch 33/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0057 - precision: 0.9983 - recall: 0.9983\nEpoch 33: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0057 - precision: 0.9983 - recall: 0.9983 - val_accuracy: 0.9920 - val_auc: 0.9984 - val_loss: 0.0434 - val_precision: 0.9920 - val_recall: 0.9920\nEpoch 34/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0036 - precision: 0.9987 - recall: 0.9987\nEpoch 34: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0036 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9935 - val_auc: 0.9988 - val_loss: 0.0316 - val_precision: 0.9936 - val_recall: 0.9934\nEpoch 35/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0021 - precision: 0.9993 - recall: 0.9993\nEpoch 35: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0021 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9902 - val_auc: 0.9981 - val_loss: 0.0492 - val_precision: 0.9904 - val_recall: 0.9901\nEpoch 36/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0019 - precision: 0.9994 - recall: 0.9993\nEpoch 36: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0019 - precision: 0.9994 - recall: 0.9993 - val_accuracy: 0.9928 - val_auc: 0.9986 - val_loss: 0.0406 - val_precision: 0.9932 - val_recall: 0.9927\nEpoch 37/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0026 - precision: 0.9992 - recall: 0.9991\nEpoch 37: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0026 - precision: 0.9992 - recall: 0.9991 - val_accuracy: 0.9933 - val_auc: 0.9988 - val_loss: 0.0335 - val_precision: 0.9934 - val_recall: 0.9931\nEpoch 38/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995\nEpoch 38: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9903 - val_auc: 0.9978 - val_loss: 0.0625 - val_precision: 0.9903 - val_recall: 0.9902\nEpoch 39/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0045 - precision: 0.9986 - recall: 0.9986\nEpoch 39: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0045 - precision: 0.9986 - recall: 0.9986 - val_accuracy: 0.9902 - val_auc: 0.9983 - val_loss: 0.0488 - val_precision: 0.9904 - val_recall: 0.9898\nEpoch 40/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991\nEpoch 40: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9948 - val_auc: 0.9986 - val_loss: 0.0346 - val_precision: 0.9948 - val_recall: 0.9948\nEpoch 41/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0029 - precision: 0.9991 - recall: 0.9990\nEpoch 41: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0029 - precision: 0.9991 - recall: 0.9990 - val_accuracy: 0.9850 - val_auc: 0.9977 - val_loss: 0.0678 - val_precision: 0.9854 - val_recall: 0.9848\nEpoch 42/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.0032 - precision: 0.9990 - recall: 0.9988\nEpoch 42: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.0032 - precision: 0.9990 - recall: 0.9988 - val_accuracy: 0.9936 - val_auc: 0.9986 - val_loss: 0.0372 - val_precision: 0.9937 - val_recall: 0.9935\nEpoch 43/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 3.9432e-04 - precision: 0.9999 - recall: 0.9999\nEpoch 43: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 3.9435e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9940 - val_auc: 0.9986 - val_loss: 0.0367 - val_precision: 0.9942 - val_recall: 0.9940\nEpoch 44/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 0.9998 - loss: 0.0063 - precision: 0.9982 - recall: 0.9982\nEpoch 44: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 0.9998 - loss: 0.0063 - precision: 0.9982 - recall: 0.9982 - val_accuracy: 0.9934 - val_auc: 0.9986 - val_loss: 0.0330 - val_precision: 0.9935 - val_recall: 0.9934\nEpoch 45/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0011 - precision: 0.9997 - recall: 0.9996\nEpoch 45: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0011 - precision: 0.9997 - recall: 0.9996 - val_accuracy: 0.9931 - val_auc: 0.9986 - val_loss: 0.0429 - val_precision: 0.9932 - val_recall: 0.9928\nEpoch 46/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995\nEpoch 46: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9923 - val_auc: 0.9985 - val_loss: 0.0421 - val_precision: 0.9925 - val_recall: 0.9922\nEpoch 47/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0032 - precision: 0.9992 - recall: 0.9991\nEpoch 47: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0032 - precision: 0.9992 - recall: 0.9991 - val_accuracy: 0.9924 - val_auc: 0.9983 - val_loss: 0.0452 - val_precision: 0.9927 - val_recall: 0.9923\nEpoch 48/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0011 - precision: 0.9996 - recall: 0.9996\nEpoch 48: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0011 - precision: 0.9996 - recall: 0.9996 - val_accuracy: 0.9886 - val_auc: 0.9981 - val_loss: 0.0546 - val_precision: 0.9891 - val_recall: 0.9884\nEpoch 49/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0051 - precision: 0.9984 - recall: 0.9984\nEpoch 49: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0051 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.9940 - val_auc: 0.9986 - val_loss: 0.0371 - val_precision: 0.9942 - val_recall: 0.9939\nEpoch 50/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0021 - precision: 0.9995 - recall: 0.9995\nEpoch 50: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0021 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9929 - val_auc: 0.9986 - val_loss: 0.0412 - val_precision: 0.9931 - val_recall: 0.9927\nEpoch 51/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991\nEpoch 51: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9921 - val_auc: 0.9981 - val_loss: 0.0521 - val_precision: 0.9921 - val_recall: 0.9919\nEpoch 52/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0016 - precision: 0.9995 - recall: 0.9995\nEpoch 52: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0016 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9852 - val_auc: 0.9968 - val_loss: 0.0935 - val_precision: 0.9854 - val_recall: 0.9850\nEpoch 53/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0037 - precision: 0.9986 - recall: 0.9986\nEpoch 53: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0037 - precision: 0.9986 - recall: 0.9986 - val_accuracy: 0.9935 - val_auc: 0.9986 - val_loss: 0.0356 - val_precision: 0.9937 - val_recall: 0.9933\nEpoch 54/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0018 - precision: 0.9994 - recall: 0.9994\nEpoch 54: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0018 - precision: 0.9994 - recall: 0.9994 - val_accuracy: 0.9933 - val_auc: 0.9986 - val_loss: 0.0362 - val_precision: 0.9936 - val_recall: 0.9933\nEpoch 55/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 4.3968e-04 - precision: 1.0000 - recall: 0.9999\nEpoch 55: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 4.4037e-04 - precision: 1.0000 - recall: 0.9999 - val_accuracy: 0.9922 - val_auc: 0.9980 - val_loss: 0.0525 - val_precision: 0.9922 - val_recall: 0.9921\nEpoch 56/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0039 - precision: 0.9991 - recall: 0.9990\nEpoch 56: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 0.9999 - loss: 0.0039 - precision: 0.9991 - recall: 0.9990 - val_accuracy: 0.9934 - val_auc: 0.9988 - val_loss: 0.0332 - val_precision: 0.9937 - val_recall: 0.9933\nEpoch 57/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0016 - precision: 0.9996 - recall: 0.9995\nEpoch 57: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0016 - precision: 0.9996 - recall: 0.9995 - val_accuracy: 0.9923 - val_auc: 0.9986 - val_loss: 0.0385 - val_precision: 0.9925 - val_recall: 0.9923\nEpoch 58/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0015 - precision: 0.9995 - recall: 0.9995\nEpoch 58: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0015 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9907 - val_auc: 0.9982 - val_loss: 0.0525 - val_precision: 0.9908 - val_recall: 0.9905\nEpoch 59/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0023 - precision: 0.9993 - recall: 0.9993\nEpoch 59: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0023 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9934 - val_auc: 0.9987 - val_loss: 0.0358 - val_precision: 0.9937 - val_recall: 0.9932\nEpoch 60/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 3.0322e-04 - precision: 0.9999 - recall: 0.9999\nEpoch 60: val_loss did not improve from 0.02439\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9999 - auc: 1.0000 - loss: 3.0337e-04 - precision: 0.9999 - recall: 0.9999 - val_accuracy: 0.9945 - val_auc: 0.9990 - val_loss: 0.0332 - val_precision: 0.9947 - val_recall: 0.9943\n\n\n Training with Sigmoid activation function\nEpoch 1/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6595 - auc: 0.9328 - loss: 0.9745 - precision: 0.8958 - recall: 0.4945\nEpoch 1: val_loss improved from inf to 0.97906, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - accuracy: 0.6600 - auc: 0.9329 - loss: 0.9733 - precision: 0.8959 - recall: 0.4952 - val_accuracy: 0.6825 - val_auc: 0.9498 - val_loss: 0.9791 - val_precision: 0.7325 - val_recall: 0.6462\nEpoch 2/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9569 - auc: 0.9977 - loss: 0.1450 - precision: 0.9634 - recall: 0.9504\nEpoch 2: val_loss did not improve from 0.97906\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9569 - auc: 0.9977 - loss: 0.1450 - precision: 0.9634 - recall: 0.9504 - val_accuracy: 0.5154 - val_auc: 0.8069 - val_loss: 2.8181 - val_precision: 0.5317 - val_recall: 0.5017\nEpoch 3/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9706 - auc: 0.9984 - loss: 0.0991 - precision: 0.9747 - recall: 0.9673\nEpoch 3: val_loss did not improve from 0.97906\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9706 - auc: 0.9984 - loss: 0.0991 - precision: 0.9747 - recall: 0.9673 - val_accuracy: 0.6173 - val_auc: 0.8811 - val_loss: 1.8114 - val_precision: 0.6513 - val_recall: 0.5957\nEpoch 4/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9775 - auc: 0.9985 - loss: 0.0789 - precision: 0.9802 - recall: 0.9747\nEpoch 4: val_loss did not improve from 0.97906\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9775 - auc: 0.9985 - loss: 0.0789 - precision: 0.9802 - recall: 0.9747 - val_accuracy: 0.5483 - val_auc: 0.8198 - val_loss: 2.3202 - val_precision: 0.5768 - val_recall: 0.5274\nEpoch 5/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9803 - auc: 0.9990 - loss: 0.0642 - precision: 0.9830 - recall: 0.9790\nEpoch 5: val_loss did not improve from 0.97906\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9803 - auc: 0.9990 - loss: 0.0642 - precision: 0.9830 - recall: 0.9790 - val_accuracy: 0.6066 - val_auc: 0.9232 - val_loss: 1.3688 - val_precision: 0.6273 - val_recall: 0.5851\nEpoch 6/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9816 - auc: 0.9991 - loss: 0.0600 - precision: 0.9841 - recall: 0.9794\nEpoch 6: val_loss improved from 0.97906 to 0.43198, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9816 - auc: 0.9991 - loss: 0.0600 - precision: 0.9841 - recall: 0.9794 - val_accuracy: 0.8685 - val_auc: 0.9854 - val_loss: 0.4320 - val_precision: 0.8775 - val_recall: 0.8615\nEpoch 7/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9832 - auc: 0.9991 - loss: 0.0538 - precision: 0.9850 - recall: 0.9816\nEpoch 7: val_loss improved from 0.43198 to 0.23641, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9832 - auc: 0.9991 - loss: 0.0538 - precision: 0.9850 - recall: 0.9816 - val_accuracy: 0.9248 - val_auc: 0.9936 - val_loss: 0.2364 - val_precision: 0.9319 - val_recall: 0.9192\nEpoch 8/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9861 - auc: 0.9995 - loss: 0.0429 - precision: 0.9876 - recall: 0.9850\nEpoch 8: val_loss improved from 0.23641 to 0.15616, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9861 - auc: 0.9995 - loss: 0.0429 - precision: 0.9876 - recall: 0.9850 - val_accuracy: 0.9523 - val_auc: 0.9970 - val_loss: 0.1562 - val_precision: 0.9611 - val_recall: 0.9462\nEpoch 9/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9888 - auc: 0.9994 - loss: 0.0363 - precision: 0.9900 - recall: 0.9881\nEpoch 9: val_loss did not improve from 0.15616\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9888 - auc: 0.9994 - loss: 0.0363 - precision: 0.9900 - recall: 0.9881 - val_accuracy: 0.9082 - val_auc: 0.9918 - val_loss: 0.2964 - val_precision: 0.9182 - val_recall: 0.8981\nEpoch 10/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9888 - auc: 0.9994 - loss: 0.0375 - precision: 0.9900 - recall: 0.9881\nEpoch 10: val_loss did not improve from 0.15616\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9888 - auc: 0.9994 - loss: 0.0375 - precision: 0.9900 - recall: 0.9881 - val_accuracy: 0.8813 - val_auc: 0.9898 - val_loss: 0.3504 - val_precision: 0.8957 - val_recall: 0.8701\nEpoch 11/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9879 - auc: 0.9994 - loss: 0.0368 - precision: 0.9886 - recall: 0.9872\nEpoch 11: val_loss did not improve from 0.15616\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9879 - auc: 0.9994 - loss: 0.0368 - precision: 0.9886 - recall: 0.9872 - val_accuracy: 0.9348 - val_auc: 0.9938 - val_loss: 0.2157 - val_precision: 0.9392 - val_recall: 0.9323\nEpoch 12/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9905 - auc: 0.9996 - loss: 0.0294 - precision: 0.9915 - recall: 0.9900\nEpoch 12: val_loss improved from 0.15616 to 0.13747, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9905 - auc: 0.9996 - loss: 0.0294 - precision: 0.9915 - recall: 0.9900 - val_accuracy: 0.9628 - val_auc: 0.9962 - val_loss: 0.1375 - val_precision: 0.9658 - val_recall: 0.9601\nEpoch 13/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9906 - auc: 0.9996 - loss: 0.0289 - precision: 0.9916 - recall: 0.9898\nEpoch 13: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9906 - auc: 0.9996 - loss: 0.0290 - precision: 0.9916 - recall: 0.9898 - val_accuracy: 0.8815 - val_auc: 0.9859 - val_loss: 0.4155 - val_precision: 0.8973 - val_recall: 0.8706\nEpoch 14/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9917 - auc: 0.9997 - loss: 0.0250 - precision: 0.9924 - recall: 0.9909\nEpoch 14: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9917 - auc: 0.9997 - loss: 0.0250 - precision: 0.9924 - recall: 0.9909 - val_accuracy: 0.8394 - val_auc: 0.9739 - val_loss: 0.6085 - val_precision: 0.8547 - val_recall: 0.8263\nEpoch 15/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9929 - auc: 0.9997 - loss: 0.0240 - precision: 0.9934 - recall: 0.9924\nEpoch 15: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9929 - auc: 0.9997 - loss: 0.0240 - precision: 0.9934 - recall: 0.9924 - val_accuracy: 0.7203 - val_auc: 0.9480 - val_loss: 1.0658 - val_precision: 0.7281 - val_recall: 0.7138\nEpoch 16/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9928 - auc: 0.9996 - loss: 0.0233 - precision: 0.9934 - recall: 0.9921\nEpoch 16: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9928 - auc: 0.9996 - loss: 0.0233 - precision: 0.9934 - recall: 0.9921 - val_accuracy: 0.8843 - val_auc: 0.9834 - val_loss: 0.4283 - val_precision: 0.8937 - val_recall: 0.8782\nEpoch 17/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9937 - auc: 0.9998 - loss: 0.0195 - precision: 0.9943 - recall: 0.9934\nEpoch 17: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9937 - auc: 0.9998 - loss: 0.0195 - precision: 0.9943 - recall: 0.9934 - val_accuracy: 0.8671 - val_auc: 0.9807 - val_loss: 0.4951 - val_precision: 0.8732 - val_recall: 0.8631\nEpoch 18/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0143 - precision: 0.9959 - recall: 0.9951\nEpoch 18: val_loss did not improve from 0.13747\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9956 - auc: 0.9999 - loss: 0.0143 - precision: 0.9959 - recall: 0.9951 - val_accuracy: 0.9633 - val_auc: 0.9957 - val_loss: 0.1441 - val_precision: 0.9673 - val_recall: 0.9613\nEpoch 19/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9937 - auc: 0.9997 - loss: 0.0199 - precision: 0.9944 - recall: 0.9935\nEpoch 19: val_loss improved from 0.13747 to 0.07517, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9937 - auc: 0.9997 - loss: 0.0199 - precision: 0.9944 - recall: 0.9935 - val_accuracy: 0.9789 - val_auc: 0.9981 - val_loss: 0.0752 - val_precision: 0.9806 - val_recall: 0.9778\nEpoch 20/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9952 - auc: 0.9998 - loss: 0.0146 - precision: 0.9955 - recall: 0.9949\nEpoch 20: val_loss did not improve from 0.07517\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9952 - auc: 0.9998 - loss: 0.0146 - precision: 0.9955 - recall: 0.9949 - val_accuracy: 0.8427 - val_auc: 0.9751 - val_loss: 0.6202 - val_precision: 0.8585 - val_recall: 0.8325\nEpoch 21/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0128 - precision: 0.9956 - recall: 0.9952\nEpoch 21: val_loss improved from 0.07517 to 0.07053, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0128 - precision: 0.9956 - recall: 0.9952 - val_accuracy: 0.9834 - val_auc: 0.9977 - val_loss: 0.0705 - val_precision: 0.9839 - val_recall: 0.9825\nEpoch 22/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0141 - precision: 0.9957 - recall: 0.9950\nEpoch 22: val_loss improved from 0.07053 to 0.05050, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0141 - precision: 0.9957 - recall: 0.9950 - val_accuracy: 0.9863 - val_auc: 0.9985 - val_loss: 0.0505 - val_precision: 0.9872 - val_recall: 0.9860\nEpoch 23/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0102 - precision: 0.9970 - recall: 0.9967\nEpoch 23: val_loss did not improve from 0.05050\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0102 - precision: 0.9970 - recall: 0.9967 - val_accuracy: 0.9785 - val_auc: 0.9973 - val_loss: 0.0846 - val_precision: 0.9798 - val_recall: 0.9777\nEpoch 24/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0109 - precision: 0.9969 - recall: 0.9966\nEpoch 24: val_loss improved from 0.05050 to 0.04847, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0109 - precision: 0.9969 - recall: 0.9966 - val_accuracy: 0.9872 - val_auc: 0.9986 - val_loss: 0.0485 - val_precision: 0.9881 - val_recall: 0.9868\nEpoch 25/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9953 - auc: 0.9998 - loss: 0.0139 - precision: 0.9956 - recall: 0.9951\nEpoch 25: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9953 - auc: 0.9998 - loss: 0.0139 - precision: 0.9956 - recall: 0.9951 - val_accuracy: 0.8832 - val_auc: 0.9859 - val_loss: 0.4049 - val_precision: 0.8936 - val_recall: 0.8758\nEpoch 26/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0109 - precision: 0.9968 - recall: 0.9965\nEpoch 26: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0109 - precision: 0.9968 - recall: 0.9965 - val_accuracy: 0.9398 - val_auc: 0.9903 - val_loss: 0.2521 - val_precision: 0.9421 - val_recall: 0.9388\nEpoch 27/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9962 - auc: 0.9998 - loss: 0.0119 - precision: 0.9963 - recall: 0.9959\nEpoch 27: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9962 - auc: 0.9998 - loss: 0.0119 - precision: 0.9963 - recall: 0.9959 - val_accuracy: 0.9588 - val_auc: 0.9945 - val_loss: 0.1635 - val_precision: 0.9608 - val_recall: 0.9571\nEpoch 28/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0073 - precision: 0.9978 - recall: 0.9976\nEpoch 28: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0073 - precision: 0.9978 - recall: 0.9976 - val_accuracy: 0.9535 - val_auc: 0.9930 - val_loss: 0.1972 - val_precision: 0.9556 - val_recall: 0.9516\nEpoch 29/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9969 - auc: 0.9998 - loss: 0.0098 - precision: 0.9971 - recall: 0.9968\nEpoch 29: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 0.9998 - loss: 0.0098 - precision: 0.9971 - recall: 0.9968 - val_accuracy: 0.8221 - val_auc: 0.9782 - val_loss: 0.6106 - val_precision: 0.8250 - val_recall: 0.8203\nEpoch 30/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9967 - auc: 0.9998 - loss: 0.0100 - precision: 0.9969 - recall: 0.9965\nEpoch 30: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9998 - loss: 0.0100 - precision: 0.9969 - recall: 0.9965 - val_accuracy: 0.9763 - val_auc: 0.9968 - val_loss: 0.0968 - val_precision: 0.9776 - val_recall: 0.9758\nEpoch 31/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9968 - auc: 0.9998 - loss: 0.0105 - precision: 0.9970 - recall: 0.9967\nEpoch 31: val_loss did not improve from 0.04847\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9998 - loss: 0.0105 - precision: 0.9970 - recall: 0.9967 - val_accuracy: 0.9805 - val_auc: 0.9978 - val_loss: 0.0729 - val_precision: 0.9815 - val_recall: 0.9796\nEpoch 32/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9980 - auc: 0.9999 - loss: 0.0067 - precision: 0.9980 - recall: 0.9979\nEpoch 32: val_loss improved from 0.04847 to 0.04040, saving model to sigmoid.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9980 - auc: 0.9999 - loss: 0.0067 - precision: 0.9980 - recall: 0.9979 - val_accuracy: 0.9904 - val_auc: 0.9987 - val_loss: 0.0404 - val_precision: 0.9907 - val_recall: 0.9902\nEpoch 33/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9980 - auc: 0.9999 - loss: 0.0071 - precision: 0.9981 - recall: 0.9979\nEpoch 33: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9980 - auc: 0.9999 - loss: 0.0071 - precision: 0.9981 - recall: 0.9979 - val_accuracy: 0.9672 - val_auc: 0.9954 - val_loss: 0.1346 - val_precision: 0.9693 - val_recall: 0.9654\nEpoch 34/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0093 - precision: 0.9968 - recall: 0.9966\nEpoch 34: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0093 - precision: 0.9968 - recall: 0.9966 - val_accuracy: 0.9002 - val_auc: 0.9818 - val_loss: 0.4446 - val_precision: 0.9037 - val_recall: 0.8969\nEpoch 35/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0079 - precision: 0.9978 - recall: 0.9975\nEpoch 35: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0079 - precision: 0.9978 - recall: 0.9975 - val_accuracy: 0.9893 - val_auc: 0.9987 - val_loss: 0.0449 - val_precision: 0.9897 - val_recall: 0.9891\nEpoch 36/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0064 - precision: 0.9982 - recall: 0.9981\nEpoch 36: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0064 - precision: 0.9982 - recall: 0.9981 - val_accuracy: 0.9812 - val_auc: 0.9974 - val_loss: 0.0767 - val_precision: 0.9820 - val_recall: 0.9805\nEpoch 37/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0076 - precision: 0.9974 - recall: 0.9972\nEpoch 37: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0076 - precision: 0.9974 - recall: 0.9972 - val_accuracy: 0.9887 - val_auc: 0.9979 - val_loss: 0.0525 - val_precision: 0.9892 - val_recall: 0.9883\nEpoch 38/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0077 - precision: 0.9977 - recall: 0.9977\nEpoch 38: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0077 - precision: 0.9977 - recall: 0.9977 - val_accuracy: 0.9703 - val_auc: 0.9951 - val_loss: 0.1369 - val_precision: 0.9715 - val_recall: 0.9697\nEpoch 39/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0081 - precision: 0.9976 - recall: 0.9974\nEpoch 39: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0081 - precision: 0.9976 - recall: 0.9974 - val_accuracy: 0.9860 - val_auc: 0.9975 - val_loss: 0.0663 - val_precision: 0.9862 - val_recall: 0.9859\nEpoch 40/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0059 - precision: 0.9981 - recall: 0.9979\nEpoch 40: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0059 - precision: 0.9981 - recall: 0.9979 - val_accuracy: 0.9615 - val_auc: 0.9940 - val_loss: 0.1754 - val_precision: 0.9626 - val_recall: 0.9612\nEpoch 41/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0069 - precision: 0.9977 - recall: 0.9976\nEpoch 41: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0069 - precision: 0.9977 - recall: 0.9976 - val_accuracy: 0.5595 - val_auc: 0.8443 - val_loss: 3.2248 - val_precision: 0.5652 - val_recall: 0.5539\nEpoch 42/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0053 - precision: 0.9985 - recall: 0.9983\nEpoch 42: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0053 - precision: 0.9985 - recall: 0.9983 - val_accuracy: 0.9737 - val_auc: 0.9954 - val_loss: 0.1207 - val_precision: 0.9756 - val_recall: 0.9728\nEpoch 43/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0042 - precision: 0.9987 - recall: 0.9987\nEpoch 43: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0042 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9884 - val_auc: 0.9983 - val_loss: 0.0518 - val_precision: 0.9890 - val_recall: 0.9880\nEpoch 44/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0068 - precision: 0.9979 - recall: 0.9977\nEpoch 44: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0068 - precision: 0.9979 - recall: 0.9977 - val_accuracy: 0.9862 - val_auc: 0.9971 - val_loss: 0.0758 - val_precision: 0.9866 - val_recall: 0.9859\nEpoch 45/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0051 - precision: 0.9982 - recall: 0.9981\nEpoch 45: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0051 - precision: 0.9982 - recall: 0.9981 - val_accuracy: 0.9887 - val_auc: 0.9980 - val_loss: 0.0529 - val_precision: 0.9890 - val_recall: 0.9886\nEpoch 46/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0047 - precision: 0.9985 - recall: 0.9985\nEpoch 46: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0047 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9684 - val_auc: 0.9964 - val_loss: 0.1172 - val_precision: 0.9691 - val_recall: 0.9680\nEpoch 47/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.0037 - precision: 0.9988 - recall: 0.9987\nEpoch 47: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9988 - auc: 1.0000 - loss: 0.0037 - precision: 0.9988 - recall: 0.9987 - val_accuracy: 0.9873 - val_auc: 0.9981 - val_loss: 0.0600 - val_precision: 0.9879 - val_recall: 0.9868\nEpoch 48/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - auc: 0.9999 - loss: 0.0040 - precision: 0.9989 - recall: 0.9988\nEpoch 48: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 0.9999 - loss: 0.0040 - precision: 0.9989 - recall: 0.9988 - val_accuracy: 0.9877 - val_auc: 0.9977 - val_loss: 0.0625 - val_precision: 0.9882 - val_recall: 0.9877\nEpoch 49/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0046 - precision: 0.9984 - recall: 0.9984\nEpoch 49: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0046 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.9605 - val_auc: 0.9933 - val_loss: 0.1820 - val_precision: 0.9614 - val_recall: 0.9602\nEpoch 50/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0051 - precision: 0.9987 - recall: 0.9987\nEpoch 50: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0051 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.8028 - val_auc: 0.9445 - val_loss: 1.2322 - val_precision: 0.8044 - val_recall: 0.8015\nEpoch 51/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: 0.0083 - precision: 0.9974 - recall: 0.9973\nEpoch 51: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: 0.0083 - precision: 0.9974 - recall: 0.9973 - val_accuracy: 0.9884 - val_auc: 0.9978 - val_loss: 0.0620 - val_precision: 0.9887 - val_recall: 0.9880\nEpoch 52/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9990\nEpoch 52: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0028 - precision: 0.9990 - recall: 0.9990 - val_accuracy: 0.9778 - val_auc: 0.9965 - val_loss: 0.1006 - val_precision: 0.9785 - val_recall: 0.9774\nEpoch 53/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0047 - precision: 0.9985 - recall: 0.9982\nEpoch 53: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0047 - precision: 0.9985 - recall: 0.9982 - val_accuracy: 0.9821 - val_auc: 0.9968 - val_loss: 0.0900 - val_precision: 0.9827 - val_recall: 0.9818\nEpoch 54/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0030 - precision: 0.9991 - recall: 0.9990\nEpoch 54: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0030 - precision: 0.9991 - recall: 0.9990 - val_accuracy: 0.9887 - val_auc: 0.9977 - val_loss: 0.0581 - val_precision: 0.9893 - val_recall: 0.9885\nEpoch 55/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0039 - precision: 0.9987 - recall: 0.9985\nEpoch 55: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0039 - precision: 0.9987 - recall: 0.9985 - val_accuracy: 0.8261 - val_auc: 0.9578 - val_loss: 0.9670 - val_precision: 0.8314 - val_recall: 0.8224\nEpoch 56/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0054 - precision: 0.9983 - recall: 0.9983\nEpoch 56: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0054 - precision: 0.9983 - recall: 0.9983 - val_accuracy: 0.8138 - val_auc: 0.9592 - val_loss: 0.9198 - val_precision: 0.8195 - val_recall: 0.8095\nEpoch 57/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0103 - precision: 0.9965 - recall: 0.9964\nEpoch 57: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0103 - precision: 0.9965 - recall: 0.9964 - val_accuracy: 0.9721 - val_auc: 0.9951 - val_loss: 0.1303 - val_precision: 0.9730 - val_recall: 0.9712\nEpoch 58/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0021 - precision: 0.9993 - recall: 0.9993\nEpoch 58: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0021 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9839 - val_auc: 0.9967 - val_loss: 0.0793 - val_precision: 0.9840 - val_recall: 0.9836\nEpoch 59/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0044 - precision: 0.9985 - recall: 0.9985\nEpoch 59: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0044 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9905 - val_auc: 0.9979 - val_loss: 0.0488 - val_precision: 0.9906 - val_recall: 0.9903\nEpoch 60/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0033 - precision: 0.9989 - recall: 0.9989\nEpoch 60: val_loss did not improve from 0.04040\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0033 - precision: 0.9989 - recall: 0.9989 - val_accuracy: 0.9899 - val_auc: 0.9981 - val_loss: 0.0505 - val_precision: 0.9902 - val_recall: 0.9898\n\n\n Training with Tanh activation function\nEpoch 1/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8429 - auc: 0.9790 - loss: 0.4776 - precision: 0.9253 - recall: 0.7784\nEpoch 1: val_loss improved from inf to 0.11385, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13ms/step - accuracy: 0.8430 - auc: 0.9790 - loss: 0.4773 - precision: 0.9253 - recall: 0.7785 - val_accuracy: 0.9654 - val_auc: 0.9982 - val_loss: 0.1139 - val_precision: 0.9704 - val_recall: 0.9609\nEpoch 2/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9739 - auc: 0.9988 - loss: 0.0859 - precision: 0.9775 - recall: 0.9709\nEpoch 2: val_loss improved from 0.11385 to 0.07874, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9739 - auc: 0.9988 - loss: 0.0859 - precision: 0.9775 - recall: 0.9709 - val_accuracy: 0.9766 - val_auc: 0.9989 - val_loss: 0.0787 - val_precision: 0.9801 - val_recall: 0.9747\nEpoch 3/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9809 - auc: 0.9992 - loss: 0.0617 - precision: 0.9831 - recall: 0.9792\nEpoch 3: val_loss did not improve from 0.07874\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9809 - auc: 0.9992 - loss: 0.0617 - precision: 0.9831 - recall: 0.9792 - val_accuracy: 0.9591 - val_auc: 0.9966 - val_loss: 0.1343 - val_precision: 0.9623 - val_recall: 0.9578\nEpoch 4/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9832 - auc: 0.9993 - loss: 0.0537 - precision: 0.9850 - recall: 0.9815\nEpoch 4: val_loss did not improve from 0.07874\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9832 - auc: 0.9993 - loss: 0.0537 - precision: 0.9850 - recall: 0.9815 - val_accuracy: 0.9657 - val_auc: 0.9981 - val_loss: 0.1069 - val_precision: 0.9706 - val_recall: 0.9628\nEpoch 5/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9859 - auc: 0.9995 - loss: 0.0442 - precision: 0.9873 - recall: 0.9845\nEpoch 5: val_loss improved from 0.07874 to 0.06676, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9859 - auc: 0.9995 - loss: 0.0442 - precision: 0.9873 - recall: 0.9845 - val_accuracy: 0.9803 - val_auc: 0.9985 - val_loss: 0.0668 - val_precision: 0.9824 - val_recall: 0.9794\nEpoch 6/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9892 - auc: 0.9994 - loss: 0.0382 - precision: 0.9901 - recall: 0.9883\nEpoch 6: val_loss improved from 0.06676 to 0.04130, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9892 - auc: 0.9994 - loss: 0.0382 - precision: 0.9901 - recall: 0.9883 - val_accuracy: 0.9879 - val_auc: 0.9993 - val_loss: 0.0413 - val_precision: 0.9886 - val_recall: 0.9869\nEpoch 7/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9906 - auc: 0.9995 - loss: 0.0315 - precision: 0.9917 - recall: 0.9896\nEpoch 7: val_loss did not improve from 0.04130\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9906 - auc: 0.9995 - loss: 0.0315 - precision: 0.9917 - recall: 0.9896 - val_accuracy: 0.9872 - val_auc: 0.9991 - val_loss: 0.0465 - val_precision: 0.9886 - val_recall: 0.9863\nEpoch 8/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9912 - auc: 0.9997 - loss: 0.0278 - precision: 0.9918 - recall: 0.9904\nEpoch 8: val_loss did not improve from 0.04130\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9911 - auc: 0.9997 - loss: 0.0278 - precision: 0.9918 - recall: 0.9904 - val_accuracy: 0.9703 - val_auc: 0.9977 - val_loss: 0.0962 - val_precision: 0.9725 - val_recall: 0.9684\nEpoch 9/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9914 - auc: 0.9998 - loss: 0.0259 - precision: 0.9922 - recall: 0.9907\nEpoch 9: val_loss did not improve from 0.04130\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9914 - auc: 0.9998 - loss: 0.0259 - precision: 0.9922 - recall: 0.9907 - val_accuracy: 0.9868 - val_auc: 0.9987 - val_loss: 0.0488 - val_precision: 0.9878 - val_recall: 0.9864\nEpoch 10/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9931 - auc: 0.9998 - loss: 0.0220 - precision: 0.9937 - recall: 0.9926\nEpoch 10: val_loss improved from 0.04130 to 0.03627, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9931 - auc: 0.9998 - loss: 0.0220 - precision: 0.9937 - recall: 0.9926 - val_accuracy: 0.9886 - val_auc: 0.9991 - val_loss: 0.0363 - val_precision: 0.9901 - val_recall: 0.9883\nEpoch 11/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0178 - precision: 0.9951 - recall: 0.9944\nEpoch 11: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0178 - precision: 0.9951 - recall: 0.9944 - val_accuracy: 0.9882 - val_auc: 0.9986 - val_loss: 0.0441 - val_precision: 0.9890 - val_recall: 0.9878\nEpoch 12/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0159 - precision: 0.9953 - recall: 0.9944\nEpoch 12: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9946 - auc: 0.9999 - loss: 0.0159 - precision: 0.9953 - recall: 0.9944 - val_accuracy: 0.9863 - val_auc: 0.9987 - val_loss: 0.0506 - val_precision: 0.9872 - val_recall: 0.9859\nEpoch 13/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0152 - precision: 0.9954 - recall: 0.9946\nEpoch 13: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9950 - auc: 0.9998 - loss: 0.0152 - precision: 0.9954 - recall: 0.9946 - val_accuracy: 0.9872 - val_auc: 0.9988 - val_loss: 0.0463 - val_precision: 0.9881 - val_recall: 0.9867\nEpoch 14/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0166 - precision: 0.9952 - recall: 0.9947\nEpoch 14: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0166 - precision: 0.9952 - recall: 0.9947 - val_accuracy: 0.9868 - val_auc: 0.9987 - val_loss: 0.0485 - val_precision: 0.9872 - val_recall: 0.9862\nEpoch 15/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0130 - precision: 0.9957 - recall: 0.9953\nEpoch 15: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9999 - loss: 0.0130 - precision: 0.9957 - recall: 0.9953 - val_accuracy: 0.9898 - val_auc: 0.9990 - val_loss: 0.0370 - val_precision: 0.9907 - val_recall: 0.9893\nEpoch 16/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0106 - precision: 0.9971 - recall: 0.9967\nEpoch 16: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9969 - auc: 0.9999 - loss: 0.0106 - precision: 0.9971 - recall: 0.9967 - val_accuracy: 0.9845 - val_auc: 0.9987 - val_loss: 0.0507 - val_precision: 0.9859 - val_recall: 0.9843\nEpoch 17/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0114 - precision: 0.9966 - recall: 0.9964\nEpoch 17: val_loss did not improve from 0.03627\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0114 - precision: 0.9966 - recall: 0.9964 - val_accuracy: 0.9898 - val_auc: 0.9990 - val_loss: 0.0374 - val_precision: 0.9902 - val_recall: 0.9893\nEpoch 18/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0094 - precision: 0.9967 - recall: 0.9964\nEpoch 18: val_loss improved from 0.03627 to 0.03467, saving model to tanh.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0095 - precision: 0.9967 - recall: 0.9964 - val_accuracy: 0.9908 - val_auc: 0.9990 - val_loss: 0.0347 - val_precision: 0.9913 - val_recall: 0.9906\nEpoch 19/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0095 - precision: 0.9967 - recall: 0.9963\nEpoch 19: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0095 - precision: 0.9967 - recall: 0.9963 - val_accuracy: 0.9913 - val_auc: 0.9988 - val_loss: 0.0381 - val_precision: 0.9914 - val_recall: 0.9910\nEpoch 20/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0087 - precision: 0.9974 - recall: 0.9971\nEpoch 20: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 0.9999 - loss: 0.0087 - precision: 0.9974 - recall: 0.9971 - val_accuracy: 0.9889 - val_auc: 0.9985 - val_loss: 0.0483 - val_precision: 0.9892 - val_recall: 0.9884\nEpoch 21/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.0065 - precision: 0.9980 - recall: 0.9978\nEpoch 21: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9980 - auc: 1.0000 - loss: 0.0065 - precision: 0.9980 - recall: 0.9978 - val_accuracy: 0.9907 - val_auc: 0.9982 - val_loss: 0.0422 - val_precision: 0.9910 - val_recall: 0.9905\nEpoch 22/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0073 - precision: 0.9977 - recall: 0.9976\nEpoch 22: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0074 - precision: 0.9977 - recall: 0.9976 - val_accuracy: 0.9896 - val_auc: 0.9987 - val_loss: 0.0439 - val_precision: 0.9899 - val_recall: 0.9896\nEpoch 23/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0059 - precision: 0.9983 - recall: 0.9980\nEpoch 23: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0059 - precision: 0.9983 - recall: 0.9980 - val_accuracy: 0.9843 - val_auc: 0.9981 - val_loss: 0.0629 - val_precision: 0.9844 - val_recall: 0.9842\nEpoch 24/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0069 - precision: 0.9977 - recall: 0.9975\nEpoch 24: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0069 - precision: 0.9977 - recall: 0.9975 - val_accuracy: 0.9902 - val_auc: 0.9988 - val_loss: 0.0371 - val_precision: 0.9906 - val_recall: 0.9899\nEpoch 25/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9972 - auc: 0.9999 - loss: 0.0082 - precision: 0.9973 - recall: 0.9971\nEpoch 25: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9972 - auc: 0.9999 - loss: 0.0082 - precision: 0.9973 - recall: 0.9971 - val_accuracy: 0.9899 - val_auc: 0.9985 - val_loss: 0.0455 - val_precision: 0.9900 - val_recall: 0.9898\nEpoch 26/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0057 - precision: 0.9981 - recall: 0.9981\nEpoch 26: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0057 - precision: 0.9981 - recall: 0.9981 - val_accuracy: 0.9916 - val_auc: 0.9988 - val_loss: 0.0367 - val_precision: 0.9918 - val_recall: 0.9915\nEpoch 27/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0048 - precision: 0.9985 - recall: 0.9984\nEpoch 27: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0048 - precision: 0.9985 - recall: 0.9984 - val_accuracy: 0.9895 - val_auc: 0.9986 - val_loss: 0.0451 - val_precision: 0.9900 - val_recall: 0.9892\nEpoch 28/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9971 - auc: 1.0000 - loss: 0.0074 - precision: 0.9974 - recall: 0.9970\nEpoch 28: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9971 - auc: 1.0000 - loss: 0.0074 - precision: 0.9974 - recall: 0.9970 - val_accuracy: 0.9895 - val_auc: 0.9984 - val_loss: 0.0448 - val_precision: 0.9900 - val_recall: 0.9894\nEpoch 29/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0047 - precision: 0.9986 - recall: 0.9985\nEpoch 29: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0047 - precision: 0.9986 - recall: 0.9985 - val_accuracy: 0.9913 - val_auc: 0.9989 - val_loss: 0.0351 - val_precision: 0.9917 - val_recall: 0.9910\nEpoch 30/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0040 - precision: 0.9987 - recall: 0.9986\nEpoch 30: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0040 - precision: 0.9987 - recall: 0.9986 - val_accuracy: 0.9918 - val_auc: 0.9985 - val_loss: 0.0388 - val_precision: 0.9922 - val_recall: 0.9916\nEpoch 31/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0054 - precision: 0.9982 - recall: 0.9981\nEpoch 31: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0054 - precision: 0.9982 - recall: 0.9981 - val_accuracy: 0.9916 - val_auc: 0.9987 - val_loss: 0.0380 - val_precision: 0.9919 - val_recall: 0.9915\nEpoch 32/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0038 - precision: 0.9984 - recall: 0.9982\nEpoch 32: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0038 - precision: 0.9984 - recall: 0.9982 - val_accuracy: 0.9893 - val_auc: 0.9984 - val_loss: 0.0483 - val_precision: 0.9898 - val_recall: 0.9891\nEpoch 33/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0044 - precision: 0.9985 - recall: 0.9985\nEpoch 33: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0044 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9893 - val_auc: 0.9984 - val_loss: 0.0478 - val_precision: 0.9897 - val_recall: 0.9892\nEpoch 34/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0040 - precision: 0.9987 - recall: 0.9987\nEpoch 34: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0040 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9875 - val_auc: 0.9981 - val_loss: 0.0537 - val_precision: 0.9877 - val_recall: 0.9873\nEpoch 35/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0069 - precision: 0.9975 - recall: 0.9973\nEpoch 35: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0069 - precision: 0.9975 - recall: 0.9973 - val_accuracy: 0.9902 - val_auc: 0.9988 - val_loss: 0.0393 - val_precision: 0.9902 - val_recall: 0.9902\nEpoch 36/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0045 - precision: 0.9987 - recall: 0.9986\nEpoch 36: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0045 - precision: 0.9987 - recall: 0.9986 - val_accuracy: 0.9902 - val_auc: 0.9984 - val_loss: 0.0436 - val_precision: 0.9903 - val_recall: 0.9901\nEpoch 37/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0035 - precision: 0.9987 - recall: 0.9986\nEpoch 37: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 1.0000 - loss: 0.0035 - precision: 0.9987 - recall: 0.9986 - val_accuracy: 0.9904 - val_auc: 0.9981 - val_loss: 0.0503 - val_precision: 0.9907 - val_recall: 0.9902\nEpoch 38/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0056 - precision: 0.9984 - recall: 0.9983\nEpoch 38: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0056 - precision: 0.9984 - recall: 0.9983 - val_accuracy: 0.9912 - val_auc: 0.9986 - val_loss: 0.0392 - val_precision: 0.9913 - val_recall: 0.9912\nEpoch 39/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0050 - precision: 0.9984 - recall: 0.9984\nEpoch 39: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0050 - precision: 0.9984 - recall: 0.9984 - val_accuracy: 0.9908 - val_auc: 0.9983 - val_loss: 0.0444 - val_precision: 0.9912 - val_recall: 0.9907\nEpoch 40/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0015 - precision: 0.9995 - recall: 0.9995\nEpoch 40: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0015 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9915 - val_auc: 0.9986 - val_loss: 0.0424 - val_precision: 0.9917 - val_recall: 0.9914\nEpoch 41/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0048 - precision: 0.9984 - recall: 0.9983\nEpoch 41: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0048 - precision: 0.9984 - recall: 0.9983 - val_accuracy: 0.9895 - val_auc: 0.9983 - val_loss: 0.0491 - val_precision: 0.9900 - val_recall: 0.9894\nEpoch 42/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0039 - precision: 0.9987 - recall: 0.9987\nEpoch 42: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0039 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9923 - val_auc: 0.9987 - val_loss: 0.0353 - val_precision: 0.9927 - val_recall: 0.9920\nEpoch 43/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0015 - precision: 0.9997 - recall: 0.9997\nEpoch 43: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0015 - precision: 0.9997 - recall: 0.9997 - val_accuracy: 0.9918 - val_auc: 0.9985 - val_loss: 0.0398 - val_precision: 0.9919 - val_recall: 0.9918\nEpoch 44/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0036 - precision: 0.9990 - recall: 0.9990\nEpoch 44: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0036 - precision: 0.9990 - recall: 0.9990 - val_accuracy: 0.9898 - val_auc: 0.9983 - val_loss: 0.0476 - val_precision: 0.9899 - val_recall: 0.9896\nEpoch 45/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0058 - precision: 0.9983 - recall: 0.9981\nEpoch 45: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0058 - precision: 0.9983 - recall: 0.9981 - val_accuracy: 0.9882 - val_auc: 0.9981 - val_loss: 0.0498 - val_precision: 0.9884 - val_recall: 0.9880\nEpoch 46/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0027 - precision: 0.9991 - recall: 0.9990\nEpoch 46: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0027 - precision: 0.9991 - recall: 0.9990 - val_accuracy: 0.9912 - val_auc: 0.9983 - val_loss: 0.0450 - val_precision: 0.9913 - val_recall: 0.9910\nEpoch 47/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0034 - precision: 0.9989 - recall: 0.9989\nEpoch 47: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0034 - precision: 0.9989 - recall: 0.9989 - val_accuracy: 0.9910 - val_auc: 0.9984 - val_loss: 0.0476 - val_precision: 0.9912 - val_recall: 0.9909\nEpoch 48/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9995 - recall: 0.9994\nEpoch 48: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9995 - recall: 0.9994 - val_accuracy: 0.9901 - val_auc: 0.9983 - val_loss: 0.0471 - val_precision: 0.9905 - val_recall: 0.9898\nEpoch 49/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0042 - precision: 0.9988 - recall: 0.9987\nEpoch 49: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0042 - precision: 0.9988 - recall: 0.9987 - val_accuracy: 0.9919 - val_auc: 0.9985 - val_loss: 0.0388 - val_precision: 0.9920 - val_recall: 0.9918\nEpoch 50/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0022 - precision: 0.9992 - recall: 0.9992\nEpoch 50: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0022 - precision: 0.9992 - recall: 0.9992 - val_accuracy: 0.9907 - val_auc: 0.9985 - val_loss: 0.0437 - val_precision: 0.9910 - val_recall: 0.9905\nEpoch 51/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0042 - precision: 0.9985 - recall: 0.9984\nEpoch 51: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0042 - precision: 0.9985 - recall: 0.9984 - val_accuracy: 0.9924 - val_auc: 0.9988 - val_loss: 0.0374 - val_precision: 0.9926 - val_recall: 0.9921\nEpoch 52/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0016 - precision: 0.9995 - recall: 0.9994\nEpoch 52: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0016 - precision: 0.9995 - recall: 0.9994 - val_accuracy: 0.9905 - val_auc: 0.9984 - val_loss: 0.0436 - val_precision: 0.9907 - val_recall: 0.9903\nEpoch 53/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0031 - precision: 0.9989 - recall: 0.9989\nEpoch 53: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0031 - precision: 0.9989 - recall: 0.9989 - val_accuracy: 0.9912 - val_auc: 0.9988 - val_loss: 0.0372 - val_precision: 0.9913 - val_recall: 0.9911\nEpoch 54/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9994 - recall: 0.9993\nEpoch 54: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9994 - recall: 0.9993 - val_accuracy: 0.9903 - val_auc: 0.9985 - val_loss: 0.0435 - val_precision: 0.9906 - val_recall: 0.9901\nEpoch 55/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991\nEpoch 55: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9919 - val_auc: 0.9987 - val_loss: 0.0375 - val_precision: 0.9919 - val_recall: 0.9918\nEpoch 56/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0027 - precision: 0.9991 - recall: 0.9991\nEpoch 56: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0027 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9917 - val_auc: 0.9985 - val_loss: 0.0404 - val_precision: 0.9920 - val_recall: 0.9915\nEpoch 57/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991\nEpoch 57: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9991 - auc: 1.0000 - loss: 0.0028 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9880 - val_auc: 0.9981 - val_loss: 0.0545 - val_precision: 0.9885 - val_recall: 0.9878\nEpoch 58/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0033 - precision: 0.9987 - recall: 0.9987\nEpoch 58: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0033 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9908 - val_auc: 0.9985 - val_loss: 0.0435 - val_precision: 0.9911 - val_recall: 0.9906\nEpoch 59/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0043 - precision: 0.9987 - recall: 0.9987\nEpoch 59: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0043 - precision: 0.9987 - recall: 0.9987 - val_accuracy: 0.9915 - val_auc: 0.9986 - val_loss: 0.0385 - val_precision: 0.9917 - val_recall: 0.9915\nEpoch 60/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0022 - precision: 0.9992 - recall: 0.9992\nEpoch 60: val_loss did not improve from 0.03467\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0022 - precision: 0.9992 - recall: 0.9992 - val_accuracy: 0.9894 - val_auc: 0.9978 - val_loss: 0.0552 - val_precision: 0.9897 - val_recall: 0.9893\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_info(history, model_name):\n    history = history.history\n    \n    print(\"*\" * 50)\n    print(f\"\\n{model_name} Results:\")\n    print(\"*\" * 50)\n    print(\"\\n\")\n\n    # Assuming history['loss'], history['val_loss'], etc., exist\n    training_loss = history['loss']\n    validation_loss = history['val_loss']\n    training_accuracy = history['accuracy']\n    validation_accuracy = history['val_accuracy']\n    training_auc = history['auc']\n    validation_auc = history['val_auc']\n    training_precision = history['precision']\n    validation_precision = history['val_precision']\n    training_recall = history['recall']\n    validation_recall = history['val_recall']\n    \n\n\n    # Calculate F1 scores based on available precision and recall in history\n    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n\n\n    top_3_dict[model_name] = {\n        \"training_loss\": sorted(training_loss)[:3],\n        \"validation_loss\": sorted(validation_loss)[:3],\n        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n\n    }\n\n    # Print Top 3 Lowest Losses\n    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n\n    # Print Top 3 Highest Accuracies\n    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n\n    # Print Top 3 AUCs\n    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n\n    # Print Top 3 F1 Scores\n    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n\n    # Print Top 3 Precision\n    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n\n    # Print Top 3 Recall\n    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:05:20.640292Z","iopub.execute_input":"2024-03-16T12:05:20.640932Z","iopub.status.idle":"2024-03-16T12:05:20.656874Z","shell.execute_reply.started":"2024-03-16T12:05:20.640902Z","shell.execute_reply":"2024-03-16T12:05:20.655757Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print_info(history_relu, \"RELU\")\nprint_info(history_sigmoid, \"SIGMOID\")\nprint_info(history_tanh, \"TANH\")","metadata":{"execution":{"iopub.status.busy":"2024-03-16T12:06:21.227474Z","iopub.execute_input":"2024-03-16T12:06:21.228116Z","iopub.status.idle":"2024-03-16T12:06:21.235066Z","shell.execute_reply.started":"2024-03-16T12:06:21.228080Z","shell.execute_reply":"2024-03-16T12:06:21.234098Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"**************************************************\n\nRELU Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.00034027054789476097, 0.00044127850560471416, 0.00095847446937114]\nTop 3 Lowest Validation Losses: [0.02439233846962452, 0.02517787739634514, 0.026302499696612358]\nTop 3 Highest Training Accuracies: [0.999916672706604, 0.9998958110809326, 0.9997291564941406]\nTop 3 Highest Validation Accuracies: [0.9947500228881836, 0.9944999814033508, 0.9944166541099548]\nTop 3 Training AUCs: [1.0, 1.0, 0.9999881982803345]\nTop 3 Validation AUCs: [0.9994715452194214, 0.9993517398834229, 0.9993330836296082]\nTop 3 Training F1 Scores: [0.999916672706604, 0.9998958110809326, 0.9997395871981462]\nTop 3 Validation F1 Scores: [0.994791446391268, 0.9944990894975356, 0.9944157324094369]\nTop 3 Training Precision: [0.999916672706604, 0.9998958110809326, 0.999750018119812]\nTop 3 Validation Precision: [0.9948328733444214, 0.9946649074554443, 0.9945815205574036]\nTop 3 Training Recall: [0.999916672706604, 0.9998958110809326, 0.9997291564941406]\nTop 3 Validation Recall: [0.9947500228881836, 0.9943333268165588, 0.9942499995231628]\n**************************************************\n\nSIGMOID Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.002277278807014227, 0.0036964374594390392, 0.004195601679384708]\nTop 3 Lowest Validation Losses: [0.04040161147713661, 0.04493817314505577, 0.04846738278865814]\nTop 3 Highest Training Accuracies: [0.9992291927337646, 0.9988750219345093, 0.9986875057220459]\nTop 3 Highest Validation Accuracies: [0.9904999732971191, 0.9904166460037231, 0.9899166822433472]\nTop 3 Training AUCs: [0.9999881386756897, 0.9999760389328003, 0.9999637603759766]\nTop 3 Validation AUCs: [0.99868243932724, 0.9986608624458313, 0.99864262342453]\nTop 3 Training F1 Scores: [0.9992395936360146, 0.99885412877115, 0.9986979066242372]\nTop 3 Validation F1 Scores: [0.9904571318785781, 0.9904142594948195, 0.9900396036411236]\nTop 3 Training Precision: [0.9992499947547913, 0.9988957643508911, 0.9987083077430725]\nTop 3 Validation Precision: [0.9906619787216187, 0.9905809760093689, 0.9902459383010864]\nTop 3 Training Recall: [0.9992291927337646, 0.9988124966621399, 0.9986875057220459]\nTop 3 Validation Recall: [0.9903333187103271, 0.9901666641235352, 0.9898333549499512]\n**************************************************\n\nTANH Results:\n**************************************************\n\n\nTop 3 Lowest Training Losses: [0.0018011170905083418, 0.002466262551024556, 0.0026669439394026995]\nTop 3 Lowest Validation Losses: [0.034665096551179886, 0.03509828448295593, 0.035258594900369644]\nTop 3 Highest Training Accuracies: [0.9994166493415833, 0.9992708563804626, 0.9991666674613953]\nTop 3 Highest Validation Accuracies: [0.9924166798591614, 0.9923333525657654, 0.9919166564941406]\nTop 3 Training AUCs: [0.9999985694885254, 0.9999880790710449, 0.9999879598617554]\nTop 3 Validation AUCs: [0.9993453621864319, 0.9991379976272583, 0.9990559220314026]\nTop 3 Training F1 Scores: [0.9994270791783698, 0.9992812572827171, 0.9991666674613953]\nTop 3 Validation F1 Scores: [0.9923314129581963, 0.9923307686351893, 0.9919139832096313]\nTop 3 Training Precision: [0.9994583129882812, 0.9992916584014893, 0.9991666674613953]\nTop 3 Validation Precision: [0.9926617741584778, 0.9925796389579773, 0.9922448396682739]\nTop 3 Training Recall: [0.9993958473205566, 0.9992708563804626, 0.9991666674613953]\nTop 3 Validation Recall: [0.9920833110809326, 0.9919999837875366, 0.9918333292007446]\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\n\nclass SmoothTransitionReLU(tf.keras.layers.Layer):\n    def __init__(self, initial_slope, final_slope, steepness=10, **kwargs):\n        super(SmoothTransitionReLU, self).__init__(**kwargs)\n        self.initial_slope = initial_slope\n        self.final_slope = final_slope\n        self.steepness = steepness\n        # Internal counter to track the relative progress of training\n        self.progress = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n\n    def call(self, inputs, training=None):\n        if training:\n            # Increment the progress during training (you might need to adjust how this increments based on your training regime)\n            self.progress.assign_add(0.01)  # Increment by a small value on each call\n\n        # Calculate the current slope based on the sigmoid function\n        x = self.progress\n        current_slope = self.initial_slope + (self.final_slope - self.initial_slope) / (1 + tf.exp(-self.steepness * (x - 0.5)))\n\n        # Apply the dynamic slope to the positive part of the inputs\n        positive_part = tf.maximum(0.0, inputs) * current_slope\n        # For negative inputs, just pass them through or adjust as needed\n        negative_part = tf.minimum(0.0, inputs)\n\n        return positive_part + negative_part\n\n    def get_config(self):\n        config = super(SmoothTransitionReLU, self).get_config()\n        config.update({\n            \"initial_slope\": self.initial_slope,\n            \"final_slope\": self.final_slope,\n            \"steepness\": self.steepness\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:58:55.014101Z","iopub.execute_input":"2024-03-16T10:58:55.014925Z","iopub.status.idle":"2024-03-16T10:58:55.024431Z","shell.execute_reply.started":"2024-03-16T10:58:55.014895Z","shell.execute_reply":"2024-03-16T10:58:55.023314Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_model_with_custom_activation(x_train, y_train, x_val, y_val, batch_size, learning_rate,\n                                       initial_slope, target_slope, total_epochs):\n    # Initialize the custom activation function with provided parameters\n    custom_activation = SmoothTransitionReLU(initial_slope=initial_slope, final_slope=target_slope)\n\n    # Build the model using the custom activation function\n    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1],\n                        activation_func=custom_activation)\n\n    # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        'dynamic_relu_model.keras',\n        save_best_only=True,\n        monitor='val_loss',\n        mode='min',\n        verbose=1\n    )\n\n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n                  metrics=['accuracy', 'precision', 'recall', 'auc'])\n\n    # Train the model\n    history = model.fit(x_train, y_train, epochs=total_epochs, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1,\n                        callbacks=[checkpoint_cb])\n\n    return history, model\n\n\n# Example parameters\nbatch_size = 32\nlearning_rate = 0.005\ninitial_slope = 1.732\ntarget_slope = 0.557\nrate = 0.01\ntotal_epochs = 60\n\n# Train the model\nhistory_custom, model_custom = train_model_with_custom_activation(\n    x_train_split, y_train_split, x_val_split, y_val_split, batch_size, learning_rate,\n    initial_slope, target_slope, total_epochs\n)\n\nhistories = {}\nhistories[\"custom\"] = history_custom","metadata":{"execution":{"iopub.status.busy":"2024-03-16T10:59:00.282810Z","iopub.execute_input":"2024-03-16T10:59:00.283806Z","iopub.status.idle":"2024-03-16T11:14:21.369145Z","shell.execute_reply.started":"2024-03-16T10:59:00.283768Z","shell.execute_reply":"2024-03-16T11:14:21.368210Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m   8/1500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.1405 - auc: 0.5384 - loss: 4.2130 - precision: 0.1382 - recall: 0.0481           ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1710586766.627562     108 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8515 - auc: 0.9762 - loss: 0.4917 - precision: 0.9063 - recall: 0.8037\nEpoch 1: val_loss improved from inf to 0.22246, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 12ms/step - accuracy: 0.8517 - auc: 0.9762 - loss: 0.4912 - precision: 0.9064 - recall: 0.8039 - val_accuracy: 0.9388 - val_auc: 0.9930 - val_loss: 0.2225 - val_precision: 0.9428 - val_recall: 0.9357\nEpoch 2/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9698 - auc: 0.9985 - loss: 0.0984 - precision: 0.9739 - recall: 0.9658\nEpoch 2: val_loss improved from 0.22246 to 0.12127, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9698 - auc: 0.9985 - loss: 0.0984 - precision: 0.9739 - recall: 0.9659 - val_accuracy: 0.9667 - val_auc: 0.9967 - val_loss: 0.1213 - val_precision: 0.9686 - val_recall: 0.9651\nEpoch 3/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9777 - auc: 0.9990 - loss: 0.0712 - precision: 0.9803 - recall: 0.9757\nEpoch 3: val_loss did not improve from 0.12127\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9777 - auc: 0.9990 - loss: 0.0712 - precision: 0.9803 - recall: 0.9757 - val_accuracy: 0.9431 - val_auc: 0.9943 - val_loss: 0.1992 - val_precision: 0.9485 - val_recall: 0.9400\nEpoch 4/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0608 - precision: 0.9837 - recall: 0.9797\nEpoch 4: val_loss improved from 0.12127 to 0.07279, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9812 - auc: 0.9992 - loss: 0.0608 - precision: 0.9837 - recall: 0.9797 - val_accuracy: 0.9787 - val_auc: 0.9982 - val_loss: 0.0728 - val_precision: 0.9809 - val_recall: 0.9776\nEpoch 5/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9835 - auc: 0.9994 - loss: 0.0521 - precision: 0.9853 - recall: 0.9817\nEpoch 5: val_loss did not improve from 0.07279\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9835 - auc: 0.9994 - loss: 0.0521 - precision: 0.9853 - recall: 0.9817 - val_accuracy: 0.9737 - val_auc: 0.9981 - val_loss: 0.0846 - val_precision: 0.9761 - val_recall: 0.9722\nEpoch 6/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9859 - auc: 0.9993 - loss: 0.0469 - precision: 0.9871 - recall: 0.9849\nEpoch 6: val_loss did not improve from 0.07279\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9859 - auc: 0.9993 - loss: 0.0469 - precision: 0.9871 - recall: 0.9849 - val_accuracy: 0.9652 - val_auc: 0.9960 - val_loss: 0.1336 - val_precision: 0.9667 - val_recall: 0.9642\nEpoch 7/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9866 - auc: 0.9994 - loss: 0.0431 - precision: 0.9880 - recall: 0.9859\nEpoch 7: val_loss did not improve from 0.07279\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9866 - auc: 0.9994 - loss: 0.0431 - precision: 0.9880 - recall: 0.9859 - val_accuracy: 0.9796 - val_auc: 0.9978 - val_loss: 0.0735 - val_precision: 0.9812 - val_recall: 0.9785\nEpoch 8/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9892 - auc: 0.9994 - loss: 0.0373 - precision: 0.9900 - recall: 0.9882\nEpoch 8: val_loss improved from 0.07279 to 0.05692, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9892 - auc: 0.9994 - loss: 0.0373 - precision: 0.9900 - recall: 0.9882 - val_accuracy: 0.9857 - val_auc: 0.9982 - val_loss: 0.0569 - val_precision: 0.9862 - val_recall: 0.9852\nEpoch 9/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9881 - auc: 0.9996 - loss: 0.0361 - precision: 0.9894 - recall: 0.9873\nEpoch 9: val_loss improved from 0.05692 to 0.03889, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9881 - auc: 0.9996 - loss: 0.0361 - precision: 0.9894 - recall: 0.9873 - val_accuracy: 0.9876 - val_auc: 0.9994 - val_loss: 0.0389 - val_precision: 0.9889 - val_recall: 0.9869\nEpoch 10/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9900 - auc: 0.9995 - loss: 0.0317 - precision: 0.9910 - recall: 0.9895\nEpoch 10: val_loss did not improve from 0.03889\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9900 - auc: 0.9995 - loss: 0.0317 - precision: 0.9910 - recall: 0.9895 - val_accuracy: 0.9852 - val_auc: 0.9988 - val_loss: 0.0529 - val_precision: 0.9867 - val_recall: 0.9839\nEpoch 11/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9903 - auc: 0.9997 - loss: 0.0290 - precision: 0.9914 - recall: 0.9895\nEpoch 11: val_loss did not improve from 0.03889\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9903 - auc: 0.9997 - loss: 0.0290 - precision: 0.9914 - recall: 0.9895 - val_accuracy: 0.9737 - val_auc: 0.9973 - val_loss: 0.0960 - val_precision: 0.9752 - val_recall: 0.9722\nEpoch 12/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9908 - auc: 0.9998 - loss: 0.0265 - precision: 0.9914 - recall: 0.9905\nEpoch 12: val_loss did not improve from 0.03889\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9908 - auc: 0.9998 - loss: 0.0265 - precision: 0.9914 - recall: 0.9905 - val_accuracy: 0.9873 - val_auc: 0.9985 - val_loss: 0.0492 - val_precision: 0.9879 - val_recall: 0.9868\nEpoch 13/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9926 - auc: 0.9997 - loss: 0.0235 - precision: 0.9932 - recall: 0.9922\nEpoch 13: val_loss did not improve from 0.03889\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9926 - auc: 0.9997 - loss: 0.0235 - precision: 0.9932 - recall: 0.9922 - val_accuracy: 0.9513 - val_auc: 0.9954 - val_loss: 0.1649 - val_precision: 0.9548 - val_recall: 0.9490\nEpoch 14/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9924 - auc: 0.9997 - loss: 0.0239 - precision: 0.9933 - recall: 0.9921\nEpoch 14: val_loss improved from 0.03889 to 0.03843, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9924 - auc: 0.9997 - loss: 0.0239 - precision: 0.9933 - recall: 0.9921 - val_accuracy: 0.9877 - val_auc: 0.9994 - val_loss: 0.0384 - val_precision: 0.9889 - val_recall: 0.9873\nEpoch 15/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9934 - auc: 0.9998 - loss: 0.0202 - precision: 0.9938 - recall: 0.9931\nEpoch 15: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9934 - auc: 0.9998 - loss: 0.0202 - precision: 0.9938 - recall: 0.9931 - val_accuracy: 0.9896 - val_auc: 0.9990 - val_loss: 0.0404 - val_precision: 0.9905 - val_recall: 0.9892\nEpoch 16/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - auc: 0.9997 - loss: 0.0216 - precision: 0.9938 - recall: 0.9928\nEpoch 16: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9932 - auc: 0.9997 - loss: 0.0216 - precision: 0.9938 - recall: 0.9928 - val_accuracy: 0.9843 - val_auc: 0.9982 - val_loss: 0.0616 - val_precision: 0.9851 - val_recall: 0.9835\nEpoch 17/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9950 - auc: 0.9999 - loss: 0.0147 - precision: 0.9954 - recall: 0.9948\nEpoch 17: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9950 - auc: 0.9999 - loss: 0.0147 - precision: 0.9954 - recall: 0.9948 - val_accuracy: 0.9869 - val_auc: 0.9990 - val_loss: 0.0442 - val_precision: 0.9880 - val_recall: 0.9868\nEpoch 18/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.0156 - precision: 0.9955 - recall: 0.9950\nEpoch 18: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9953 - auc: 0.9999 - loss: 0.0156 - precision: 0.9955 - recall: 0.9950 - val_accuracy: 0.9878 - val_auc: 0.9991 - val_loss: 0.0417 - val_precision: 0.9881 - val_recall: 0.9875\nEpoch 19/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0178 - precision: 0.9943 - recall: 0.9936\nEpoch 19: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0178 - precision: 0.9943 - recall: 0.9936 - val_accuracy: 0.9890 - val_auc: 0.9991 - val_loss: 0.0399 - val_precision: 0.9899 - val_recall: 0.9887\nEpoch 20/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0154 - precision: 0.9955 - recall: 0.9946\nEpoch 20: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9949 - auc: 0.9998 - loss: 0.0154 - precision: 0.9955 - recall: 0.9946 - val_accuracy: 0.9872 - val_auc: 0.9989 - val_loss: 0.0458 - val_precision: 0.9878 - val_recall: 0.9866\nEpoch 21/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9954 - auc: 0.9998 - loss: 0.0151 - precision: 0.9957 - recall: 0.9951\nEpoch 21: val_loss did not improve from 0.03843\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9954 - auc: 0.9998 - loss: 0.0151 - precision: 0.9957 - recall: 0.9951 - val_accuracy: 0.9877 - val_auc: 0.9986 - val_loss: 0.0483 - val_precision: 0.9884 - val_recall: 0.9873\nEpoch 22/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9954 - auc: 0.9999 - loss: 0.0138 - precision: 0.9956 - recall: 0.9953\nEpoch 22: val_loss improved from 0.03843 to 0.02913, saving model to dynamic_relu_model.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9954 - auc: 0.9999 - loss: 0.0138 - precision: 0.9956 - recall: 0.9953 - val_accuracy: 0.9922 - val_auc: 0.9993 - val_loss: 0.0291 - val_precision: 0.9926 - val_recall: 0.9921\nEpoch 23/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0093 - precision: 0.9971 - recall: 0.9969\nEpoch 23: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0093 - precision: 0.9971 - recall: 0.9969 - val_accuracy: 0.9912 - val_auc: 0.9990 - val_loss: 0.0321 - val_precision: 0.9918 - val_recall: 0.9908\nEpoch 24/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9961 - auc: 0.9999 - loss: 0.0113 - precision: 0.9964 - recall: 0.9959\nEpoch 24: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9961 - auc: 0.9999 - loss: 0.0113 - precision: 0.9964 - recall: 0.9959 - val_accuracy: 0.9898 - val_auc: 0.9989 - val_loss: 0.0385 - val_precision: 0.9901 - val_recall: 0.9896\nEpoch 25/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0140 - precision: 0.9952 - recall: 0.9947\nEpoch 25: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9951 - auc: 0.9999 - loss: 0.0140 - precision: 0.9952 - recall: 0.9947 - val_accuracy: 0.9903 - val_auc: 0.9990 - val_loss: 0.0367 - val_precision: 0.9907 - val_recall: 0.9902\nEpoch 26/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.0094 - precision: 0.9969 - recall: 0.9964\nEpoch 26: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 1.0000 - loss: 0.0094 - precision: 0.9969 - recall: 0.9964 - val_accuracy: 0.9921 - val_auc: 0.9989 - val_loss: 0.0354 - val_precision: 0.9924 - val_recall: 0.9921\nEpoch 27/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0111 - precision: 0.9959 - recall: 0.9957\nEpoch 27: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0111 - precision: 0.9959 - recall: 0.9957 - val_accuracy: 0.9904 - val_auc: 0.9992 - val_loss: 0.0352 - val_precision: 0.9907 - val_recall: 0.9903\nEpoch 28/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0093 - precision: 0.9967 - recall: 0.9963\nEpoch 28: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9964 - auc: 0.9999 - loss: 0.0093 - precision: 0.9967 - recall: 0.9963 - val_accuracy: 0.9888 - val_auc: 0.9991 - val_loss: 0.0401 - val_precision: 0.9893 - val_recall: 0.9883\nEpoch 29/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0083 - precision: 0.9976 - recall: 0.9974\nEpoch 29: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9974 - auc: 0.9999 - loss: 0.0083 - precision: 0.9976 - recall: 0.9974 - val_accuracy: 0.9881 - val_auc: 0.9979 - val_loss: 0.0551 - val_precision: 0.9882 - val_recall: 0.9880\nEpoch 30/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0100 - precision: 0.9969 - recall: 0.9965\nEpoch 30: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9967 - auc: 0.9999 - loss: 0.0100 - precision: 0.9969 - recall: 0.9965 - val_accuracy: 0.9822 - val_auc: 0.9978 - val_loss: 0.0727 - val_precision: 0.9831 - val_recall: 0.9818\nEpoch 31/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0081 - precision: 0.9978 - recall: 0.9977\nEpoch 31: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0081 - precision: 0.9978 - recall: 0.9977 - val_accuracy: 0.9902 - val_auc: 0.9981 - val_loss: 0.0509 - val_precision: 0.9904 - val_recall: 0.9900\nEpoch 32/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0103 - precision: 0.9966 - recall: 0.9963\nEpoch 32: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0103 - precision: 0.9966 - recall: 0.9963 - val_accuracy: 0.9908 - val_auc: 0.9985 - val_loss: 0.0380 - val_precision: 0.9912 - val_recall: 0.9907\nEpoch 33/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0095 - precision: 0.9967 - recall: 0.9965\nEpoch 33: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0095 - precision: 0.9967 - recall: 0.9965 - val_accuracy: 0.9901 - val_auc: 0.9985 - val_loss: 0.0418 - val_precision: 0.9906 - val_recall: 0.9900\nEpoch 34/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0098 - precision: 0.9968 - recall: 0.9965\nEpoch 34: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9966 - auc: 0.9999 - loss: 0.0098 - precision: 0.9968 - recall: 0.9965 - val_accuracy: 0.9846 - val_auc: 0.9978 - val_loss: 0.0680 - val_precision: 0.9856 - val_recall: 0.9840\nEpoch 35/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0072 - precision: 0.9977 - recall: 0.9975\nEpoch 35: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9976 - auc: 0.9999 - loss: 0.0072 - precision: 0.9977 - recall: 0.9975 - val_accuracy: 0.9850 - val_auc: 0.9969 - val_loss: 0.0836 - val_precision: 0.9857 - val_recall: 0.9848\nEpoch 36/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0069 - precision: 0.9978 - recall: 0.9977\nEpoch 36: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0069 - precision: 0.9978 - recall: 0.9977 - val_accuracy: 0.9818 - val_auc: 0.9971 - val_loss: 0.0841 - val_precision: 0.9823 - val_recall: 0.9815\nEpoch 37/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0094 - precision: 0.9965 - recall: 0.9965\nEpoch 37: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9999 - loss: 0.0094 - precision: 0.9965 - recall: 0.9965 - val_accuracy: 0.9915 - val_auc: 0.9988 - val_loss: 0.0379 - val_precision: 0.9919 - val_recall: 0.9914\nEpoch 38/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 1.0000 - loss: 0.0063 - precision: 0.9978 - recall: 0.9977\nEpoch 38: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 1.0000 - loss: 0.0063 - precision: 0.9978 - recall: 0.9977 - val_accuracy: 0.9893 - val_auc: 0.9983 - val_loss: 0.0500 - val_precision: 0.9896 - val_recall: 0.9888\nEpoch 39/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0069 - precision: 0.9979 - recall: 0.9977\nEpoch 39: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0069 - precision: 0.9979 - recall: 0.9977 - val_accuracy: 0.9901 - val_auc: 0.9986 - val_loss: 0.0442 - val_precision: 0.9907 - val_recall: 0.9898\nEpoch 40/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0071 - precision: 0.9979 - recall: 0.9977\nEpoch 40: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9978 - auc: 0.9999 - loss: 0.0071 - precision: 0.9979 - recall: 0.9977 - val_accuracy: 0.9926 - val_auc: 0.9986 - val_loss: 0.0375 - val_precision: 0.9929 - val_recall: 0.9925\nEpoch 41/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0051 - precision: 0.9983 - recall: 0.9982\nEpoch 41: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9982 - auc: 1.0000 - loss: 0.0051 - precision: 0.9983 - recall: 0.9982 - val_accuracy: 0.9885 - val_auc: 0.9976 - val_loss: 0.0653 - val_precision: 0.9887 - val_recall: 0.9883\nEpoch 42/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0070 - precision: 0.9977 - recall: 0.9976\nEpoch 42: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0070 - precision: 0.9977 - recall: 0.9976 - val_accuracy: 0.9908 - val_auc: 0.9988 - val_loss: 0.0407 - val_precision: 0.9911 - val_recall: 0.9905\nEpoch 43/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: 0.0060 - precision: 0.9979 - recall: 0.9978\nEpoch 43: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9979 - auc: 0.9999 - loss: 0.0060 - precision: 0.9979 - recall: 0.9978 - val_accuracy: 0.9807 - val_auc: 0.9970 - val_loss: 0.0864 - val_precision: 0.9811 - val_recall: 0.9803\nEpoch 44/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065 - precision: 0.9975 - recall: 0.9974\nEpoch 44: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0065 - precision: 0.9975 - recall: 0.9974 - val_accuracy: 0.9907 - val_auc: 0.9985 - val_loss: 0.0427 - val_precision: 0.9911 - val_recall: 0.9905\nEpoch 45/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0058 - precision: 0.9980 - recall: 0.9979\nEpoch 45: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0058 - precision: 0.9980 - recall: 0.9979 - val_accuracy: 0.9892 - val_auc: 0.9985 - val_loss: 0.0475 - val_precision: 0.9893 - val_recall: 0.9889\nEpoch 46/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0058 - precision: 0.9983 - recall: 0.9983\nEpoch 46: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0058 - precision: 0.9983 - recall: 0.9983 - val_accuracy: 0.9902 - val_auc: 0.9984 - val_loss: 0.0454 - val_precision: 0.9904 - val_recall: 0.9902\nEpoch 47/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0043 - precision: 0.9985 - recall: 0.9983\nEpoch 47: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0043 - precision: 0.9985 - recall: 0.9983 - val_accuracy: 0.9890 - val_auc: 0.9981 - val_loss: 0.0533 - val_precision: 0.9893 - val_recall: 0.9887\nEpoch 48/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0052 - precision: 0.9983 - recall: 0.9982\nEpoch 48: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0052 - precision: 0.9983 - recall: 0.9982 - val_accuracy: 0.9887 - val_auc: 0.9979 - val_loss: 0.0595 - val_precision: 0.9889 - val_recall: 0.9883\nEpoch 49/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0079 - precision: 0.9976 - recall: 0.9974\nEpoch 49: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9975 - auc: 0.9999 - loss: 0.0079 - precision: 0.9976 - recall: 0.9974 - val_accuracy: 0.9922 - val_auc: 0.9989 - val_loss: 0.0372 - val_precision: 0.9924 - val_recall: 0.9922\nEpoch 50/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0052 - precision: 0.9985 - recall: 0.9984\nEpoch 50: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0052 - precision: 0.9985 - recall: 0.9984 - val_accuracy: 0.9920 - val_auc: 0.9987 - val_loss: 0.0375 - val_precision: 0.9922 - val_recall: 0.9918\nEpoch 51/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0061 - precision: 0.9981 - recall: 0.9980\nEpoch 51: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0061 - precision: 0.9981 - recall: 0.9980 - val_accuracy: 0.9850 - val_auc: 0.9971 - val_loss: 0.0777 - val_precision: 0.9853 - val_recall: 0.9848\nEpoch 52/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0054 - precision: 0.9982 - recall: 0.9981\nEpoch 52: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0054 - precision: 0.9982 - recall: 0.9981 - val_accuracy: 0.9687 - val_auc: 0.9950 - val_loss: 0.1443 - val_precision: 0.9696 - val_recall: 0.9685\nEpoch 53/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0060 - precision: 0.9978 - recall: 0.9976\nEpoch 53: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9977 - auc: 0.9999 - loss: 0.0060 - precision: 0.9978 - recall: 0.9976 - val_accuracy: 0.9922 - val_auc: 0.9986 - val_loss: 0.0402 - val_precision: 0.9923 - val_recall: 0.9919\nEpoch 54/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0041 - precision: 0.9987 - recall: 0.9986\nEpoch 54: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0041 - precision: 0.9987 - recall: 0.9986 - val_accuracy: 0.9915 - val_auc: 0.9984 - val_loss: 0.0435 - val_precision: 0.9917 - val_recall: 0.9913\nEpoch 55/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0048 - precision: 0.9985 - recall: 0.9984\nEpoch 55: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0048 - precision: 0.9985 - recall: 0.9983 - val_accuracy: 0.9830 - val_auc: 0.9961 - val_loss: 0.0952 - val_precision: 0.9831 - val_recall: 0.9830\nEpoch 56/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0048 - precision: 0.9985 - recall: 0.9985\nEpoch 56: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0048 - precision: 0.9985 - recall: 0.9985 - val_accuracy: 0.9912 - val_auc: 0.9982 - val_loss: 0.0433 - val_precision: 0.9917 - val_recall: 0.9912\nEpoch 57/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.0036 - precision: 0.9989 - recall: 0.9988\nEpoch 57: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.0036 - precision: 0.9989 - recall: 0.9988 - val_accuracy: 0.9902 - val_auc: 0.9979 - val_loss: 0.0597 - val_precision: 0.9904 - val_recall: 0.9902\nEpoch 58/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0047 - precision: 0.9985 - recall: 0.9983\nEpoch 58: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0047 - precision: 0.9985 - recall: 0.9983 - val_accuracy: 0.9879 - val_auc: 0.9972 - val_loss: 0.0792 - val_precision: 0.9880 - val_recall: 0.9877\nEpoch 59/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0041 - precision: 0.9985 - recall: 0.9984\nEpoch 59: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0041 - precision: 0.9985 - recall: 0.9984 - val_accuracy: 0.9903 - val_auc: 0.9979 - val_loss: 0.0547 - val_precision: 0.9903 - val_recall: 0.9903\nEpoch 60/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0048 - precision: 0.9985 - recall: 0.9984\nEpoch 60: val_loss did not improve from 0.02913\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 0.9999 - loss: 0.0049 - precision: 0.9985 - recall: 0.9984 - val_accuracy: 0.9893 - val_auc: 0.9981 - val_loss: 0.0577 - val_precision: 0.9896 - val_recall: 0.9893\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to calculate F1 scores from precision and recall\ndef calculate_f1_scores(precision, recall):\n    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n\ntop_3_dict = {}\ntest = {}\n# histories = dict(histories[\"custom\"].history)\ntest[\"custom\"] = histories[\"custom\"].history\n\n\nfor model_name, history in test.items():\n    print(f\"\\n{model_name} Results:\")\n\n    # Assuming history['loss'], history['val_loss'], etc., exist\n    training_loss = history['loss']\n    validation_loss = history['val_loss']\n    training_accuracy = history['accuracy']\n    validation_accuracy = history['val_accuracy']\n    training_auc = history['auc']\n    validation_auc = history['val_auc']\n    training_precision = history['precision']\n    validation_precision = history['val_precision']\n    training_recall = history['recall']\n    validation_recall = history['val_recall']\n    \n\n\n    # Calculate F1 scores based on available precision and recall in history\n    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n\n\n    top_3_dict[model_name] = {\n        \"training_loss\": sorted(training_loss)[:3],\n        \"validation_loss\": sorted(validation_loss)[:3],\n        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n\n    }\n\n    # Print Top 3 Lowest Losses\n    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n\n    # Print Top 3 Highest Accuracies\n    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n\n    # Print Top 3 AUCs\n    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n\n    # Print Top 3 F1 Scores\n    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n\n    # Print Top 3 Precision\n    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n\n    # Print Top 3 Recall\n    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T11:15:28.612165Z","iopub.execute_input":"2024-03-16T11:15:28.612787Z","iopub.status.idle":"2024-03-16T11:15:28.629885Z","shell.execute_reply.started":"2024-03-16T11:15:28.612756Z","shell.execute_reply":"2024-03-16T11:15:28.628935Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\ncustom Results:\nTop 3 Lowest Training Losses: [0.003998010419309139, 0.0048003895208239555, 0.004837052430957556]\nTop 3 Lowest Validation Losses: [0.029127471148967743, 0.03208880499005318, 0.03524142503738403]\nTop 3 Highest Training Accuracies: [0.9987916946411133, 0.9985833168029785, 0.9984583258628845]\nTop 3 Highest Validation Accuracies: [0.9925833344459534, 0.9921666383743286, 0.9921666383743286]\nTop 3 Training AUCs: [0.9999642968177795, 0.9999637603759766, 0.9999528527259827]\nTop 3 Validation AUCs: [0.9994426369667053, 0.9994312524795532, 0.9992927312850952]\nTop 3 Training F1 Scores: [0.9987812015127027, 0.9985415896514309, 0.9984791261503784]\nTop 3 Validation F1 Scores: [0.992706821967684, 0.9923314129581963, 0.9922906899302029]\nTop 3 Training Precision: [0.9988332390785217, 0.9986039996147156, 0.9985207319259644]\nTop 3 Validation Precision: [0.9929137229919434, 0.9925796389579773, 0.9924147725105286]\nTop 3 Training Recall: [0.9987291693687439, 0.9984791874885559, 0.9984375238418579]\nTop 3 Validation Recall: [0.9925000071525574, 0.9921666383743286, 0.9920833110809326]\n","output_type":"stream"}]},{"cell_type":"code","source":"def mish(x):\n    return x * tf.math.tanh(tf.math.softplus(x))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T14:57:45.443248Z","iopub.execute_input":"2024-03-11T14:57:45.443574Z","iopub.status.idle":"2024-03-11T14:57:45.448746Z","shell.execute_reply.started":"2024-03-11T14:57:45.443546Z","shell.execute_reply":"2024-03-11T14:57:45.447657Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"history_mish, model_mish = train_model(mish, x_train_split, y_train_split,\n                          x_val_split, y_val_split, batch_size, learning_rate, \"mish\")\nhistories[\"mish\"] = history_mish","metadata":{"execution":{"iopub.status.busy":"2024-03-11T15:30:05.743419Z","iopub.execute_input":"2024-03-11T15:30:05.744314Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9119 - auc: 0.9890 - loss: 0.2891 - precision: 0.9450 - recall: 0.8806\nEpoch 1: val_loss improved from inf to 0.15094, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 13ms/step - accuracy: 0.9119 - auc: 0.9890 - loss: 0.2890 - precision: 0.9450 - recall: 0.8806 - val_accuracy: 0.9617 - val_auc: 0.9950 - val_loss: 0.1509 - val_precision: 0.9646 - val_recall: 0.9594\nEpoch 2/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9833 - auc: 0.9991 - loss: 0.0569 - precision: 0.9853 - recall: 0.9816\nEpoch 2: val_loss improved from 0.15094 to 0.10609, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9833 - auc: 0.9991 - loss: 0.0569 - precision: 0.9853 - recall: 0.9816 - val_accuracy: 0.9713 - val_auc: 0.9974 - val_loss: 0.1061 - val_precision: 0.9739 - val_recall: 0.9699\nEpoch 3/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9848 - auc: 0.9993 - loss: 0.0474 - precision: 0.9860 - recall: 0.9837\nEpoch 3: val_loss improved from 0.10609 to 0.03816, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9848 - auc: 0.9993 - loss: 0.0473 - precision: 0.9860 - recall: 0.9837 - val_accuracy: 0.9872 - val_auc: 0.9996 - val_loss: 0.0382 - val_precision: 0.9883 - val_recall: 0.9865\nEpoch 4/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9899 - auc: 0.9996 - loss: 0.0325 - precision: 0.9911 - recall: 0.9890\nEpoch 4: val_loss did not improve from 0.03816\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9899 - auc: 0.9996 - loss: 0.0325 - precision: 0.9911 - recall: 0.9890 - val_accuracy: 0.9873 - val_auc: 0.9992 - val_loss: 0.0422 - val_precision: 0.9887 - val_recall: 0.9868\nEpoch 5/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9908 - auc: 0.9996 - loss: 0.0290 - precision: 0.9917 - recall: 0.9902\nEpoch 5: val_loss improved from 0.03816 to 0.03048, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9908 - auc: 0.9996 - loss: 0.0290 - precision: 0.9917 - recall: 0.9902 - val_accuracy: 0.9906 - val_auc: 0.9994 - val_loss: 0.0305 - val_precision: 0.9912 - val_recall: 0.9902\nEpoch 6/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9918 - auc: 0.9996 - loss: 0.0264 - precision: 0.9925 - recall: 0.9909\nEpoch 6: val_loss did not improve from 0.03048\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9918 - auc: 0.9996 - loss: 0.0264 - precision: 0.9925 - recall: 0.9909 - val_accuracy: 0.9881 - val_auc: 0.9991 - val_loss: 0.0383 - val_precision: 0.9887 - val_recall: 0.9877\nEpoch 7/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9931 - auc: 0.9999 - loss: 0.0206 - precision: 0.9936 - recall: 0.9927\nEpoch 7: val_loss did not improve from 0.03048\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9931 - auc: 0.9999 - loss: 0.0206 - precision: 0.9936 - recall: 0.9927 - val_accuracy: 0.9844 - val_auc: 0.9991 - val_loss: 0.0512 - val_precision: 0.9855 - val_recall: 0.9836\nEpoch 8/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0159 - precision: 0.9947 - recall: 0.9939\nEpoch 8: val_loss did not improve from 0.03048\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9940 - auc: 0.9999 - loss: 0.0159 - precision: 0.9947 - recall: 0.9939 - val_accuracy: 0.9847 - val_auc: 0.9989 - val_loss: 0.0516 - val_precision: 0.9864 - val_recall: 0.9834\nEpoch 9/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0159 - precision: 0.9957 - recall: 0.9951\nEpoch 9: val_loss did not improve from 0.03048\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9955 - auc: 0.9998 - loss: 0.0159 - precision: 0.9957 - recall: 0.9951 - val_accuracy: 0.9903 - val_auc: 0.9991 - val_loss: 0.0337 - val_precision: 0.9908 - val_recall: 0.9899\nEpoch 10/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0112 - precision: 0.9971 - recall: 0.9966\nEpoch 10: val_loss did not improve from 0.03048\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0112 - precision: 0.9971 - recall: 0.9966 - val_accuracy: 0.9907 - val_auc: 0.9990 - val_loss: 0.0368 - val_precision: 0.9912 - val_recall: 0.9899\nEpoch 11/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0116 - precision: 0.9968 - recall: 0.9963\nEpoch 11: val_loss improved from 0.03048 to 0.02623, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9965 - auc: 0.9998 - loss: 0.0116 - precision: 0.9968 - recall: 0.9963 - val_accuracy: 0.9917 - val_auc: 0.9996 - val_loss: 0.0262 - val_precision: 0.9924 - val_recall: 0.9913\nEpoch 12/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0076 - precision: 0.9973 - recall: 0.9972\nEpoch 12: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9973 - auc: 1.0000 - loss: 0.0076 - precision: 0.9973 - recall: 0.9972 - val_accuracy: 0.9903 - val_auc: 0.9992 - val_loss: 0.0336 - val_precision: 0.9908 - val_recall: 0.9902\nEpoch 13/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0101 - precision: 0.9970 - recall: 0.9968\nEpoch 13: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9968 - auc: 0.9999 - loss: 0.0101 - precision: 0.9970 - recall: 0.9968 - val_accuracy: 0.9887 - val_auc: 0.9989 - val_loss: 0.0444 - val_precision: 0.9897 - val_recall: 0.9880\nEpoch 14/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0046 - precision: 0.9988 - recall: 0.9985\nEpoch 14: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9987 - auc: 1.0000 - loss: 0.0046 - precision: 0.9988 - recall: 0.9985 - val_accuracy: 0.9887 - val_auc: 0.9985 - val_loss: 0.0433 - val_precision: 0.9890 - val_recall: 0.9887\nEpoch 15/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0087 - precision: 0.9972 - recall: 0.9969\nEpoch 15: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9970 - auc: 0.9999 - loss: 0.0087 - precision: 0.9972 - recall: 0.9969 - val_accuracy: 0.9911 - val_auc: 0.9987 - val_loss: 0.0368 - val_precision: 0.9913 - val_recall: 0.9908\nEpoch 16/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0059 - precision: 0.9981 - recall: 0.9979\nEpoch 16: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9979 - auc: 1.0000 - loss: 0.0059 - precision: 0.9981 - recall: 0.9979 - val_accuracy: 0.9912 - val_auc: 0.9987 - val_loss: 0.0405 - val_precision: 0.9914 - val_recall: 0.9911\nEpoch 17/60\n\u001b[1m1495/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0060 - precision: 0.9981 - recall: 0.9979\nEpoch 17: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0060 - precision: 0.9981 - recall: 0.9979 - val_accuracy: 0.9935 - val_auc: 0.9991 - val_loss: 0.0287 - val_precision: 0.9937 - val_recall: 0.9935\nEpoch 18/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0052 - precision: 0.9984 - recall: 0.9983\nEpoch 18: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9983 - auc: 1.0000 - loss: 0.0052 - precision: 0.9984 - recall: 0.9983 - val_accuracy: 0.9933 - val_auc: 0.9991 - val_loss: 0.0265 - val_precision: 0.9937 - val_recall: 0.9933\nEpoch 19/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0035 - precision: 0.9990 - recall: 0.9990\nEpoch 19: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9990 - auc: 1.0000 - loss: 0.0035 - precision: 0.9990 - recall: 0.9990 - val_accuracy: 0.9923 - val_auc: 0.9987 - val_loss: 0.0356 - val_precision: 0.9926 - val_recall: 0.9922\nEpoch 20/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0046 - precision: 0.9984 - recall: 0.9983\nEpoch 20: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0046 - precision: 0.9984 - recall: 0.9983 - val_accuracy: 0.9901 - val_auc: 0.9985 - val_loss: 0.0434 - val_precision: 0.9902 - val_recall: 0.9900\nEpoch 21/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0053 - precision: 0.9982 - recall: 0.9980\nEpoch 21: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9981 - auc: 0.9999 - loss: 0.0053 - precision: 0.9982 - recall: 0.9980 - val_accuracy: 0.9934 - val_auc: 0.9992 - val_loss: 0.0267 - val_precision: 0.9935 - val_recall: 0.9933\nEpoch 22/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0028 - precision: 0.9992 - recall: 0.9991\nEpoch 22: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0028 - precision: 0.9992 - recall: 0.9991 - val_accuracy: 0.9889 - val_auc: 0.9987 - val_loss: 0.0452 - val_precision: 0.9893 - val_recall: 0.9887\nEpoch 23/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9998 - loss: 0.0066 - precision: 0.9983 - recall: 0.9982\nEpoch 23: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9983 - auc: 0.9998 - loss: 0.0066 - precision: 0.9983 - recall: 0.9982 - val_accuracy: 0.9920 - val_auc: 0.9991 - val_loss: 0.0310 - val_precision: 0.9923 - val_recall: 0.9916\nEpoch 24/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0027 - precision: 0.9992 - recall: 0.9992\nEpoch 24: val_loss did not improve from 0.02623\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9992 - auc: 1.0000 - loss: 0.0027 - precision: 0.9992 - recall: 0.9992 - val_accuracy: 0.9902 - val_auc: 0.9984 - val_loss: 0.0478 - val_precision: 0.9905 - val_recall: 0.9899\nEpoch 25/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0052 - precision: 0.9987 - recall: 0.9986\nEpoch 25: val_loss improved from 0.02623 to 0.02335, saving model to mish.keras\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9987 - auc: 0.9999 - loss: 0.0052 - precision: 0.9987 - recall: 0.9986 - val_accuracy: 0.9942 - val_auc: 0.9992 - val_loss: 0.0234 - val_precision: 0.9945 - val_recall: 0.9941\nEpoch 26/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0019 - precision: 0.9993 - recall: 0.9993\nEpoch 26: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0019 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9883 - val_auc: 0.9983 - val_loss: 0.0489 - val_precision: 0.9888 - val_recall: 0.9881\nEpoch 27/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.0049 - precision: 0.9988 - recall: 0.9987\nEpoch 27: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9988 - auc: 0.9999 - loss: 0.0049 - precision: 0.9988 - recall: 0.9987 - val_accuracy: 0.9901 - val_auc: 0.9983 - val_loss: 0.0478 - val_precision: 0.9904 - val_recall: 0.9900\nEpoch 28/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995\nEpoch 28: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9995 - auc: 1.0000 - loss: 0.0017 - precision: 0.9995 - recall: 0.9995 - val_accuracy: 0.9910 - val_auc: 0.9986 - val_loss: 0.0447 - val_precision: 0.9912 - val_recall: 0.9908\nEpoch 29/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0051 - precision: 0.9983 - recall: 0.9983\nEpoch 29: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0051 - precision: 0.9983 - recall: 0.9983 - val_accuracy: 0.9830 - val_auc: 0.9977 - val_loss: 0.0744 - val_precision: 0.9837 - val_recall: 0.9827\nEpoch 30/60\n\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0018 - precision: 0.9994 - recall: 0.9993\nEpoch 30: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0018 - precision: 0.9994 - recall: 0.9993 - val_accuracy: 0.9937 - val_auc: 0.9989 - val_loss: 0.0315 - val_precision: 0.9939 - val_recall: 0.9936\nEpoch 31/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 0.9999 - loss: 0.0044 - precision: 0.9989 - recall: 0.9988\nEpoch 31: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9989 - auc: 0.9999 - loss: 0.0044 - precision: 0.9989 - recall: 0.9988 - val_accuracy: 0.9930 - val_auc: 0.9987 - val_loss: 0.0323 - val_precision: 0.9932 - val_recall: 0.9929\nEpoch 32/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0020 - precision: 0.9994 - recall: 0.9994\nEpoch 32: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0020 - precision: 0.9994 - recall: 0.9994 - val_accuracy: 0.9934 - val_auc: 0.9987 - val_loss: 0.0357 - val_precision: 0.9937 - val_recall: 0.9933\nEpoch 33/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0021 - precision: 0.9994 - recall: 0.9994\nEpoch 33: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0021 - precision: 0.9994 - recall: 0.9994 - val_accuracy: 0.9928 - val_auc: 0.9987 - val_loss: 0.0354 - val_precision: 0.9931 - val_recall: 0.9927\nEpoch 34/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0034 - precision: 0.9993 - recall: 0.9993\nEpoch 34: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0034 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9934 - val_auc: 0.9990 - val_loss: 0.0300 - val_precision: 0.9936 - val_recall: 0.9934\nEpoch 35/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0015 - precision: 0.9996 - recall: 0.9996\nEpoch 35: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0015 - precision: 0.9996 - recall: 0.9996 - val_accuracy: 0.9932 - val_auc: 0.9982 - val_loss: 0.0426 - val_precision: 0.9933 - val_recall: 0.9931\nEpoch 36/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0037 - precision: 0.9990 - recall: 0.9989\nEpoch 36: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0037 - precision: 0.9990 - recall: 0.9989 - val_accuracy: 0.9928 - val_auc: 0.9987 - val_loss: 0.0355 - val_precision: 0.9929 - val_recall: 0.9927\nEpoch 37/60\n\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0013 - precision: 0.9996 - recall: 0.9996\nEpoch 37: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9996 - auc: 1.0000 - loss: 0.0013 - precision: 0.9996 - recall: 0.9996 - val_accuracy: 0.9927 - val_auc: 0.9983 - val_loss: 0.0427 - val_precision: 0.9928 - val_recall: 0.9926\nEpoch 38/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0043 - precision: 0.9986 - recall: 0.9986\nEpoch 38: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9986 - auc: 0.9999 - loss: 0.0043 - precision: 0.9986 - recall: 0.9986 - val_accuracy: 0.9933 - val_auc: 0.9987 - val_loss: 0.0337 - val_precision: 0.9935 - val_recall: 0.9931\nEpoch 39/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0025 - precision: 0.9993 - recall: 0.9993\nEpoch 39: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 1.0000 - loss: 0.0025 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9929 - val_auc: 0.9988 - val_loss: 0.0353 - val_precision: 0.9930 - val_recall: 0.9929\nEpoch 40/60\n\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0013 - precision: 0.9997 - recall: 0.9997\nEpoch 40: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0013 - precision: 0.9997 - recall: 0.9997 - val_accuracy: 0.9906 - val_auc: 0.9984 - val_loss: 0.0454 - val_precision: 0.9907 - val_recall: 0.9903\nEpoch 41/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9995 - recall: 0.9994\nEpoch 41: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - accuracy: 0.9994 - auc: 1.0000 - loss: 0.0019 - precision: 0.9995 - recall: 0.9994 - val_accuracy: 0.9927 - val_auc: 0.9986 - val_loss: 0.0394 - val_precision: 0.9929 - val_recall: 0.9927\nEpoch 42/60\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0029 - precision: 0.9993 - recall: 0.9993\nEpoch 42: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.9993 - auc: 0.9999 - loss: 0.0029 - precision: 0.9993 - recall: 0.9993 - val_accuracy: 0.9948 - val_auc: 0.9990 - val_loss: 0.0268 - val_precision: 0.9948 - val_recall: 0.9948\nEpoch 43/60\n\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.3794e-04 - precision: 1.0000 - recall: 1.0000\nEpoch 43: val_loss did not improve from 0.02335\n\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 3.3887e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9913 - val_auc: 0.9983 - val_loss: 0.0525 - val_precision: 0.9916 - val_recall: 0.9913\nEpoch 44/60\n\u001b[1m 263/1500\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0062 - precision: 0.9983 - recall: 0.9983","output_type":"stream"}]},{"cell_type":"code","source":"history_Save = {\n    \"relu\" : histories[\"relu\"].history, \n    \"sigmoid\" : histories[\"sigmoid\"].history, \n    \"tanh\" : histories[\"tanh\"].history, \n    \"custom\" : histories[\"custom\"].history, \n    \"mish\" : histories[\"mish\"].history}","metadata":{"execution":{"iopub.status.busy":"2024-03-11T15:25:02.367258Z","iopub.execute_input":"2024-03-11T15:25:02.367670Z","iopub.status.idle":"2024-03-11T15:25:02.374739Z","shell.execute_reply.started":"2024-03-11T15:25:02.367637Z","shell.execute_reply":"2024-03-11T15:25:02.373708Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Dumping the dictionary into a pickle file\nfile_path = \"history_mitbih.pkl\"\nwith open(file_path, 'wb') as file:\n    pickle.dump(history_Save, file)\n\nfile_path","metadata":{"execution":{"iopub.status.busy":"2024-03-11T15:25:04.052116Z","iopub.execute_input":"2024-03-11T15:25:04.052508Z","iopub.status.idle":"2024-03-11T15:25:04.061338Z","shell.execute_reply.started":"2024-03-11T15:25:04.052469Z","shell.execute_reply":"2024-03-11T15:25:04.060214Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'history_mitbih.pkl'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}