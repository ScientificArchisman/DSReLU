{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37484,"sourceType":"datasetVersion","datasetId":29414}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:33:46.643925Z","iopub.execute_input":"2024-04-05T19:33:46.644436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{"execution":{"iopub.status.busy":"2024-04-04T14:20:51.816382Z","iopub.execute_input":"2024-04-04T14:20:51.817610Z","iopub.status.idle":"2024-04-04T14:20:51.824371Z","shell.execute_reply.started":"2024-04-04T14:20:51.817555Z","shell.execute_reply":"2024-04-04T14:20:51.822936Z"}}},{"cell_type":"code","source":"train_set = pd.read_csv(\"/kaggle/input/heartbeat/mitbih_train.csv\", header = None)\nvalidation_set = pd.read_csv(\"/kaggle/input/heartbeat/mitbih_test.csv\", header = None)\n\ntotal_size = len(train_set) + len(validation_set)\ntrain_split_frac = len(train_set) / total_size\ntest_split_frac = len(validation_set) / total_size\n\n\n\nprint(\"Shape of train set\", train_set.shape)\nprint(\"Shape of test set\", validation_set.shape)\nprint(f\"Train-test split {train_split_frac:.2f} - {test_split_frac:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into features and labels and shuffle\nRANDOM_STATE = 42\n\nX_train, y_train = train_set.iloc[:, :-1].values, train_set.iloc[:, -1].values\nX_train, y_train = shuffle(X_train, y_train, random_state=RANDOM_STATE)\nX_train = X_train.reshape((-1, 187, 1, 1))  # Reshape X_train to have a single channel\n\n\nX_test, y_test = validation_set.iloc[:, :-1].values, validation_set.iloc[:, -1].values\nX_test, y_test = shuffle(X_test, y_test, random_state=RANDOM_STATE)\nX_test = X_test.reshape((-1, 187, 1, 1))  # Reshape X_train to have a single channel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 8))\n\npd.DataFrame(y_train).value_counts().plot(kind = \"pie\", ax = ax1, autopct = \"%1.1f%%\")\npd.DataFrame(y_test).value_counts().plot(kind = \"pie\", ax = ax2, autopct = \"%1.1f%%\")\n\nax1.set_title(\"Distribution of Train labels\", fontsize = 15, fontweight = \"bold\")\nax2.set_title(\"Distribution of Test labels\", fontsize = 15, fontweight = \"bold\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\nnum_classes = 5\ny_train = to_categorical(y_train, num_classes=num_classes)\ny_test = to_categorical(y_test, num_classes = num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \\\nActivation, Add, AveragePooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\ndef resnet_block(input_data, filters, conv_size, activation_func):\n    x = Conv2D(filters, conv_size, padding='same')(input_data)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    x = Conv2D(filters, conv_size, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    # Adding the input data to the output of the block (Skip Connection)\n    x = Add()([x, input_data])\n\n    x = Activation(activation_func)(x)\n    return x\n\ndef build_resnet20(input_shape, num_classes, activation_func='relu'):\n    inputs = Input(shape=input_shape)\n\n    # Initial Conv Layer\n    x = Conv2D(16, (3, 3), padding='same')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    # ResNet Blocks\n    for _ in range(3):\n        x = resnet_block(x, 16, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 32, (3, 3), activation_func)\n\n    # Transition Layer\n    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_func)(x)\n\n    for _ in range(3):\n        x = resnet_block(x, 64, (3, 3), activation_func)\n\n    # Average Pooling and Flattening\n    x = AveragePooling2D(pool_size=(2, 1))(x)\n    x = Flatten()(x)\n\n    # Output Layer\n    outputs = Dense(num_classes, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Custom activation function\ndef custom_activation(x):\n    # Define your custom activation logic here\n    return tf.nn.relu(x)  # Example: using ReLU as a placeholder\n\n# Building the model with the custom activation function\ninput_shape = (32, 32, 3)  # Change based on your dataset\nnum_classes = 5  # Change based on your dataset\n\nmodel = build_resnet20(input_shape, num_classes, custom_activation)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(activation_func, x_train, y_train, x_val, y_val, batch_size, learning_rate, name):\n    # Build the model\n    model = build_resnet20(input_shape=(187, 1, 1), num_classes=num_classes, activation_func=activation_func)\n\n    # Compile the model with specified learning rate\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                  metrics=[\n                  'accuracy',\n                  tf.keras.metrics.Precision(name='precision'),\n                  tf.keras.metrics.Recall(name='recall'),\n                  tf.keras.metrics.AUC(name='auc')\n              ])\n    \n        # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        f'{name}.keras', # Path where to save the model\n        save_best_only=True, # Only save a model if `val_loss` has improved\n        save_weights_only = False,\n        monitor='val_loss', # Monitor the validation loss\n        mode='min', # The lower the validation loss, the better the model\n        verbose=1 # Log a message when a better model is found\n    )\n\n\n    # Train the model with specified batch size\n    history = model.fit(x_train, y_train, epochs=60, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1, callbacks = [checkpoint_cb])\n\n    return history\n\n\n\n# Parameters\nbatch_size = 32\nlearning_rate = 0.005\n\n# Activation functions to try\nactivation_functions = [tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\nnames = [\"relu\", \"sigmoid\", \"tanh\"]\nhistories = {}\n\n# Train and evaluate the model with each activation function\nprint(f\"Training with Relu activation function\")\nhistory_relu = train_model(tf.nn.relu, X_train, y_train,\n                          X_test, y_test, batch_size, learning_rate, \"relu\")\nhistories[\"RELU\"] = history_relu\n\nprint(f\"\\n\\n Training with Sigmoid activation function\")\nhistory_sigmoid = train_model( tf.nn.sigmoid, X_train, y_train,\n                          X_test, y_test, batch_size, learning_rate, \"sigmoid\")\nhistories[\"SIGMOID\"] = history_sigmoid\n\n\nprint(f\"\\n\\n Training with Tanh activation function\")\nhistory_tanh = train_model(tf.nn.tanh, X_train, y_train,\n                          X_test, y_test, batch_size, learning_rate, \"tanh\")\nhistories[\"TANH\"] = history_tanh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n\nclass SmoothTransitionReLU(tf.keras.layers.Layer):\n    def __init__(self, initial_slope, final_slope, steepness=10, **kwargs):\n        super(SmoothTransitionReLU, self).__init__(**kwargs)\n        self.initial_slope = initial_slope\n        self.final_slope = final_slope\n        self.steepness = steepness\n        # Internal counter to track the relative progress of training\n        self.progress = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n\n    def call(self, inputs, training=None):\n        if training:\n            # Increment the progress during training (you might need to adjust how this increments based on your training regime)\n            self.progress.assign_add(0.01)  # Increment by a small value on each call\n\n        # Calculate the current slope based on the sigmoid function\n        x = self.progress\n        current_slope = self.initial_slope + (self.final_slope - self.initial_slope) / (1 + tf.exp(-self.steepness * (x - 0.5)))\n\n        # Apply the dynamic slope to the positive part of the inputs\n        positive_part = tf.maximum(0.0, inputs) * current_slope\n        # For negative inputs, just pass them through or adjust as needed\n        negative_part = tf.minimum(0.0, inputs)\n\n        return positive_part + negative_part\n\n    def get_config(self):\n        config = super(SmoothTransitionReLU, self).get_config()\n        config.update({\n            \"initial_slope\": self.initial_slope,\n            \"final_slope\": self.final_slope,\n            \"steepness\": self.steepness\n        })\n        return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model_with_custom_activation(x_train, y_train, x_val, y_val, batch_size, learning_rate,\n                                       initial_slope, target_slope, total_epochs):\n    # Initialize the custom activation function with provided parameters\n    custom_activation = SmoothTransitionReLU(initial_slope=initial_slope, final_slope=target_slope)\n\n    # Build the model using the custom activation function\n    model = build_resnet20(input_shape=(187, 1, 1), num_classes=5,\n                        activation_func=custom_activation)\n\n    # Define the checkpoint callback\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        'dynamic_relu_model.keras',\n        save_best_only=True,\n        monitor='val_loss',\n        mode='min',\n        verbose=1\n    )\n\n    # Compile the model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n                  metrics=['accuracy', 'precision', 'recall', 'auc'])\n\n    # Train the model\n    history = model.fit(x_train, y_train, epochs=total_epochs, batch_size=batch_size,\n                        validation_data=(x_val, y_val), verbose=1,\n                        callbacks=[checkpoint_cb])\n\n    return history, model\n\n\n# Example parameters\nbatch_size = 32\nlearning_rate = 0.005\ninitial_slope = 1.732\ntarget_slope = 0.557\nrate = 0.01\ntotal_epochs = 60\n\n# Train the model\nhistory_custom, model_custom = train_model_with_custom_activation(\n    X_train, y_train, X_test, y_test, batch_size, learning_rate,\n    initial_slope, target_slope, total_epochs\n)\n\nhistories[\"DSRELU\"] = history_custom","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mish(x):\n    return x * tf.math.tanh(tf.math.softplus(x))\n\nhistory_mish = train_model(mish, X_train, y_train, X_test, y_test, batch_size, learning_rate, \"mish\")\nhistories[\"MISH\"] = history_mish","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate F1 scores from precision and recall\ndef calculate_f1_scores(precision, recall):\n    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n\ndef print_info(history, model_name):\n    top_3_dict = {}\n    history = history.history\n    \n    print(\"*\" * 50)\n    print(f\"\\n{model_name} Results:\")\n    print(\"*\" * 50)\n    print(\"\\n\")\n\n    # Assuming history['loss'], history['val_loss'], etc., exist\n    training_loss = history['loss']\n    validation_loss = history['val_loss']\n    training_accuracy = history['accuracy']\n    validation_accuracy = history['val_accuracy']\n    training_auc = history['auc']\n    validation_auc = history['val_auc']\n    training_precision = history['precision']\n    validation_precision = history['val_precision']\n    training_recall = history['recall']\n    validation_recall = history['val_recall']\n    \n\n\n    # Calculate F1 scores based on available precision and recall in history\n    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n\n\n    top_3_dict[model_name] = {\n        \"training_loss\": sorted(training_loss)[:3],\n        \"validation_loss\": sorted(validation_loss)[:3],\n        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n\n    }\n\n    # Print Top 3 Lowest Losses\n    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n\n    # Print Top 3 Highest Accuracies\n    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n\n    # Print Top 3 AUCs\n    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n\n    # Print Top 3 F1 Scores\n    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n\n    # Print Top 3 Precision\n    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n\n    # Print Top 3 Recall\n    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_info(history_relu, \"RELU\")\nprint_info(history_sigmoid, \"SIGMOID\")\nprint_info(history_tanh, \"TANH\")\nprint_info(history_custom, \"DSRELU\")\nprint_info(history_mish, \"MISH\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}