{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:34.527394Z","iopub.status.busy":"2024-03-16T10:58:34.526729Z","iopub.status.idle":"2024-03-16T10:58:34.532091Z","shell.execute_reply":"2024-03-16T10:58:34.531101Z","shell.execute_reply.started":"2024-03-16T10:58:34.527350Z"},"trusted":true},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:35.610454Z","iopub.status.busy":"2024-03-16T10:58:35.609790Z","iopub.status.idle":"2024-03-16T10:58:36.981040Z","shell.execute_reply":"2024-03-16T10:58:36.980118Z","shell.execute_reply.started":"2024-03-16T10:58:35.610423Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load the MNIST dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Normalize the images to the range 0-1\n","x_train = x_train.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","# Reshape dataset to have a single channel\n","x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n","x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n","\n","# Convert class vectors to binary class matrices (one-hot encoding)\n","num_classes = 10\n","y_train = to_categorical(y_train, num_classes)\n","y_test = to_categorical(y_test, num_classes)\n","\n","print('MNIST dataset loaded and preprocessed.')\n","print('Training set size:', x_train.shape, y_train.shape)\n","print('Test set size:', x_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:39.300606Z","iopub.status.busy":"2024-03-16T10:58:39.300030Z","iopub.status.idle":"2024-03-16T10:58:39.812770Z","shell.execute_reply":"2024-03-16T10:58:39.811948Z","shell.execute_reply.started":"2024-03-16T10:58:39.300573Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the original training data to create a new training set and a validation set\n","x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n","    x_train, y_train, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:40.141467Z","iopub.status.busy":"2024-03-16T10:58:40.140296Z","iopub.status.idle":"2024-03-16T10:58:40.147324Z","shell.execute_reply":"2024-03-16T10:58:40.146336Z","shell.execute_reply.started":"2024-03-16T10:58:40.141431Z"},"trusted":true},"outputs":[],"source":["#  Print the shapes of the splits to verify\n","print(\"Shape of new training images:\", x_train_split.shape)\n","print(\"Shape of new training labels:\", y_train_split.shape)\n","print(\"Shape of validation images:\", x_val_split.shape)\n","print(\"Shape of validation labels:\", y_val_split.shape)\n","\n","\n","# Check the shapes of testing data after preprocessing\n","print(\"Shape of test images:\", x_test.shape)\n","print(\"Shape of test labels:\", y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:44.304964Z","iopub.status.busy":"2024-03-16T10:58:44.304130Z","iopub.status.idle":"2024-03-16T10:58:45.625881Z","shell.execute_reply":"2024-03-16T10:58:45.624907Z","shell.execute_reply.started":"2024-03-16T10:58:44.304928Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \\\n","Activation, Add, AveragePooling2D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","\n","def resnet_block(input_data, filters, conv_size, activation_func):\n","    x = Conv2D(filters, conv_size, padding='same')(input_data)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    x = Conv2D(filters, conv_size, padding='same')(x)\n","    x = BatchNormalization()(x)\n","\n","    # Adding the input data to the output of the block (Skip Connection)\n","    x = Add()([x, input_data])\n","\n","    x = Activation(activation_func)(x)\n","    return x\n","\n","def build_resnet20(input_shape, num_classes, activation_func):\n","    inputs = Input(shape=input_shape)\n","\n","    # Initial Conv Layer\n","    x = Conv2D(16, (3, 3), padding='same')(inputs)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    # ResNet Blocks\n","    for _ in range(3):\n","        x = resnet_block(x, 16, (3, 3), activation_func)\n","\n","    # Transition Layer\n","    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2))(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    for _ in range(3):\n","        x = resnet_block(x, 32, (3, 3), activation_func)\n","\n","    # Transition Layer\n","    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2))(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    for _ in range(3):\n","        x = resnet_block(x, 64, (3, 3), activation_func)\n","\n","    # Average Pooling\n","    x = AveragePooling2D(pool_size=(7, 7))(x)\n","    x = Flatten()(x)\n","\n","    # Output Layer\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Custom activation function\n","def custom_activation(x):\n","    # Define your custom activation logic here\n","    return tf.nn.relu(x)  # Example: using ReLU as a placeholder\n","\n","# Building the model with the custom activation function\n","input_shape = (32, 32, 3)  # Change based on your dataset\n","num_classes = 10  # Change based on your dataset\n","\n","model = build_resnet20(input_shape, num_classes, custom_activation)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T11:18:19.249242Z","iopub.status.busy":"2024-03-16T11:18:19.248887Z","iopub.status.idle":"2024-03-16T12:04:43.434639Z","shell.execute_reply":"2024-03-16T12:04:43.433761Z","shell.execute_reply.started":"2024-03-16T11:18:19.249213Z"},"trusted":true},"outputs":[],"source":["def train_model(activation_func, x_train, y_train, x_val, y_val, batch_size, learning_rate, name):\n","    # Build the model\n","    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1], activation_func=activation_func)\n","\n","    # Compile the model with specified learning rate\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n","                  metrics=[\n","                  'accuracy',\n","                  tf.keras.metrics.Precision(name='precision'),\n","                  tf.keras.metrics.Recall(name='recall'),\n","                  tf.keras.metrics.AUC(name='auc')\n","              ])\n","    \n","        # Define the checkpoint callback\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        f'{name}.keras', # Path where to save the model\n","        save_best_only=True, # Only save a model if `val_loss` has improved\n","        save_weights_only = False,\n","        monitor='val_loss', # Monitor the validation loss\n","        mode='min', # The lower the validation loss, the better the model\n","        verbose=1 # Log a message when a better model is found\n","    )\n","\n","\n","    # Train the model with specified batch size\n","    history = model.fit(x_train, y_train, epochs=60, batch_size=batch_size,\n","                        validation_data=(x_val, y_val), verbose=1, callbacks = [checkpoint_cb])\n","\n","    return history, model\n","\n","\n","\n","# Parameters\n","batch_size = 32\n","learning_rate = 0.005\n","\n","# Activation functions to try\n","activation_functions = [tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\n","names = [\"relu\", \"sigmoid\", \"tanh\"]\n","histories = {}\n","\n","# Train and evaluate the model with each activation function\n","print(f\"Training with Relu activation function\")\n","history_relu, model_relu = train_model(tf.nn.relu, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"relu\")\n","histories[\"relu\"] = history_relu\n","\n","print(f\"\\n\\n Training with Sigmoid activation function\")\n","history_sigmoid, model_sig = train_model( tf.nn.sigmoid, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"sigmoid\")\n","histories[\"sigmoid\"] = history_sigmoid\n","\n","\n","print(f\"\\n\\n Training with Tanh activation function\")\n","history_tanh, model_tanh = train_model(tf.nn.tanh, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"tanh\")\n","histories[\"tanh\"] = history_tanh"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T12:05:20.640932Z","iopub.status.busy":"2024-03-16T12:05:20.640292Z","iopub.status.idle":"2024-03-16T12:05:20.656874Z","shell.execute_reply":"2024-03-16T12:05:20.655757Z","shell.execute_reply.started":"2024-03-16T12:05:20.640902Z"},"trusted":true},"outputs":[],"source":["# Function to calculate F1 scores from precision and recall\n","def calculate_f1_scores(precision, recall):\n","    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n","\n","def print_info(history, model_name):\n","    history = history.history\n","    \n","    print(\"*\" * 50)\n","    print(f\"\\n{model_name} Results:\")\n","    print(\"*\" * 50)\n","    print(\"\\n\")\n","\n","    # Assuming history['loss'], history['val_loss'], etc., exist\n","    training_loss = history['loss']\n","    validation_loss = history['val_loss']\n","    training_accuracy = history['accuracy']\n","    validation_accuracy = history['val_accuracy']\n","    training_auc = history['auc']\n","    validation_auc = history['val_auc']\n","    training_precision = history['precision']\n","    validation_precision = history['val_precision']\n","    training_recall = history['recall']\n","    validation_recall = history['val_recall']\n","    \n","\n","\n","    # Calculate F1 scores based on available precision and recall in history\n","    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n","    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n","\n","\n","    top_3_dict[model_name] = {\n","        \"training_loss\": sorted(training_loss)[:3],\n","        \"validation_loss\": sorted(validation_loss)[:3],\n","        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n","        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n","        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n","        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n","        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n","        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n","        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n","        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n","        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n","        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n","\n","    }\n","\n","    # Print Top 3 Lowest Losses\n","    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n","    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n","\n","    # Print Top 3 Highest Accuracies\n","    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n","    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n","\n","    # Print Top 3 AUCs\n","    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n","    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n","\n","    # Print Top 3 F1 Scores\n","    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n","    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n","\n","    # Print Top 3 Precision\n","    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n","    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n","\n","    # Print Top 3 Recall\n","    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n","    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T12:06:21.228116Z","iopub.status.busy":"2024-03-16T12:06:21.227474Z","iopub.status.idle":"2024-03-16T12:06:21.235066Z","shell.execute_reply":"2024-03-16T12:06:21.234098Z","shell.execute_reply.started":"2024-03-16T12:06:21.228080Z"},"trusted":true},"outputs":[],"source":["print_info(history_relu, \"RELU\")\n","print_info(history_sigmoid, \"SIGMOID\")\n","print_info(history_tanh, \"TANH\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:58:55.014925Z","iopub.status.busy":"2024-03-16T10:58:55.014101Z","iopub.status.idle":"2024-03-16T10:58:55.024431Z","shell.execute_reply":"2024-03-16T10:58:55.023314Z","shell.execute_reply.started":"2024-03-16T10:58:55.014895Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","\n","class SmoothTransitionReLU(tf.keras.layers.Layer):\n","    def __init__(self, initial_slope, final_slope, steepness=10, **kwargs):\n","        super(SmoothTransitionReLU, self).__init__(**kwargs)\n","        self.initial_slope = initial_slope\n","        self.final_slope = final_slope\n","        self.steepness = steepness\n","        # Internal counter to track the relative progress of training\n","        self.progress = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n","\n","    def call(self, inputs, training=None):\n","        if training:\n","            # Increment the progress during training (you might need to adjust how this increments based on your training regime)\n","            self.progress.assign_add(0.01)  # Increment by a small value on each call\n","\n","        # Calculate the current slope based on the sigmoid function\n","        x = self.progress\n","        current_slope = self.initial_slope + (self.final_slope - self.initial_slope) / (1 + tf.exp(-self.steepness * (x - 0.5)))\n","\n","        # Apply the dynamic slope to the positive part of the inputs\n","        positive_part = tf.maximum(0.0, inputs) * current_slope\n","        # For negative inputs, just pass them through or adjust as needed\n","        negative_part = tf.minimum(0.0, inputs)\n","\n","        return positive_part + negative_part\n","\n","    def get_config(self):\n","        config = super(SmoothTransitionReLU, self).get_config()\n","        config.update({\n","            \"initial_slope\": self.initial_slope,\n","            \"final_slope\": self.final_slope,\n","            \"steepness\": self.steepness\n","        })\n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T10:59:00.283806Z","iopub.status.busy":"2024-03-16T10:59:00.282810Z","iopub.status.idle":"2024-03-16T11:14:21.369145Z","shell.execute_reply":"2024-03-16T11:14:21.368210Z","shell.execute_reply.started":"2024-03-16T10:59:00.283768Z"},"trusted":true},"outputs":[],"source":["def train_model_with_custom_activation(x_train, y_train, x_val, y_val, batch_size, learning_rate,\n","                                       initial_slope, target_slope, total_epochs):\n","    # Initialize the custom activation function with provided parameters\n","    custom_activation = SmoothTransitionReLU(initial_slope=initial_slope, final_slope=target_slope)\n","\n","    # Build the model using the custom activation function\n","    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1],\n","                        activation_func=custom_activation)\n","\n","    # Define the checkpoint callback\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        'dynamic_relu_model.keras',\n","        save_best_only=True,\n","        monitor='val_loss',\n","        mode='min',\n","        verbose=1\n","    )\n","\n","    # Compile the model\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n","                  metrics=['accuracy', 'precision', 'recall', 'auc'])\n","\n","    # Train the model\n","    history = model.fit(x_train, y_train, epochs=total_epochs, batch_size=batch_size,\n","                        validation_data=(x_val, y_val), verbose=1,\n","                        callbacks=[checkpoint_cb])\n","\n","    return history, model\n","\n","\n","# Example parameters\n","batch_size = 32\n","learning_rate = 0.005\n","initial_slope = 1.732\n","target_slope = 0.557\n","rate = 0.01\n","total_epochs = 60\n","\n","# Train the model\n","history_custom, model_custom = train_model_with_custom_activation(\n","    x_train_split, y_train_split, x_val_split, y_val_split, batch_size, learning_rate,\n","    initial_slope, target_slope, total_epochs\n",")\n","\n","histories = {}\n","histories[\"custom\"] = history_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-16T11:15:28.612787Z","iopub.status.busy":"2024-03-16T11:15:28.612165Z","iopub.status.idle":"2024-03-16T11:15:28.629885Z","shell.execute_reply":"2024-03-16T11:15:28.628935Z","shell.execute_reply.started":"2024-03-16T11:15:28.612756Z"},"trusted":true},"outputs":[],"source":["# Function to calculate F1 scores from precision and recall\n","def calculate_f1_scores(precision, recall):\n","    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n","\n","top_3_dict = {}\n","test = {}\n","# histories = dict(histories[\"custom\"].history)\n","test[\"custom\"] = histories[\"custom\"].history\n","\n","\n","for model_name, history in test.items():\n","    print(f\"\\n{model_name} Results:\")\n","\n","    # Assuming history['loss'], history['val_loss'], etc., exist\n","    training_loss = history['loss']\n","    validation_loss = history['val_loss']\n","    training_accuracy = history['accuracy']\n","    validation_accuracy = history['val_accuracy']\n","    training_auc = history['auc']\n","    validation_auc = history['val_auc']\n","    training_precision = history['precision']\n","    validation_precision = history['val_precision']\n","    training_recall = history['recall']\n","    validation_recall = history['val_recall']\n","    \n","\n","\n","    # Calculate F1 scores based on available precision and recall in history\n","    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n","    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n","\n","\n","    top_3_dict[model_name] = {\n","        \"training_loss\": sorted(training_loss)[:3],\n","        \"validation_loss\": sorted(validation_loss)[:3],\n","        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n","        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n","        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n","        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n","        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n","        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n","        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n","        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n","        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n","        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n","\n","    }\n","\n","    # Print Top 3 Lowest Losses\n","    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n","    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n","\n","    # Print Top 3 Highest Accuracies\n","    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n","    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n","\n","    # Print Top 3 AUCs\n","    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n","    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n","\n","    # Print Top 3 F1 Scores\n","    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n","    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n","\n","    # Print Top 3 Precision\n","    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n","    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n","\n","    # Print Top 3 Recall\n","    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n","    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T14:57:45.443574Z","iopub.status.busy":"2024-03-11T14:57:45.443248Z","iopub.status.idle":"2024-03-11T14:57:45.448746Z","shell.execute_reply":"2024-03-11T14:57:45.447657Z","shell.execute_reply.started":"2024-03-11T14:57:45.443546Z"},"trusted":true},"outputs":[],"source":["def mish(x):\n","    return x * tf.math.tanh(tf.math.softplus(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T15:30:05.744314Z","iopub.status.busy":"2024-03-11T15:30:05.743419Z"},"trusted":true},"outputs":[],"source":["history_mish, model_mish = train_model(mish, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"mish\")\n","histories[\"mish\"] = history_mish"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T15:25:02.367670Z","iopub.status.busy":"2024-03-11T15:25:02.367258Z","iopub.status.idle":"2024-03-11T15:25:02.374739Z","shell.execute_reply":"2024-03-11T15:25:02.373708Z","shell.execute_reply.started":"2024-03-11T15:25:02.367637Z"},"trusted":true},"outputs":[],"source":["history_Save = {\n","    \"relu\" : histories[\"relu\"].history, \n","    \"sigmoid\" : histories[\"sigmoid\"].history, \n","    \"tanh\" : histories[\"tanh\"].history, \n","    \"custom\" : histories[\"custom\"].history, \n","    \"mish\" : histories[\"mish\"].history}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T15:25:04.052508Z","iopub.status.busy":"2024-03-11T15:25:04.052116Z","iopub.status.idle":"2024-03-11T15:25:04.061338Z","shell.execute_reply":"2024-03-11T15:25:04.060214Z","shell.execute_reply.started":"2024-03-11T15:25:04.052469Z"},"trusted":true},"outputs":[],"source":["# Dumping the dictionary into a pickle file\n","file_path = \"history_mitbih.pkl\"\n","with open(file_path, 'wb') as file:\n","    pickle.dump(history_Save, file)\n","\n","file_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
