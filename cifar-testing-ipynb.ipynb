{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:30:10.878459Z","iopub.status.busy":"2024-03-17T19:30:10.877586Z","iopub.status.idle":"2024-03-17T19:30:26.290987Z","shell.execute_reply":"2024-03-17T19:30:26.289983Z","shell.execute_reply.started":"2024-03-17T19:30:10.878427Z"},"trusted":true},"outputs":[],"source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:30:26.293383Z","iopub.status.busy":"2024-03-17T19:30:26.292801Z","iopub.status.idle":"2024-03-17T19:31:28.548897Z","shell.execute_reply":"2024-03-17T19:31:28.547766Z","shell.execute_reply.started":"2024-03-17T19:30:26.293344Z"},"trusted":true},"outputs":[],"source":["# Load CIFAR-100 dataset\n","(x_train, y_train), (x_test, y_test) = tfds.as_numpy(tfds.load(\n","    'cifar100',\n","    split=['train', 'test'],\n","    batch_size=-1, \n","    as_supervised=True,\n","))\n","\n","# Normalize the images to a 0-1 range\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","# One-hot encode the labels\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=100)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:31:28.550522Z","iopub.status.busy":"2024-03-17T19:31:28.550194Z","iopub.status.idle":"2024-03-17T19:31:29.431703Z","shell.execute_reply":"2024-03-17T19:31:29.430733Z","shell.execute_reply.started":"2024-03-17T19:31:28.550496Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the original training data to create a new training set and a validation set\n","x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n","    x_train, y_train, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:31:29.434666Z","iopub.status.busy":"2024-03-17T19:31:29.433803Z","iopub.status.idle":"2024-03-17T19:31:29.441187Z","shell.execute_reply":"2024-03-17T19:31:29.440090Z","shell.execute_reply.started":"2024-03-17T19:31:29.434634Z"},"trusted":true},"outputs":[],"source":["#  Print the shapes of the splits to verify\n","print(\"Shape of new training images:\", x_train_split.shape)\n","print(\"Shape of new training labels:\", y_train_split.shape)\n","print(\"Shape of validation images:\", x_val_split.shape)\n","print(\"Shape of validation labels:\", y_val_split.shape)\n","\n","\n","# Check the shapes of testing data after preprocessing\n","print(\"Shape of test images:\", x_test.shape)\n","print(\"Shape of test labels:\", y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:31:29.443381Z","iopub.status.busy":"2024-03-17T19:31:29.442998Z","iopub.status.idle":"2024-03-17T19:31:29.984017Z","shell.execute_reply":"2024-03-17T19:31:29.983057Z","shell.execute_reply.started":"2024-03-17T19:31:29.443329Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \\\n","Activation, Add, AveragePooling2D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","\n","def resnet_block(input_data, filters, conv_size, activation_func):\n","    x = Conv2D(filters, conv_size, padding='same')(input_data)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    x = Conv2D(filters, conv_size, padding='same')(x)\n","    x = BatchNormalization()(x)\n","\n","    # Adding the input data to the output of the block (Skip Connection)\n","    x = Add()([x, input_data])\n","\n","    x = Activation(activation_func)(x)\n","    return x\n","\n","def build_resnet20(input_shape, num_classes, activation_func):\n","    inputs = Input(shape=input_shape)\n","\n","    # Initial Conv Layer\n","    x = Conv2D(16, (3, 3), padding='same')(inputs)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    # ResNet Blocks\n","    for _ in range(3):\n","        x = resnet_block(x, 16, (3, 3), activation_func)\n","\n","    # Transition Layer\n","    x = Conv2D(32, (3, 3), padding='same', strides=(2, 2))(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    for _ in range(3):\n","        x = resnet_block(x, 32, (3, 3), activation_func)\n","\n","    # Transition Layer\n","    x = Conv2D(64, (3, 3), padding='same', strides=(2, 2))(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(activation_func)(x)\n","\n","    for _ in range(3):\n","        x = resnet_block(x, 64, (3, 3), activation_func)\n","\n","    # Average Pooling\n","    x = AveragePooling2D(pool_size=(8, 8))(x)\n","    x = Flatten()(x)\n","\n","    # Output Layer\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Custom activation function\n","def custom_activation(x):\n","    # Define your custom activation logic here\n","    return tf.nn.relu(x)  # Example: using ReLU as a placeholder\n","\n","# Building the model with the custom activation function\n","input_shape = (32, 32, 3)  # Change based on your dataset\n","num_classes = 100  # Change based on your dataset\n","\n","model = build_resnet20(input_shape, num_classes, custom_activation)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T19:31:29.985778Z","iopub.status.busy":"2024-03-17T19:31:29.985456Z","iopub.status.idle":"2024-03-17T20:15:16.022655Z","shell.execute_reply":"2024-03-17T20:15:16.021670Z","shell.execute_reply.started":"2024-03-17T19:31:29.985752Z"},"trusted":true},"outputs":[],"source":["def train_model(activation_func, x_train, y_train, x_val, y_val, batch_size, learning_rate, name):\n","    # Build the model\n","    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1], activation_func=activation_func)\n","\n","    # Compile the model with specified learning rate\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n","                  metrics=[\n","                  'accuracy',\n","                  tf.keras.metrics.Precision(name='precision'),\n","                  tf.keras.metrics.Recall(name='recall'),\n","                  tf.keras.metrics.AUC(name='auc')\n","              ])\n","    \n","        # Define the checkpoint callback\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        f'{name}.keras', # Path where to save the model\n","        save_best_only=True, # Only save a model if `val_loss` has improved\n","        save_weights_only = False,\n","        monitor='val_loss', # Monitor the validation loss\n","        mode='min', # The lower the validation loss, the better the model\n","        verbose=1 # Log a message when a better model is found\n","    )\n","\n","\n","    # Train the model with specified batch size\n","    history = model.fit(x_train, y_train, epochs=60, batch_size=batch_size,\n","                        validation_data=(x_val, y_val), verbose=1, callbacks = [checkpoint_cb])\n","\n","    return history, model\n","\n","\n","\n","# Parameters\n","batch_size = 32\n","learning_rate = 0.005\n","\n","# Activation functions to try\n","activation_functions = [tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh]\n","names = [\"relu\", \"sigmoid\", \"tanh\"]\n","histories = {}\n","\n","# Train and evaluate the model with each activation function\n","print(f\"Training with Relu activation function\")\n","history_relu, model_relu = train_model(tf.nn.relu, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"relu\")\n","histories[\"relu\"] = history_relu\n","\n","print(f\"\\n\\n Training with Sigmoid activation function\")\n","history_sigmoid, model_sigmoid = train_model( tf.nn.sigmoid, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"sigmoid\")\n","histories[\"sigmoid\"] = history_sigmoid\n","\n","\n","print(f\"\\n\\n Training with Tanh activation function\")\n","history_tanh, model_tanh = train_model(tf.nn.tanh, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"tanh\")\n","histories[\"tanh\"] = history_tanh"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T20:15:16.024683Z","iopub.status.busy":"2024-03-17T20:15:16.024197Z","iopub.status.idle":"2024-03-17T20:15:16.035306Z","shell.execute_reply":"2024-03-17T20:15:16.034325Z","shell.execute_reply.started":"2024-03-17T20:15:16.024645Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","\n","class SmoothTransitionReLU(tf.keras.layers.Layer):\n","    def __init__(self, initial_slope, final_slope, steepness=10, **kwargs):\n","        super(SmoothTransitionReLU, self).__init__(**kwargs)\n","        self.initial_slope = initial_slope\n","        self.final_slope = final_slope\n","        self.steepness = steepness\n","        # Internal counter to track the relative progress of training\n","        self.progress = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n","\n","    def call(self, inputs, training=None):\n","        if training:\n","            # Increment the progress during training (you might need to adjust how this increments based on your training regime)\n","            self.progress.assign_add(0.01)  # Increment by a small value on each call\n","\n","        # Calculate the current slope based on the sigmoid function\n","        x = self.progress\n","        current_slope = self.initial_slope + (self.final_slope - self.initial_slope) / (1 + tf.exp(-self.steepness * (x - 0.5)))\n","\n","        # Apply the dynamic slope to the positive part of the inputs\n","        positive_part = tf.maximum(0.0, inputs) * current_slope\n","        # For negative inputs, just pass them through or adjust as needed\n","        negative_part = tf.minimum(0.0, inputs)\n","\n","        return positive_part + negative_part\n","\n","    def get_config(self):\n","        config = super(SmoothTransitionReLU, self).get_config()\n","        config.update({\n","            \"initial_slope\": self.initial_slope,\n","            \"final_slope\": self.final_slope,\n","            \"steepness\": self.steepness\n","        })\n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T20:15:16.037087Z","iopub.status.busy":"2024-03-17T20:15:16.036747Z","iopub.status.idle":"2024-03-17T20:29:38.334660Z","shell.execute_reply":"2024-03-17T20:29:38.333611Z","shell.execute_reply.started":"2024-03-17T20:15:16.037058Z"},"trusted":true},"outputs":[],"source":["def train_model_with_custom_activation(x_train, y_train, x_val, y_val, batch_size, learning_rate,\n","                                       initial_slope, target_slope, total_epochs):\n","    # Initialize the custom activation function with provided parameters\n","    custom_activation = SmoothTransitionReLU(initial_slope=initial_slope, final_slope=target_slope)\n","\n","    # Build the model using the custom activation function\n","    model = build_resnet20(input_shape=x_train.shape[1:], num_classes=y_train.shape[1],\n","                        activation_func=custom_activation)\n","\n","    # Define the checkpoint callback\n","    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n","        'dynamic_relu_model.keras',\n","        save_best_only=True,\n","        monitor='val_loss',\n","        mode='min',\n","        verbose=1\n","    )\n","\n","    # Compile the model\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', \n","                  metrics=['accuracy', 'precision', 'recall', 'auc'])\n","\n","    # Train the model\n","    history = model.fit(x_train, y_train, epochs=total_epochs, batch_size=batch_size,\n","                        validation_data=(x_val, y_val), verbose=1,\n","                        callbacks=[checkpoint_cb])\n","\n","    return history, model\n","\n","\n","# Example parameters\n","batch_size = 32\n","learning_rate = 0.005\n","initial_slope = 1.732\n","target_slope = 0.557\n","rate = 0.01\n","total_epochs = 60\n","\n","# Train the model\n","history_custom, model_custom = train_model_with_custom_activation(\n","    x_train_split, y_train_split, x_val_split, y_val_split, batch_size, learning_rate,\n","    initial_slope, target_slope, total_epochs\n",")\n","\n","histories[\"custom\"] = history_custom"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T20:29:38.336280Z","iopub.status.busy":"2024-03-17T20:29:38.335957Z","iopub.status.idle":"2024-03-17T20:29:38.354118Z","shell.execute_reply":"2024-03-17T20:29:38.352947Z","shell.execute_reply.started":"2024-03-17T20:29:38.336250Z"},"trusted":true},"outputs":[],"source":["# Function to calculate F1 scores from precision and recall\n","def calculate_f1_scores(precision, recall):\n","    return 2 * (np.array(precision) * np.array(recall)) / (np.array(precision) + np.array(recall))\n","\n","def print_info(history, model_name):\n","    history = history.history\n","    \n","    print(\"*\" * 50)\n","    print(f\"\\n{model_name} Results:\")\n","    print(\"*\" * 50)\n","    print(\"\\n\")\n","\n","    # Assuming history['loss'], history['val_loss'], etc., exist\n","    training_loss = history['loss']\n","    validation_loss = history['val_loss']\n","    training_accuracy = history['accuracy']\n","    validation_accuracy = history['val_accuracy']\n","    training_auc = history['auc']\n","    validation_auc = history['val_auc']\n","    training_precision = history['precision']\n","    validation_precision = history['val_precision']\n","    training_recall = history['recall']\n","    validation_recall = history['val_recall']\n","    \n","\n","\n","    # Calculate F1 scores based on available precision and recall in history\n","    training_f1 = calculate_f1_scores(history['precision'], history['recall'])\n","    validation_f1 = calculate_f1_scores(history['val_precision'], history['val_recall'])\n","\n","\n","    top_3_dict[model_name] = {\n","        \"training_loss\": sorted(training_loss)[:3],\n","        \"validation_loss\": sorted(validation_loss)[:3],\n","        \"training_accuracy\": sorted(training_accuracy, reverse=True)[:3],\n","        \"validation_accuracy\": sorted(validation_accuracy, reverse=True)[:3],\n","        \"training_auc\": sorted(training_auc, reverse=True)[:3],\n","        \"validation_auc\": sorted(validation_auc, reverse=True)[:3], \n","        \"training_precision\": sorted(training_precision, reverse=True)[:3],\n","        \"validation_precision\": sorted(validation_precision, reverse=True)[:3],\n","        \"training_recall\": sorted(training_recall, reverse=True)[:3],\n","        \"validation_recall\": sorted(validation_recall, reverse=True)[:3], \n","        \"training_f1\": sorted(training_f1, reverse=True)[:3],\n","        \"validation_f1\": sorted(validation_f1, reverse=True)[:3]\n","\n","    }\n","\n","    # Print Top 3 Lowest Losses\n","    print(\"Top 3 Lowest Training Losses:\", sorted(training_loss)[:3])\n","    print(\"Top 3 Lowest Validation Losses:\", sorted(validation_loss)[:3])\n","\n","    # Print Top 3 Highest Accuracies\n","    print(\"Top 3 Highest Training Accuracies:\", sorted(training_accuracy, reverse=True)[:3])\n","    print(\"Top 3 Highest Validation Accuracies:\", sorted(validation_accuracy, reverse=True)[:3])\n","\n","    # Print Top 3 AUCs\n","    print(\"Top 3 Training AUCs:\", sorted(training_auc, reverse=True)[:3])\n","    print(\"Top 3 Validation AUCs:\", sorted(validation_auc, reverse=True)[:3])\n","\n","    # Print Top 3 F1 Scores\n","    print(\"Top 3 Training F1 Scores:\", sorted(training_f1, reverse=True)[:3])\n","    print(\"Top 3 Validation F1 Scores:\", sorted(validation_f1, reverse=True)[:3])\n","\n","    # Print Top 3 Precision\n","    print(\"Top 3 Training Precision:\", sorted(training_precision, reverse=True)[:3])\n","    print(\"Top 3 Validation Precision:\", sorted(validation_precision, reverse=True)[:3])\n","\n","    # Print Top 3 Recall\n","    print(\"Top 3 Training Recall:\", sorted(training_recall, reverse=True)[:3])\n","    print(\"Top 3 Validation Recall:\", sorted(validation_recall, reverse=True)[:3])"]},{"cell_type":"markdown","metadata":{},"source":["## The Mish Activation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T20:29:38.358559Z","iopub.status.busy":"2024-03-17T20:29:38.358248Z","iopub.status.idle":"2024-03-17T20:29:38.367897Z","shell.execute_reply":"2024-03-17T20:29:38.366931Z","shell.execute_reply.started":"2024-03-17T20:29:38.358534Z"},"trusted":true},"outputs":[],"source":["def mish(x):\n","    return x * tf.math.tanh(tf.math.softplus(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T20:29:38.369492Z","iopub.status.busy":"2024-03-17T20:29:38.369178Z","iopub.status.idle":"2024-03-17T20:44:21.221437Z","shell.execute_reply":"2024-03-17T20:44:21.220050Z","shell.execute_reply.started":"2024-03-17T20:29:38.369468Z"},"trusted":true},"outputs":[],"source":["history_mish, model_mish = train_model(mish, x_train_split, y_train_split,\n","                          x_val_split, y_val_split, batch_size, learning_rate, \"mish\")\n","histories[\"mish\"] = history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-17T20:44:21.222666Z","iopub.status.idle":"2024-03-17T20:44:21.223156Z","shell.execute_reply":"2024-03-17T20:44:21.222923Z","shell.execute_reply.started":"2024-03-17T20:44:21.222903Z"},"trusted":true},"outputs":[],"source":["print_info(history_relu, \"RELU\")\n","print_info(history_sigmoid, \"SIGMOID\")\n","print_info(history_tanh, \"TANH\")\n","print_info(history_mish, \"MISH\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-17T20:44:21.224875Z","iopub.status.idle":"2024-03-17T20:44:21.225406Z","shell.execute_reply":"2024-03-17T20:44:21.225143Z","shell.execute_reply.started":"2024-03-17T20:44:21.225124Z"},"trusted":true},"outputs":[],"source":["history_Save = {\n","    \"relu\" : histories[\"relu\"].history, \n","    \"sigmoid\" : histories[\"sigmoid\"].history, \n","    \"tanh\" : histories[\"tanh\"].history, \n","    \"custom\" : histories[\"custom\"].history, \n","    \"mish\" : histories[\"mish\"].history}\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-17T20:44:21.227117Z","iopub.status.idle":"2024-03-17T20:44:21.227525Z","shell.execute_reply":"2024-03-17T20:44:21.227323Z","shell.execute_reply.started":"2024-03-17T20:44:21.227307Z"},"trusted":true},"outputs":[],"source":["# Dumping the dictionary into a pickle file\n","file_path = \"history.pkl\"\n","with open(file_path, 'wb') as file:\n","    pickle.dump(history_Save, file)\n","\n","file_path"]},{"cell_type":"markdown","metadata":{},"source":["## Testing and evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-17T20:44:21.229144Z","iopub.status.idle":"2024-03-17T20:44:21.229501Z","shell.execute_reply":"2024-03-17T20:44:21.229319Z","shell.execute_reply.started":"2024-03-17T20:44:21.229306Z"},"trusted":true},"outputs":[],"source":["model_paths = [\n","    \"/kaggle/working/relu.keras\", \n","    \"/kaggle/working/tanh.keras\", \n","    \"/kaggle/working/sigmoid.keras\", \n","    \"/kaggle/working/mish.keras\", \n","    \"/kaggle/working/custom_relu.keras\"\n","]\n","\n","model_path = \"/kaggle/input/models/relu.keras\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4533425,"sourceId":7753319,"sourceType":"datasetVersion"}],"dockerImageVersionId":30666,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
